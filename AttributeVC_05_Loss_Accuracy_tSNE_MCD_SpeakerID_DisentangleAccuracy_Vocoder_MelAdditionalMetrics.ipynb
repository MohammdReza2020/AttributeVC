{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2l4qqwpIuD9"
      },
      "source": [
        "# بسم الله الرحمن الرحیم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgw3ImUVOkFK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlBGX9opOkSG"
      },
      "source": [
        "**--------------------------------**\n",
        "\n",
        "**Developing VC**\n",
        "\n",
        "**--------------------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDU8aDIUXozc",
        "outputId": "85cafda0-d0ee-4cb0-91ba-8bf0dbd0176b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s5FILrjxXxhT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Class ---\n",
        "# (Keep your existing Config class as is)\n",
        "class Config:\n",
        "    # Dataset paths\n",
        "    VC_DIR = \"/content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental\" # Base VCTK dataset directory\n",
        "    VC_Output = \"/content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2\" # Base VCTK dataset directory\n",
        "    WAV_DIR = os.path.join(VC_DIR, \"wav48\") # Directory containing wav files\n",
        "    METADATA_PATH = os.path.join(VC_DIR, \"speaker-info.txt\") # Speaker metadata file path\n",
        "\n",
        "    # Audio processing parameters\n",
        "    SAMPLE_RATE = 16000 # Target sample rate for all audio (Hz)\n",
        "    N_MELS = 80         # Number of Mel bands for spectrogram\n",
        "    N_FFT = 400         # FFT window size (samples)\n",
        "    HOP_LENGTH = 160    # Hop length (samples) between windows\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2    # Number of CPU workers for data loading\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    NUM_EPOCHS = 50\n",
        "    TRAIN_SPLIT_RATIO = 0.8 # 80% for training, 20% for validation\n",
        "    RANDOM_SEED = 42    # For reproducibility\n",
        "\n",
        "    # Device configuration\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Model parameters (Specific to this VC architecture)\n",
        "    Z_CONTENT_DIM = 256 # Dimension for speaker-invariant content embedding\n",
        "    Z_GENDER_DIM = 16   # Dimension for gender embedding\n",
        "    Z_ACCENT_DIM = 64   # Dimension for accent embedding\n",
        "    Z_SPEAKER_ID_DIM = 256 # Dimension for individual speaker ID embedding (like x-vector)\n",
        "    Z_AGE_DIM = 32      # Dimension for age embedding (you can adjust this)\n",
        "    Z_REGION_DIM = 64   # Dimension for region embedding (you can adjust this)\n",
        "\n",
        "# Create directories if they don't exist\n",
        "if not os.path.exists(Config.VC_DIR):\n",
        "    os.makedirs(Config.VC_DIR)\n",
        "if not os.path.exists(Config.WAV_DIR):\n",
        "    os.makedirs(Config.WAV_DIR)\n",
        "\n",
        "print(f\"Using device: {Config.DEVICE}\")\n",
        "print(f\"VC_DIR: {Config.VC_DIR}\")\n",
        "print(f\"WAV_DIR: {Config.WAV_DIR}\")\n",
        "\n",
        "\n",
        "# --- Step 1: Load and Process Speaker Metadata ---\n",
        "# This part reads your speaker-info.txt to get AGE, GENDER, ACCENTS, and REGION for each speaker ID.\n",
        "speaker_metadata = {}\n",
        "unique_genders = set()\n",
        "unique_accents = set()\n",
        "unique_regions = set() # New set for unique regions\n",
        "unique_ages = set()\n",
        "all_speaker_ids = set()\n",
        "\n",
        "# Load metadata from speaker-info.txt\n",
        "try:\n",
        "    with open(Config.METADATA_PATH, 'r') as f:\n",
        "        # Skip header\n",
        "        header = f.readline().strip().split() # This reads \"ID AGE GENDER ACCENTS REGION\"\n",
        "        print(f\"Metadata header found: {header}\")\n",
        "\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            # The 'REGION' column can have multiple words (e.g., \"Southern England\").\n",
        "            # Simple split might combine \"Southern\" and \"England\" if there are multiple spaces,\n",
        "            # but usually, for VCTK, it's tab-separated or fixed-width, or just space-separated\n",
        "            # where multi-word regions are still handled as single entries after the accent.\n",
        "\n",
        "            # Let's adjust based on the structure:\n",
        "            # ID (0), AGE (1), GENDER (2), ACCENTS (3), REGION (4 onwards)\n",
        "            if len(parts) >= 5: # Ensure there are enough parts for ID, AGE, GENDER, ACCENTS, REGION\n",
        "                speaker_id = parts[0]\n",
        "                age = parts[1] # Age is typically an integer, but can be read as string first\n",
        "                gender = parts[2]\n",
        "                accent = parts[3]\n",
        "                # Region might be multiple words, so join the rest of the parts\n",
        "                region = \" \".join(parts[4:])\n",
        "\n",
        "                # Store all extracted information\n",
        "                speaker_metadata[speaker_id] = {\n",
        "                    'age': age,\n",
        "                    'gender': gender,\n",
        "                    'accent': accent,\n",
        "                    'region': region\n",
        "                }\n",
        "                unique_genders.add(gender)\n",
        "                unique_accents.add(accent)\n",
        "                unique_regions.add(region) # Add region to the set\n",
        "                unique_ages.add(age)\n",
        "                all_speaker_ids.add(speaker_id)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping malformed line in metadata: {line.strip()} (expected at least 5 parts)\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: speaker-info.txt not found at {Config.METADATA_PATH}.\")\n",
        "    print(\"Please ensure the VCTK dataset is correctly set up and the path is accurate.\")\n",
        "    # Exit or handle gracefully if essential file is missing\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while reading metadata: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Create integer mappings for categorical data (gender, accent, region)\n",
        "speaker_id_to_int = {sid: i for i, sid in enumerate(sorted(list(all_speaker_ids)))}\n",
        "gender_to_id = {gender: i for i, gender in enumerate(sorted(list(unique_genders)))}\n",
        "accent_to_id = {accent: i for i, accent in enumerate(sorted(list(unique_accents)))}\n",
        "region_to_id = {region: i for i, region in enumerate(sorted(list(unique_regions)))} # New mapping for regions\n",
        "age_to_id = {age: i for i, age in enumerate(sorted(list(unique_ages)))} # New mapping for ages\n",
        "\n",
        "print(f\"Loaded metadata for {len(speaker_metadata)} speakers.\")\n",
        "print(f\"Unique Genders: {unique_genders} (Mapped to: {gender_to_id})\")\n",
        "print(f\"Unique Accents: {unique_accents} (Mapped to: {accent_to_id})\")\n",
        "print(f\"Unique Regions: {unique_regions} (Mapped to: {region_to_id})\") # Print regions\n",
        "print(f\"Unique ages: {unique_ages} (Mapped to: {age_to_id})\") # Print ages\n",
        "print(f\"Total Speakers: {len(speaker_id_to_int)}\")\n",
        "\n",
        "# Example of accessing the new data for a speaker (e.g., '225')\n",
        "if '225' in speaker_metadata:\n",
        "    print(f\"\\nExample speaker '225' metadata: {speaker_metadata['225']}\")\n",
        "    print(f\"Mapped IDs for speaker '225':\")\n",
        "    print(f\"  Speaker ID: {speaker_id_to_int['225']}\")\n",
        "    print(f\"  Gender ID: {gender_to_id[speaker_metadata['225']['gender']]}\")\n",
        "    print(f\"  Accent ID: {accent_to_id[speaker_metadata['225']['accent']]}\")\n",
        "    print(f\"  Region ID: {region_to_id[speaker_metadata['225']['region']]}\")\n",
        "    print(f\"  Age ID: {age_to_id[speaker_metadata['225']['age']]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ-t583NkVWZ",
        "outputId": "df1d337b-4d4f-436b-ac35-f8780651f5a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "VC_DIR: /content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental\n",
            "WAV_DIR: /content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental/wav48\n",
            "Metadata header found: ['ID', 'AGE', 'GENDER', 'ACCENTS', 'REGION']\n",
            "Warning: Skipping malformed line in metadata: 248  23  F    Indian (expected at least 5 parts)\n",
            "Warning: Skipping malformed line in metadata: 251  26  M    Indian (expected at least 5 parts)\n",
            "Warning: Skipping malformed line in metadata: 329  23  F    American (expected at least 5 parts)\n",
            "Warning: Skipping malformed line in metadata: 330  26  F    American (expected at least 5 parts)\n",
            "Warning: Skipping malformed line in metadata: 362  29  F    American (expected at least 5 parts)\n",
            "Warning: Skipping malformed line in metadata: 376  22  M    Indian (expected at least 5 parts)\n",
            "Loaded metadata for 102 speakers.\n",
            "Unique Genders: {'M', 'F'} (Mapped to: {'F': 0, 'M': 1})\n",
            "Unique Accents: {'Scottish', 'NorthernIrish', 'Australian', 'Canadian', 'Irish', 'NewZealand', 'SouthAfrican', 'Welsh', 'American', 'English'} (Mapped to: {'American': 0, 'Australian': 1, 'Canadian': 2, 'English': 3, 'Irish': 4, 'NewZealand': 5, 'NorthernIrish': 6, 'Scottish': 7, 'SouthAfrican': 8, 'Welsh': 9})\n",
            "Unique Regions: {'Derry', 'Pretoria', 'Ohio', 'California', 'San Francisco', 'Essex', 'North Carolina', 'Cumbria', 'Newcastle', 'Edinburgh', 'West Lothian', 'Aberdeen', 'Florida', 'Chicago', 'Cork', 'Belfast', 'West Dumfries', 'Tipperary', 'Napa', 'Donegal', 'Alberta', 'Staffordshire', 'Galloway', 'Alabama', 'New York', 'English Sydney', 'New Jersey', 'Leicester', 'Surrey', 'Iowa', 'SE England', 'Stockton-on-tees', 'Oxford', 'Philadelphia', 'Cape Town', 'Midlothian', 'Yorkshire', 'Toronto', 'Hamilton', 'SW England', 'London', 'Selkirk', 'Cardiff', 'Nottingham', 'Cheshire', 'New England', 'Athlone', 'Manchester', 'Indiana', 'Argyll', 'Orkney', 'Southern England', 'Ontario', 'County Down', 'York', 'NE England', 'Ross', 'Fife', 'Dublin', 'Suffolk', 'Tennessee', 'Pennsylvania', 'Montreal', 'English', 'Perth', 'Johannesburg', 'Birmingham'} (Mapped to: {'Aberdeen': 0, 'Alabama': 1, 'Alberta': 2, 'Argyll': 3, 'Athlone': 4, 'Belfast': 5, 'Birmingham': 6, 'California': 7, 'Cape Town': 8, 'Cardiff': 9, 'Cheshire': 10, 'Chicago': 11, 'Cork': 12, 'County Down': 13, 'Cumbria': 14, 'Derry': 15, 'Donegal': 16, 'Dublin': 17, 'Edinburgh': 18, 'English': 19, 'English Sydney': 20, 'Essex': 21, 'Fife': 22, 'Florida': 23, 'Galloway': 24, 'Hamilton': 25, 'Indiana': 26, 'Iowa': 27, 'Johannesburg': 28, 'Leicester': 29, 'London': 30, 'Manchester': 31, 'Midlothian': 32, 'Montreal': 33, 'NE England': 34, 'Napa': 35, 'New England': 36, 'New Jersey': 37, 'New York': 38, 'Newcastle': 39, 'North Carolina': 40, 'Nottingham': 41, 'Ohio': 42, 'Ontario': 43, 'Orkney': 44, 'Oxford': 45, 'Pennsylvania': 46, 'Perth': 47, 'Philadelphia': 48, 'Pretoria': 49, 'Ross': 50, 'SE England': 51, 'SW England': 52, 'San Francisco': 53, 'Selkirk': 54, 'Southern England': 55, 'Staffordshire': 56, 'Stockton-on-tees': 57, 'Suffolk': 58, 'Surrey': 59, 'Tennessee': 60, 'Tipperary': 61, 'Toronto': 62, 'West Dumfries': 63, 'West Lothian': 64, 'York': 65, 'Yorkshire': 66})\n",
            "Unique ages: {'38', '26', '33', '25', '24', '19', '21', '20', '27', '22', '28', '23', '32', '18', '29'} (Mapped to: {'18': 0, '19': 1, '20': 2, '21': 3, '22': 4, '23': 5, '24': 6, '25': 7, '26': 8, '27': 9, '28': 10, '29': 11, '32': 12, '33': 13, '38': 14})\n",
            "Total Speakers: 102\n",
            "\n",
            "Example speaker '225' metadata: {'age': '23', 'gender': 'F', 'accent': 'English', 'region': 'Southern England'}\n",
            "Mapped IDs for speaker '225':\n",
            "  Speaker ID: 0\n",
            "  Gender ID: 0\n",
            "  Accent ID: 3\n",
            "  Region ID: 55\n",
            "  Age ID: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset # Added Subset import\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming Config is defined and the metadata processing (Step 1)\n",
        "# which defines speaker_metadata, speaker_id_to_int, gender_to_id,\n",
        "# accent_to_id, region_to_id, and age_to_id has been executed.\n",
        "\n",
        "# --- Step 2: VCTK Dataset Preparation (Mel Spectrograms) ---\n",
        "# Define the Mel Spectrogram transform (CPU-bound initially)\n",
        "mel_spectrogram_transform_cpu = T.MelSpectrogram( # Renamed for clarity\n",
        "    sample_rate=Config.SAMPLE_RATE,\n",
        "    n_fft=Config.N_FFT,\n",
        "    hop_length=Config.HOP_LENGTH,\n",
        "    n_mels=Config.N_MELS\n",
        ")\n",
        "\n",
        "# worker_transforms = {} # This is not needed as it's handled within the dataset object\n",
        "\n",
        "class VCTKMelDataset(Dataset):\n",
        "    def __init__(self, wav_dir, speaker_metadata, speaker_id_to_int,\n",
        "                 gender_to_id, accent_to_id, region_to_id, age_to_id, # Added region_to_id, age_to_id\n",
        "                 mel_transform_cpu):\n",
        "        self.wav_dir = wav_dir\n",
        "        self.speaker_metadata = speaker_metadata\n",
        "        self.speaker_id_to_int = speaker_id_to_int\n",
        "        self.gender_to_id = gender_to_id\n",
        "        self.accent_to_id = accent_to_id\n",
        "        self.region_to_id = region_to_id # Store new mappings\n",
        "        self.age_to_id = age_to_id       # Store new mappings\n",
        "\n",
        "        self.mel_transform_cpu = mel_transform_cpu\n",
        "\n",
        "        # These will be initialized per worker in worker_init_fn\n",
        "        # (or manually for num_workers=0)\n",
        "        self.mel_transform_on_device = None\n",
        "        self.resampler_cache = {}\n",
        "\n",
        "        self.data_samples = self._load_data_samples()\n",
        "\n",
        "    def _load_data_samples(self):\n",
        "        data_samples = []\n",
        "        print(\"Loading audio file paths and preparing dataset indices...\")\n",
        "        for speaker_id_from_metadata in tqdm(self.speaker_id_to_int.keys(), desc=\"Processing Speakers\"):\n",
        "            speaker_dir_name = speaker_id_from_metadata\n",
        "            if not speaker_dir_name.startswith('p'):\n",
        "                speaker_dir_name = 'p' + speaker_dir_name\n",
        "            speaker_path = os.path.join(self.wav_dir, speaker_dir_name)\n",
        "\n",
        "            if not os.path.isdir(speaker_path):\n",
        "                print(f\"Warning: Speaker directory {speaker_path} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Retrieve all metadata for the current speaker\n",
        "            speaker_info = self.speaker_metadata[speaker_id_from_metadata]\n",
        "\n",
        "            speaker_int_id = self.speaker_id_to_int[speaker_id_from_metadata]\n",
        "\n",
        "            gender_label = speaker_info['gender']\n",
        "            gender_int_id = self.gender_to_id[gender_label]\n",
        "\n",
        "            accent_label = speaker_info['accent']\n",
        "            accent_int_id = self.accent_to_id[accent_label]\n",
        "\n",
        "            # NEW: Get region and age labels and their integer IDs\n",
        "            region_label = speaker_info['region']\n",
        "            region_int_id = self.region_to_id[region_label]\n",
        "\n",
        "            age_label = speaker_info['age']\n",
        "            age_int_id = self.age_to_id[age_label]\n",
        "\n",
        "\n",
        "            for filename in os.listdir(speaker_path):\n",
        "                if filename.endswith(\".wav\"):\n",
        "                    file_path = os.path.join(speaker_path, filename)\n",
        "                    data_samples.append({\n",
        "                        'file_path': file_path,\n",
        "                        'speaker_id': speaker_int_id,\n",
        "                        'gender_id': gender_int_id,\n",
        "                        'accent_id': accent_int_id,\n",
        "                        'region_id': region_int_id, # Added\n",
        "                        'age_id': age_int_id         # Added\n",
        "                    })\n",
        "        print(f\"Finished loading {len(data_samples)} audio file paths.\")\n",
        "        return data_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure transforms are initialized. This block serves as a fallback for num_workers=0,\n",
        "        # but for num_workers > 0, worker_init_fn will handle it.\n",
        "        if self.mel_transform_on_device is None:\n",
        "            # print(f\"WARNING: Transforms not initialized by worker_init_fn. Doing it now in __getitem__ for main process.\")\n",
        "            self.mel_transform_on_device = self.mel_transform_cpu.to(Config.DEVICE)\n",
        "            self.resampler_cache = {}\n",
        "\n",
        "        sample_info = self.data_samples[idx]\n",
        "        file_path = sample_info['file_path']\n",
        "\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading {file_path}: {e}\")\n",
        "            raise # Re-raise to get full traceback\n",
        "\n",
        "        resampler_on_device = None\n",
        "        if sample_rate != Config.SAMPLE_RATE:\n",
        "            resampler_key = f\"{sample_rate}_to_{Config.SAMPLE_RATE}\"\n",
        "            if resampler_key not in self.resampler_cache:\n",
        "                self.resampler_cache[resampler_key] = T.Resample(orig_freq=sample_rate, new_freq=Config.SAMPLE_RATE).to(Config.DEVICE)\n",
        "            resampler_on_device = self.resampler_cache[resampler_key]\n",
        "\n",
        "        if resampler_on_device is not None:\n",
        "            waveform = resampler_on_device(waveform.to(Config.DEVICE))\n",
        "        else:\n",
        "            waveform = waveform.to(Config.DEVICE)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform[0, :].unsqueeze(0)\n",
        "\n",
        "        mel_spec = self.mel_transform_on_device(waveform)\n",
        "        mel_spec = torch.log(mel_spec + 1e-6)\n",
        "        mel_spec = mel_spec.squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'mel_spec': mel_spec.cpu(),\n",
        "            'speaker_id': torch.tensor(sample_info['speaker_id'], dtype=torch.long),\n",
        "            'gender_id': torch.tensor(sample_info['gender_id'], dtype=torch.long),\n",
        "            'accent_id': torch.tensor(sample_info['accent_id'], dtype=torch.long),\n",
        "            'region_id': torch.tensor(sample_info['region_id'], dtype=torch.long), # Added\n",
        "            'age_id': torch.tensor(sample_info['age_id'], dtype=torch.long)         # Added\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_time = max(item['mel_spec'].shape[1] for item in batch)\n",
        "    padded_mel_specs = []\n",
        "    for item in batch:\n",
        "        mel_spec = item['mel_spec']\n",
        "        padding_size = max_time - mel_spec.shape[1]\n",
        "        padded_mel = F.pad(mel_spec, (0, padding_size), \"constant\", 0)\n",
        "        padded_mel_specs.append(padded_mel)\n",
        "    batch_mel_specs = torch.stack(padded_mel_specs)\n",
        "\n",
        "    batch_speaker_ids = torch.stack([item['speaker_id'] for item in batch])\n",
        "    batch_gender_ids = torch.stack([item['gender_id'] for item in batch])\n",
        "    batch_accent_ids = torch.stack([item['accent_id'] for item in batch])\n",
        "    batch_region_ids = torch.stack([item['region_id'] for item in batch]) # Added\n",
        "    batch_age_ids = torch.stack([item['age_id'] for item in batch])       # Added\n",
        "\n",
        "    return {\n",
        "        'mel_spec': batch_mel_specs,\n",
        "        'speaker_id': batch_speaker_ids,\n",
        "        'gender_id': batch_gender_ids,\n",
        "        'accent_id': batch_accent_ids,\n",
        "        'region_id': batch_region_ids, # Added\n",
        "        'age_id': batch_age_ids         # Added\n",
        "    }\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    # print(f\"Worker {worker_id}: worker_init_fn started.\") # Uncomment for debugging\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    if worker_info is not None:\n",
        "        dataset_obj = worker_info.dataset\n",
        "        if isinstance(dataset_obj, Subset):\n",
        "            actual_dataset = dataset_obj.dataset\n",
        "        else:\n",
        "            actual_dataset = dataset_obj\n",
        "\n",
        "        # Move the mel transform to the correct device for this worker\n",
        "        actual_dataset.mel_transform_on_device = actual_dataset.mel_transform_cpu.to(Config.DEVICE)\n",
        "        # Initialize an empty cache for resamplers for this worker\n",
        "        actual_dataset.resampler_cache = {}\n",
        "        # print(f\"Worker {worker_id}: worker_init_fn finished.\") # Uncomment for debugging\n",
        "\n",
        "# --- Main execution of Step 2 ---\n",
        "# IMPORTANT: If num_workers=0, the worker_init_fn is NOT called.\n",
        "# In that case, you need to manually move the mel_transform to the device for the main process.\n",
        "if Config.NUM_WORKERS == 0:\n",
        "    print(\"Running with num_workers=0. Manually moving Mel transform to device for main process.\")\n",
        "    mel_spectrogram_transform_cpu = mel_spectrogram_transform_cpu.to(Config.DEVICE) # Move to device for main process\n",
        "\n",
        "vctk_dataset = VCTKMelDataset(\n",
        "    wav_dir=Config.WAV_DIR,\n",
        "    speaker_metadata=speaker_metadata,\n",
        "    speaker_id_to_int=speaker_id_to_int,\n",
        "    gender_to_id=gender_to_id,\n",
        "    accent_to_id=accent_to_id,\n",
        "    region_to_id=region_to_id, # Pass new mapping\n",
        "    age_to_id=age_to_id,       # Pass new mapping\n",
        "    mel_transform_cpu=mel_spectrogram_transform_cpu # Use the explicitly named CPU transform\n",
        ")\n",
        "\n",
        "vctk_dataloader = DataLoader(\n",
        "    vctk_dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Set back to Config.NUM_WORKERS for proper usage\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True if Config.DEVICE == 'cuda' else False, # Pin memory for GPU\n",
        "    worker_init_fn=worker_init_fn if Config.NUM_WORKERS > 0 else None # Only pass worker_init_fn if workers > 0\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset created with {len(vctk_dataset)} samples.\")\n",
        "print(f\"DataLoader created with {len(vctk_dataloader)} batches (batch size: {Config.BATCH_SIZE}).\")\n",
        "\n",
        "print(\"\\nTesting DataLoader...\")\n",
        "try:\n",
        "    for i, batch in enumerate(vctk_dataloader):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Mel Spectrograms shape: {batch['mel_spec'].shape} on {batch['mel_spec'].device}\")\n",
        "        print(f\"  Speaker IDs shape: {batch['speaker_id'].shape} on {batch['speaker_id'].device}\")\n",
        "        print(f\"  Gender IDs shape: {batch['gender_id'].shape} on {batch['gender_id'].device}\")\n",
        "        print(f\"  Accent IDs shape: {batch['accent_id'].shape} on {batch['accent_id'].device}\")\n",
        "        print(f\"  Region IDs shape: {batch['region_id'].shape} on {batch['region_id'].device}\") # Added\n",
        "        print(f\"  Age IDs shape: {batch['age_id'].shape} on {batch['age_id'].device}\")           # Added\n",
        "        if i == 0:\n",
        "            print(f\"  Example Speaker IDs: {batch['speaker_id'][:5].tolist()}\")\n",
        "            print(f\"  Example Gender IDs: {batch['gender_id'][:5].tolist()}\")\n",
        "            print(f\"  Example Accent IDs: {batch['accent_id'][:5].tolist()}\")\n",
        "            print(f\"  Example Region IDs: {batch['region_id'][:5].tolist()}\") # Added\n",
        "            print(f\"  Example Age IDs: {batch['age_id'][:5].tolist()}\")       # Added\n",
        "        break # Process only one batch for testing\n",
        "    print(\"DataLoader test successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"DataLoader test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print full traceback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSzkRaJ4nIrD",
        "outputId": "17d74a81-7c65-463f-d927-92576c2d026f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio file paths and preparing dataset indices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Speakers:  78%|███████▊  | 80/102 [00:41<00:19,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Speaker directory /content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental/wav48/p315 not found. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Speakers: 100%|██████████| 102/102 [00:51<00:00,  1.96it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished loading 1970 audio file paths.\n",
            "\n",
            "Dataset created with 1970 samples.\n",
            "DataLoader created with 62 batches (batch size: 32).\n",
            "\n",
            "Testing DataLoader...\n",
            "Batch 1:\n",
            "  Mel Spectrograms shape: torch.Size([32, 80, 876]) on cpu\n",
            "  Speaker IDs shape: torch.Size([32]) on cpu\n",
            "  Gender IDs shape: torch.Size([32]) on cpu\n",
            "  Accent IDs shape: torch.Size([32]) on cpu\n",
            "  Region IDs shape: torch.Size([32]) on cpu\n",
            "  Age IDs shape: torch.Size([32]) on cpu\n",
            "  Example Speaker IDs: [16, 79, 93, 65, 21]\n",
            "  Example Gender IDs: [1, 0, 0, 0, 0]\n",
            "  Example Accent IDs: [3, 8, 2, 0, 7]\n",
            "  Example Region IDs: [30, 8, 2, 7, 0]\n",
            "  Example Age IDs: [4, 8, 9, 7, 4]\n",
            "DataLoader test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from tqdm import tqdm\n",
        "import numpy as np # For random splitting\n",
        "\n",
        "# Assume Config, speaker_metadata, speaker_id_to_int, gender_to_id,\n",
        "# accent_to_id, region_to_id, age_to_id are already defined from Step 1.\n",
        "\n",
        "# Assume mel_spectrogram_transform_cpu is defined and VCTKMelDataset,\n",
        "# collate_fn, worker_init_fn are defined from Step 2.\n",
        "\n",
        "# Assume vc_encoders, gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "# age_criterion, region_criterion, optimizer are defined from Step 3 & 4.\n",
        "\n",
        "# --- Data Splitting Logic ---\n",
        "print(\"\\n--- Performing Dataset Splits (Train, In-Test, Out-Test) ---\")\n",
        "\n",
        "# First, create the full dataset\n",
        "vctk_dataset = VCTKMelDataset(\n",
        "    wav_dir=Config.WAV_DIR,\n",
        "    speaker_metadata=speaker_metadata,\n",
        "    speaker_id_to_int=speaker_id_to_int,\n",
        "    gender_to_id=gender_to_id,\n",
        "    accent_to_id=accent_to_id,\n",
        "    region_to_id=region_to_id,\n",
        "    age_to_id=age_to_id,\n",
        "    mel_transform_cpu=mel_spectrogram_transform_cpu # This should be the CPU-bound transform\n",
        ")\n",
        "\n",
        "# Get all unique speaker IDs that have data in the dataset\n",
        "# It's important to iterate through the data_samples to get speakers *actually present*\n",
        "all_speakers_in_dataset = list(set(s['speaker_id'] for s in vctk_dataset.data_samples))\n",
        "np.random.seed(Config.RANDOM_SEED) # Ensure reproducibility for splits\n",
        "np.random.shuffle(all_speakers_in_dataset)\n",
        "\n",
        "# Determine out-test speakers (e.g., 10% of total speakers)\n",
        "num_out_test_speakers = int(len(all_speakers_in_dataset) * 0.10)\n",
        "out_test_speaker_ids = set(all_speakers_in_dataset[:num_out_test_speakers])\n",
        "in_train_val_test_speaker_ids = set(all_speakers_in_dataset[num_out_test_speakers:])\n",
        "\n",
        "print(f\"Total speakers with data: {len(all_speakers_in_dataset)}\")\n",
        "print(f\"Out-test speakers ({num_out_test_speakers}): {[k for k,v in speaker_id_to_int.items() if v in out_test_speaker_ids]}\") # Map back to original IDs for readability\n",
        "\n",
        "# --- Create Indices for each split ---\n",
        "train_indices = []\n",
        "in_test_indices = [] # Seen speakers, unseen utterances\n",
        "out_test_indices = [] # Unseen speakers, unseen utterances\n",
        "\n",
        "# Dictionaries to hold indices for each speaker for \"in-test\" split\n",
        "speaker_utterance_indices = {speaker_id: [] for speaker_id in in_train_val_test_speaker_ids}\n",
        "\n",
        "# Populate indices\n",
        "for i, sample in enumerate(vctk_dataset.data_samples):\n",
        "    speaker_id = sample['speaker_id']\n",
        "    if speaker_id in out_test_speaker_ids:\n",
        "        out_test_indices.append(i)\n",
        "    elif speaker_id in in_train_val_test_speaker_ids:\n",
        "        speaker_utterance_indices[speaker_id].append(i)\n",
        "    # No 'else' needed as all_speakers_in_dataset covers all\n",
        "    # speakers that actually have data.\n",
        "\n",
        "# Now, split the in_train_val_test speakers' utterances into train and in-test\n",
        "for speaker_id, indices in speaker_utterance_indices.items():\n",
        "    np.random.shuffle(indices) # Shuffle utterances for each speaker\n",
        "    # For in-test, take a percentage of utterances from each speaker (e.g., 10%)\n",
        "    num_in_test_utterances = int(len(indices) * 0.10)\n",
        "    in_test_indices.extend(indices[:num_in_test_utterances])\n",
        "    train_indices.extend(indices[num_in_test_utterances:])\n",
        "\n",
        "# Further split 'train_indices' into actual train and validation sets\n",
        "# (e.g., 80% train, 20% validation from the train_indices)\n",
        "np.random.shuffle(train_indices) # Shuffle before splitting\n",
        "num_train_samples = int(len(train_indices) * Config.TRAIN_SPLIT_RATIO)\n",
        "actual_train_indices = train_indices[:num_train_samples]\n",
        "val_indices = train_indices[num_train_samples:]\n",
        "\n",
        "print(f\"Total samples: {len(vctk_dataset)}\")\n",
        "print(f\"Train samples: {len(actual_train_indices)}\")\n",
        "print(f\"Validation samples: {len(val_indices)}\")\n",
        "print(f\"In-Test samples (seen speakers, unseen utterances): {len(in_test_indices)}\")\n",
        "print(f\"Out-Test samples (unseen speakers, unseen utterances): {len(out_test_indices)}\")\n",
        "\n",
        "\n",
        "# Create Subset objects for each split\n",
        "train_dataset = Subset(vctk_dataset, actual_train_indices)\n",
        "val_dataset = Subset(vctk_dataset, val_indices)\n",
        "in_test_dataset = Subset(vctk_dataset, in_test_indices)\n",
        "out_test_dataset = Subset(vctk_dataset, out_test_indices)\n",
        "\n",
        "# --- Create DataLoaders for each split ---\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True if Config.DEVICE == 'cuda' else False,\n",
        "    worker_init_fn=worker_init_fn if Config.NUM_WORKERS > 0 else None\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True if Config.DEVICE == 'cuda' else False,\n",
        "    worker_init_fn=worker_init_fn if Config.NUM_WORKERS > 0 else None\n",
        ")\n",
        "\n",
        "in_test_dataloader = DataLoader(\n",
        "    in_test_dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True if Config.DEVICE == 'cuda' else False,\n",
        "    worker_init_fn=worker_init_fn if Config.NUM_WORKERS > 0 else None\n",
        ")\n",
        "\n",
        "out_test_dataloader = DataLoader(\n",
        "    out_test_dataset,\n",
        "    batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True if Config.DEVICE == 'cuda' else False,\n",
        "    worker_init_fn=worker_init_fn if Config.NUM_WORKERS > 0 else None\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created for Train ({len(train_dataloader)} batches), Validation ({len(val_dataloader)} batches),\")\n",
        "print(f\"In-Test ({len(in_test_dataloader)} batches), and Out-Test ({len(out_test_dataloader)} batches).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b3ANxszrEy-",
        "outputId": "f741d80e-6190-42ce-b9c8-4a7c4455f145"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performing Dataset Splits (Train, In-Test, Out-Test) ---\n",
            "Loading audio file paths and preparing dataset indices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Speakers: 100%|██████████| 102/102 [00:00<00:00, 1675.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Speaker directory /content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental/wav48/p315 not found. Skipping.\n",
            "Finished loading 1970 audio file paths.\n",
            "Total speakers with data: 101\n",
            "Out-test speakers (10): ['225', '236', '250', '268', '273', '274', '285', '300', '301', '326']\n",
            "Total samples: 1970\n",
            "Train samples: 1296\n",
            "Validation samples: 324\n",
            "In-Test samples (seen speakers, unseen utterances): 154\n",
            "Out-Test samples (unseen speakers, unseen utterances): 196\n",
            "\n",
            "DataLoaders created for Train (41 batches), Validation (11 batches),\n",
            "In-Test (5 batches), and Out-Test (7 batches).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Dataset paths\n",
        "    VC_DIR = \"/content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental\" # Base VCTK dataset directory\n",
        "    VC_Output = \"/content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2\" # Base VCTK dataset directory\n",
        "    WAV_DIR = os.path.join(VC_DIR, \"wav48\") # Directory containing wav files\n",
        "    METADATA_PATH = os.path.join(VC_DIR, \"speaker-info.txt\") # Speaker metadata file path\n",
        "\n",
        "    # Audio processing parameters\n",
        "    SAMPLE_RATE = 16000 # Target sample rate for all audio (Hz)\n",
        "    N_MELS = 80         # Number of Mel bands for spectrogram\n",
        "    N_FFT = 400         # FFT window size (samples)\n",
        "    HOP_LENGTH = 160    # Hop length (samples) between windows\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2    # Number of CPU workers for data loading\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    NUM_EPOCHS = 50\n",
        "    TRAIN_SPLIT_RATIO = 0.8 # 80% for training, 20% for validation\n",
        "    RANDOM_SEED = 42    # For reproducibility\n",
        "\n",
        "    # Device configuration\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Model parameters (Specific to this VC architecture)\n",
        "    Z_CONTENT_DIM = 256 # Dimension for speaker-invariant content embedding\n",
        "    Z_GENDER_DIM = 16   # Dimension for gender embedding\n",
        "    Z_ACCENT_DIM = 64   # Dimension for accent embedding\n",
        "    Z_SPEAKER_ID_DIM = 256 # Dimension for individual speaker ID embedding (like x-vector)\n",
        "    Z_AGE_DIM = 32      # Dimension for age embedding (you can adjust this)\n",
        "    Z_REGION_DIM = 64   # Dimension for region embedding (you can adjust this)\n"
      ],
      "metadata": {
        "id": "03IPPcrqyAov"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCi1EWdYwDty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**==========================================**\n",
        "\n",
        "**-------------Developing Decoder------------**\n",
        "\n",
        "**==========================================**"
      ],
      "metadata": {
        "id": "9bkyz5mJwFD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assuming ConvBlock, SharedAudioFeatureExtractor, GenderEncoder, AccentEncoder,\n",
        "# AgeEncoder, RegionEncoder, SpeakerIDEncoder, ContentEncoder, VoiceConversionEncoders\n",
        "# are defined as in your previous code snippet.\n",
        "# For self-contained example, I'll include the necessary ConvBlock again.\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"A simple Convolutional Block with optional Batch Normalization and LeakyReLU.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding,\n",
        "                 use_bn=True, use_dropout=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm1d(out_channels) if use_bn else None\n",
        "        self.relu = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(0.2) if use_dropout else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module to reconstruct Mel spectrogram from content and speaker embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_mels = config.N_MELS\n",
        "\n",
        "        # Calculate the total dimension of combined speaker embeddings\n",
        "        self.total_speaker_dim = (\n",
        "            config.Z_GENDER_DIM + config.Z_ACCENT_DIM +\n",
        "            config.Z_AGE_DIM + config.Z_REGION_DIM +\n",
        "            config.Z_SPEAKER_ID_DIM\n",
        "        )\n",
        "\n",
        "        # The input to the first decoder block will be z_content concatenated with\n",
        "        # the broadcasted combined speaker embedding.\n",
        "        initial_decoder_input_dim = config.Z_CONTENT_DIM + self.total_speaker_dim\n",
        "\n",
        "        # Decoder layers to progressively transform the features back to Mel spectrogram dimensions.\n",
        "        # We start with the combined latent space dimension and work towards N_MELS.\n",
        "        # Use ConvBlock for internal layers.\n",
        "        # The last layer should output N_MELS and typically doesn't have BN or ReLU\n",
        "        # as it directly produces the Mel values.\n",
        "\n",
        "        decoder_hidden_dims = [256, 128, 64] # Example dimensions, can be tuned\n",
        "\n",
        "        layers = []\n",
        "        in_c = initial_decoder_input_dim\n",
        "        for h_dim in decoder_hidden_dims:\n",
        "            layers.append(ConvBlock(in_c, h_dim, kernel_size=5, stride=1, padding=2))\n",
        "            in_c = h_dim\n",
        "\n",
        "        self.decoder_blocks = nn.Sequential(*layers)\n",
        "\n",
        "        # Final convolutional layer to map to N_MELS\n",
        "        # Using a kernel size of 5 and padding of 2 to maintain temporal dimension.\n",
        "        self.final_conv = nn.Conv1d(in_c, self.n_mels, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # Optional: Add a final activation if your Mel spectrograms are normalized\n",
        "        # e.g., if Mels are in [0, 1] use nn.Sigmoid(), if in [-1, 1] use nn.Tanh()\n",
        "        # If they are raw log-mels, a linear output (no activation) is typical.\n",
        "        # self.final_activation = nn.Sigmoid() # Uncomment and adjust if needed\n",
        "\n",
        "    def forward(self, z_content, z_gender, z_accent, z_age, z_region, z_speaker_id):\n",
        "        # z_content shape: (Batch, Z_CONTENT_DIM, N_FRAMES)\n",
        "        # z_speaker_attrs shapes: (Batch, Z_ATTR_DIM) - static vectors\n",
        "\n",
        "        # 1. Get the number of frames from content embedding\n",
        "        batch_size, _, n_frames = z_content.shape\n",
        "\n",
        "        # 2. Concatenate all speaker embeddings into a single combined vector\n",
        "        # Resulting shape: (Batch, total_speaker_dim)\n",
        "        z_speaker_combined = torch.cat(\n",
        "            (z_gender, z_accent, z_age, z_region, z_speaker_id),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # 3. Broadcast the combined speaker embedding to match content's temporal dimension\n",
        "        # (Batch, total_speaker_dim) -> (Batch, total_speaker_dim, 1)\n",
        "        # -> (Batch, total_speaker_dim, N_FRAMES)\n",
        "        z_speaker_broadcast = z_speaker_combined.unsqueeze(-1).expand(-1, -1, n_frames)\n",
        "\n",
        "        # 4. Concatenate z_content and the broadcasted speaker embedding\n",
        "        # Resulting shape: (Batch, Z_CONTENT_DIM + total_speaker_dim, N_FRAMES)\n",
        "        decoder_input = torch.cat((z_content, z_speaker_broadcast), dim=1)\n",
        "\n",
        "        # 5. Pass through the decoder convolutional blocks\n",
        "        x = self.decoder_blocks(decoder_input)\n",
        "\n",
        "        # 6. Final convolution to output N_MELS\n",
        "        output_mel = self.final_conv(x)\n",
        "\n",
        "        # 7. Apply optional final activation\n",
        "        # if hasattr(self, 'final_activation'):\n",
        "        #     output_mel = self.final_activation(output_mel)\n",
        "\n",
        "        return output_mel\n",
        "\n",
        "# --- Integration with your existing Encoders and Example Usage ---\n",
        "\n",
        "# (Re-using your VoiceConversionEncoders and Config placeholder)\n",
        "\n",
        "# Placeholder for Config (should match your actual Config object)\n",
        "\n",
        "# Placeholder for your encoders (just for the example to run)\n",
        "# (You already have these defined above, so you can omit this block in your main file)\n",
        "class SharedAudioFeatureExtractor(nn.Module):\n",
        "    def __init__(self, n_mels, hidden_dims=[128, 256, 256]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_c = n_mels\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(ConvBlock(in_c, h_dim, kernel_size=5, stride=2, padding=2))\n",
        "            in_c = h_dim\n",
        "        self.conv_blocks = nn.Sequential(*layers)\n",
        "        self.out_channels = hidden_dims[-1]\n",
        "    def forward(self, mel_spec): return self.conv_blocks(mel_spec)\n",
        "\n",
        "class AttributeEncoder(nn.Module): # Using the refactored AttributeEncoder\n",
        "    def __init__(self, in_channels, num_classes, z_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_convs = nn.Sequential(\n",
        "            ConvBlock(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ConvBlock(in_channels, in_channels // 2, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_embedding = nn.Linear(in_channels // 2, z_dim)\n",
        "        self.classifier = nn.Linear(z_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_convs(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        z_attr = self.linear_embedding(x)\n",
        "        attr_logits = self.classifier(z_attr)\n",
        "        return z_attr, attr_logits\n",
        "\n",
        "class SpeakerIDEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_speakers, z_speaker_id_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            ConvBlock(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            ConvBlock(in_channels, in_channels * 2, kernel_size=3, stride=1, padding=1),\n",
        "            ConvBlock(in_channels * 2, in_channels * 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.AdaptiveAvgPool1d(1), # Global average pooling\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels * 2, z_speaker_id_dim) # Output speaker ID embedding\n",
        "        )\n",
        "        self.classifier = nn.Linear(z_speaker_id_dim, num_speakers) # For classification/verification objective\n",
        "    def forward(self, x):\n",
        "        z_speaker_id = self.encoder(x)\n",
        "        speaker_logits = self.classifier(z_speaker_id)\n",
        "        return z_speaker_id, speaker_logits\n",
        "\n",
        "class ContentEncoder(nn.Module):\n",
        "    def __init__(self, n_mels, z_content_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            ConvBlock(n_mels, 128, kernel_size=5, stride=1, padding=2),\n",
        "            ConvBlock(128, 128, kernel_size=5, stride=1, padding=2),\n",
        "            ConvBlock(128, z_content_dim, kernel_size=5, stride=1, padding=2, use_bn=False),\n",
        "        )\n",
        "    def forward(self, mel_spec): return self.encoder(mel_spec)\n",
        "\n",
        "class VoiceConversionEncoders(nn.Module):\n",
        "    def __init__(self, config, num_genders, num_accents, num_speakers, num_ages, num_regions):\n",
        "        super().__init__()\n",
        "        self.shared_extractor = SharedAudioFeatureExtractor(config.N_MELS)\n",
        "        shared_out_channels = self.shared_extractor.out_channels\n",
        "\n",
        "        # Using the refactored AttributeEncoder\n",
        "        self.gender_encoder = AttributeEncoder(shared_out_channels, num_genders, config.Z_GENDER_DIM)\n",
        "        self.accent_encoder = AttributeEncoder(shared_out_channels, num_accents, config.Z_ACCENT_DIM)\n",
        "        self.age_encoder = AttributeEncoder(shared_out_channels, num_ages, config.Z_AGE_DIM)\n",
        "        self.region_encoder = AttributeEncoder(shared_out_channels, num_regions, config.Z_REGION_DIM)\n",
        "\n",
        "        self.speaker_id_encoder = SpeakerIDEncoder(shared_out_channels, num_speakers, config.Z_SPEAKER_ID_DIM)\n",
        "        self.content_encoder = ContentEncoder(config.N_MELS, config.Z_CONTENT_DIM)\n",
        "\n",
        "    def forward(self, mel_spec):\n",
        "        shared_features = self.shared_extractor(mel_spec)\n",
        "        pooled_shared_features = F.adaptive_avg_pool1d(shared_features, 1).squeeze(-1) # (Batch, C)\n",
        "\n",
        "        # Gender\n",
        "        z_gender, gender_logits = self.gender_encoder(pooled_shared_features.unsqueeze(-1))\n",
        "        # Accent\n",
        "        z_accent, accent_logits = self.accent_encoder(pooled_shared_features.unsqueeze(-1))\n",
        "        # Speaker ID\n",
        "        z_speaker_id, speaker_id_logits = self.speaker_id_encoder(pooled_shared_features.unsqueeze(-1))\n",
        "        # Age\n",
        "        z_age, age_logits = self.age_encoder(pooled_shared_features.unsqueeze(-1))\n",
        "        # Region\n",
        "        z_region, region_logits = self.region_encoder(pooled_shared_features.unsqueeze(-1))\n",
        "\n",
        "        # Content Encoder (operates on raw Mel, aims to be speaker-independent)\n",
        "        z_content = self.content_encoder(mel_spec)\n",
        "\n",
        "        return {\n",
        "            'z_content': z_content,\n",
        "            'z_gender': z_gender,\n",
        "            'gender_logits': gender_logits,\n",
        "            'z_accent': z_accent,\n",
        "            'accent_logits': accent_logits,\n",
        "            'z_speaker_id': z_speaker_id,\n",
        "            'speaker_id_logits': speaker_id_logits,\n",
        "            'z_age': z_age,\n",
        "            'age_logits': age_logits,\n",
        "            'z_region': z_region,\n",
        "            'region_logits': region_logits,\n",
        "        }\n",
        "\n",
        "\n",
        "# --- Full Model Integration for Testing ---\n",
        "class VCModel(nn.Module):\n",
        "    def __init__(self, config, num_genders, num_accents, num_speakers, num_ages, num_regions):\n",
        "        super().__init__()\n",
        "        self.encoders = VoiceConversionEncoders(config, num_genders, num_accents, num_speakers, num_ages, num_regions)\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "    def forward(self, source_mel_spec, target_gender=None, target_accent=None,\n",
        "                target_age=None, target_region=None, target_speaker_id=None):\n",
        "        # 1. Encode the source Mel spectrogram\n",
        "        encoded_outputs = self.encoders(source_mel_spec)\n",
        "\n",
        "        # 2. Extract content from source\n",
        "        z_content = encoded_outputs['z_content']\n",
        "\n",
        "        # 3. Determine target speaker attributes for decoding\n",
        "        # If specific targets are provided, use them. Otherwise, use the source's attributes.\n",
        "        # This is crucial for training (reconstruction) and inference (conversion).\n",
        "        z_gender_dec = target_gender if target_gender is not None else encoded_outputs['z_gender']\n",
        "        z_accent_dec = target_accent if target_accent is not None else encoded_outputs['z_accent']\n",
        "        z_age_dec = target_age if target_age is not None else encoded_outputs['z_age']\n",
        "        z_region_dec = target_region if target_region is not None else encoded_outputs['z_region']\n",
        "        z_speaker_id_dec = target_speaker_id if target_speaker_id is not None else encoded_outputs['z_speaker_id']\n",
        "\n",
        "        # 4. Decode\n",
        "        reconstructed_mel = self.decoder(\n",
        "            z_content,\n",
        "            z_gender_dec,\n",
        "            z_accent_dec,\n",
        "            z_age_dec,\n",
        "            z_region_dec,\n",
        "            z_speaker_id_dec\n",
        "        )\n",
        "\n",
        "        # Return both the reconstruction and the original encoded outputs for loss calculations\n",
        "        return reconstructed_mel, encoded_outputs\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Dummy data for testing\n",
        "    # Replace these with actual counts from your dataset metadata\n",
        "    num_genders = len(gender_to_id)\n",
        "    num_accents = len(accent_to_id)\n",
        "    num_speakers = len(speaker_id_to_int) # Total unique speakers\n",
        "    num_ages = len(age_to_id)\n",
        "    num_regions = len(region_to_id)\n",
        "\n",
        "    print(f\"Detected {num_genders} unique genders: {list(gender_to_id.keys())}\")\n",
        "    print(f\"Detected {num_accents} unique accents: {list(accent_to_id.keys())}\")\n",
        "    print(f\"Detected {num_speakers} unique speakers.\")\n",
        "    print(f\"Detected {num_ages} unique ages: {list(age_to_id.keys())}\")\n",
        "    print(f\"Detected {num_regions} unique regions: {list(region_to_id.keys())}\")\n",
        "\n",
        "    batch_size = Config.BATCH_SIZE\n",
        "    n_mels = Config.N_MELS\n",
        "    max_frames = 200 # Example max frames\n",
        "    dummy_mel_spec = torch.randn(batch_size, n_mels, max_frames).to(Config.DEVICE)\n",
        "\n",
        "    # Initialize the full VCModel\n",
        "    model = VCModel(Config, num_genders, num_accents, num_speakers, num_ages, num_regions).to(Config.DEVICE)\n",
        "\n",
        "    print(f\"Model initialized on device: {Config.DEVICE}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    ## Training mode\n",
        "    # --- Example 1: Reconstruction (Source speaker and content) ---\n",
        "    print(\"\\n--- Testing Reconstruction (source voice) ---\")\n",
        "    reconstructed_mel_source, encoded_outputs_source = model(dummy_mel_spec)\n",
        "\n",
        "    print(f\"Input Mel spec shape: {dummy_mel_spec.shape}\")\n",
        "    print(f\"Reconstructed Mel spec shape (source voice): {reconstructed_mel_source.shape}\")\n",
        "    print(\"\\nEncoder Outputs (source):\")\n",
        "    for key, value in encoded_outputs_source.items():\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            print(f\"  {key}: shape={value.shape}\")\n",
        "\n",
        "    ####### MRH: Infernce with zero shot\n",
        "\n",
        "    # --- Example 2: Voice Conversion (Target speaker attributes) ---\n",
        "    print(\"\\n--- Testing Voice Conversion (target voice) ---\")\n",
        "\n",
        "\n",
        "    # For conversion, you would typically get target embeddings from a separate reference audio\n",
        "    # or by sampling/picking from a learned embedding space.\n",
        "    # For this dummy example, let's just create random target embeddings.\n",
        "    target_z_gender = torch.randn(batch_size, Config.Z_GENDER_DIM).to(Config.DEVICE)\n",
        "    target_z_accent = torch.randn(batch_size, Config.Z_ACCENT_DIM).to(Config.DEVICE)\n",
        "    target_z_age = torch.randn(batch_size, Config.Z_AGE_DIM).to(Config.DEVICE)\n",
        "    target_z_region = torch.randn(batch_size, Config.Z_REGION_DIM).to(Config.DEVICE)\n",
        "    target_z_speaker_id = torch.randn(batch_size, Config.Z_SPEAKER_ID_DIM).to(Config.DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "    # MRH: following line is false and just done to let code progress\n",
        "    source_mel_spec= reconstructed_mel_source\n",
        "    #\n",
        "    converted_mel, _ = model(\n",
        "        source_mel_spec,\n",
        "        target_gender=target_z_gender,\n",
        "        target_accent=target_z_accent,\n",
        "        target_age=target_z_age,\n",
        "        target_region=target_z_region,\n",
        "        target_speaker_id=target_z_speaker_id\n",
        "    )\n",
        "    print(f\"Converted Mel spec shape (target voice): {converted_mel.shape}\")\n",
        "\n",
        "    print(\"\\nDecoder architecture:\")\n",
        "    print(model.decoder) # Uncomment to see full decoder architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElqfl0fwPQt",
        "outputId": "56f0fa58-a3fe-4084-b952-59905cd57939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 2 unique genders: ['F', 'M']\n",
            "Detected 10 unique accents: ['American', 'Australian', 'Canadian', 'English', 'Irish', 'NewZealand', 'NorthernIrish', 'Scottish', 'SouthAfrican', 'Welsh']\n",
            "Detected 102 unique speakers.\n",
            "Detected 15 unique ages: ['18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '32', '33', '38']\n",
            "Detected 67 unique regions: ['Aberdeen', 'Alabama', 'Alberta', 'Argyll', 'Athlone', 'Belfast', 'Birmingham', 'California', 'Cape Town', 'Cardiff', 'Cheshire', 'Chicago', 'Cork', 'County Down', 'Cumbria', 'Derry', 'Donegal', 'Dublin', 'Edinburgh', 'English', 'English Sydney', 'Essex', 'Fife', 'Florida', 'Galloway', 'Hamilton', 'Indiana', 'Iowa', 'Johannesburg', 'Leicester', 'London', 'Manchester', 'Midlothian', 'Montreal', 'NE England', 'Napa', 'New England', 'New Jersey', 'New York', 'Newcastle', 'North Carolina', 'Nottingham', 'Ohio', 'Ontario', 'Orkney', 'Oxford', 'Pennsylvania', 'Perth', 'Philadelphia', 'Pretoria', 'Ross', 'SE England', 'SW England', 'San Francisco', 'Selkirk', 'Southern England', 'Staffordshire', 'Stockton-on-tees', 'Suffolk', 'Surrey', 'Tennessee', 'Tipperary', 'Toronto', 'West Dumfries', 'West Lothian', 'York', 'Yorkshire']\n",
            "Model initialized on device: cuda\n",
            "Total parameters: 4,705,220\n",
            "\n",
            "--- Testing Reconstruction (source voice) ---\n",
            "Input Mel spec shape: torch.Size([32, 80, 200])\n",
            "Reconstructed Mel spec shape (source voice): torch.Size([32, 80, 200])\n",
            "\n",
            "Encoder Outputs (source):\n",
            "  z_content: shape=torch.Size([32, 256, 200])\n",
            "  z_gender: shape=torch.Size([32, 16])\n",
            "  gender_logits: shape=torch.Size([32, 2])\n",
            "  z_accent: shape=torch.Size([32, 64])\n",
            "  accent_logits: shape=torch.Size([32, 10])\n",
            "  z_speaker_id: shape=torch.Size([32, 256])\n",
            "  speaker_id_logits: shape=torch.Size([32, 102])\n",
            "  z_age: shape=torch.Size([32, 32])\n",
            "  age_logits: shape=torch.Size([32, 15])\n",
            "  z_region: shape=torch.Size([32, 64])\n",
            "  region_logits: shape=torch.Size([32, 67])\n",
            "\n",
            "--- Testing Voice Conversion (target voice) ---\n",
            "Converted Mel spec shape (target voice): torch.Size([32, 80, 200])\n",
            "\n",
            "Decoder architecture:\n",
            "Decoder(\n",
            "  (decoder_blocks): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv1d(688, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "  )\n",
            "  (final_conv): Conv1d(64, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=========================**\n",
        "\n",
        "**------ Losses--------------**\n",
        "\n",
        "**=============================**"
      ],
      "metadata": {
        "id": "5ch7LF9a-HPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "\n",
        "# --- Gradient Reversal Layer ---\n",
        "class GradReverse(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Gradients for the input 'x' are reversed and scaled by 'alpha'\n",
        "        return grad_output.neg() * ctx.alpha, None\n",
        "\n",
        "class GradientReversalLayer(nn.Module):\n",
        "    def __init__(self, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return GradReverse.apply(x, self.alpha)\n",
        "\n",
        "# --- Content Speaker Discriminator ---\n",
        "# This discriminator tries to predict the speaker ID from the content embedding (z_content)\n",
        "# The content encoder will try to fool this discriminator.\n",
        "class ContentSpeakerDiscriminator(nn.Module):\n",
        "    def __init__(self, z_content_dim, num_speakers):\n",
        "        super().__init__()\n",
        "        # z_content shape: (Batch, Z_CONTENT_DIM, N_FRAMES)\n",
        "        # We want to classify the speaker based on this sequence.\n",
        "        # Use convolutional layers to process the sequence, then pool and classify.\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            ConvBlock(z_content_dim, z_content_dim // 2, kernel_size=5, stride=1, padding=2),\n",
        "            ConvBlock(z_content_dim // 2, z_content_dim // 4, kernel_size=5, stride=1, padding=2),\n",
        "            # Add more layers if needed for complexity\n",
        "        )\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1) # Pool over the temporal dimension\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_classifier = nn.Sequential(\n",
        "            nn.Linear(z_content_dim // 4, 128), # Adjust input features based on conv_blocks output\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, num_speakers)\n",
        "        )\n",
        "\n",
        "    def forward(self, z_content):\n",
        "        x = self.conv_blocks(z_content)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        speaker_logits = self.linear_classifier(x)\n",
        "        return speaker_logits"
      ],
      "metadata": {
        "id": "Jrkf1rvl9QsU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Full Model Integration for Testing ---\n",
        "class VCModel(nn.Module):\n",
        "    def __init__(self, config, num_genders, num_accents, num_speakers, num_ages, num_regions):\n",
        "        super().__init__()\n",
        "        self.encoders = VoiceConversionEncoders(config, num_genders, num_accents, num_speakers, num_ages, num_regions)\n",
        "        self.decoder = Decoder(config)\n",
        "\n",
        "        # Content disentanglement components\n",
        "        self.gradient_reversal = GradientReversalLayer(alpha=1.0) # alpha can be tuned, often between 0.1 and 1.0\n",
        "        self.content_speaker_discriminator = ContentSpeakerDiscriminator(\n",
        "            config.Z_CONTENT_DIM, num_speakers\n",
        "        )\n",
        "\n",
        "    def forward(self, source_mel_spec, target_gender=None, target_accent=None,\n",
        "                target_age=None, target_region=None, target_speaker_id=None):\n",
        "        # 1. Encode the source Mel spectrogram\n",
        "        encoded_outputs = self.encoders(source_mel_spec)\n",
        "\n",
        "        # 2. Extract content from source\n",
        "        z_content = encoded_outputs['z_content']\n",
        "\n",
        "        # 3. Determine target speaker attributes for decoding\n",
        "        # If specific targets are provided, use them. Otherwise, use the source's attributes.\n",
        "        # This is crucial for training (reconstruction) and inference (conversion).\n",
        "        z_gender_dec = target_gender if target_gender is not None else encoded_outputs['z_gender']\n",
        "        z_accent_dec = target_accent if target_accent is not None else encoded_outputs['z_accent']\n",
        "        z_age_dec = target_age if target_age is not None else encoded_outputs['z_age']\n",
        "        z_region_dec = target_region if target_region is not None else encoded_outputs['z_region']\n",
        "        z_speaker_id_dec = target_speaker_id if target_speaker_id is not None else encoded_outputs['z_speaker_id']\n",
        "\n",
        "        # 4. Decode\n",
        "        reconstructed_mel = self.decoder(\n",
        "            z_content,\n",
        "            z_gender_dec,\n",
        "            z_accent_dec,\n",
        "            z_age_dec,\n",
        "            z_region_dec,\n",
        "            z_speaker_id_dec\n",
        "        )\n",
        "\n",
        "        # 5. Content Disentanglement for the ContentEncoder\n",
        "        # Apply GRL to z_content. Gradients are reversed for the ContentEncoder only.\n",
        "        reversed_z_content = self.gradient_reversal(z_content)\n",
        "        # Discriminator tries to predict speaker ID from the reversed z_content\n",
        "        content_speaker_discriminator_logits = self.content_speaker_discriminator(reversed_z_content)\n",
        "\n",
        "        # Add these logits to encoded_outputs for loss calculation in the training loop\n",
        "        encoded_outputs['content_speaker_discriminator_logits'] = content_speaker_discriminator_logits\n",
        "\n",
        "\n",
        "        # Return both the reconstruction and the original encoded outputs for loss calculations\n",
        "        return reconstructed_mel, encoded_outputs"
      ],
      "metadata": {
        "id": "xShJzskB9eTb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In your loss definitions\n"
      ],
      "metadata": {
        "id": "T7ew-kDs81-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Define Loss Functions and Optimizers ---\n",
        "\n",
        "# Reconstruction Loss (for the Decoder output)\n",
        "reconstruction_criterion = nn.L1Loss() # Common for spectrograms, produces less blurry results than MSE\n",
        "\n",
        "# Attribute Classification Losses (for the AttributeEncoders and SpeakerIDEncoder)\n",
        "gender_criterion = nn.CrossEntropyLoss()\n",
        "accent_criterion = nn.CrossEntropyLoss()\n",
        "speaker_id_criterion = nn.CrossEntropyLoss()\n",
        "age_criterion = nn.CrossEntropyLoss()\n",
        "region_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Content Disentanglement Loss (for the ContentEncoder's adversarial training)\n",
        "# This uses CrossEntropyLoss because the discriminator tries to classify speaker ID.\n",
        "content_disentanglement_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "N4OIZ4Um-XcQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "zpkUrclNy_0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the full VCModel\n",
        "# num_genders, num_accents, etc. should be dynamically determined from your metadata script\n",
        "# as you previously showed.\n",
        "model = VCModel(Config, num_genders, num_accents, num_speakers, num_ages, num_regions).to(Config.DEVICE)\n",
        "\n",
        "\n",
        "# Optimizer: Now optimizing ALL parameters of the combined model (encoders + decoder + discriminator)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "print(\"\\nLoss functions and Optimizer defined.\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__} with learning rate {Config.LEARNING_RATE} and weight decay {Config.WEIGHT_DECAY}\")\n",
        "\n",
        "# Optional: Print number of trainable parameters for the *entire* model\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in the model: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5XlIvrHyS2",
        "outputId": "f31a1e5c-62fc-41e1-f9ed-cb204794306a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss functions and Optimizer defined.\n",
            "Optimizer: AdamW with learning rate 0.0001 and weight decay 1e-05\n",
            "Total trainable parameters in the model: 4,932,074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZURIffHjQQEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**==============================**\n",
        "\n",
        "**-------------Evaluation Metrics-------------**\n",
        "\n",
        "**==============================**"
      ],
      "metadata": {
        "id": "aieKKh6aRmFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa\n",
        "!pip install fastdtw # A faster implementation of Dynamic Time Warping\n",
        "!pip install python_speech_features # For alternative MFCC calculation if preferred, though librosa is common\n",
        "!pip install numpy scipy # Should be pre-installed but good to ensure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTkTkVyRRv2v",
        "outputId": "55e0fc6d-18e9-49cd-ebcb-5cb2b4de44c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastdtw) (2.0.2)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp312-cp312-linux_x86_64.whl size=567857 sha256=c6e18c7934106a49876db41a5263f1acc414791bdf8a978cb98c9be44558cf0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/d0/26/b82cb0f49ae73e5e6bba4e8462fff2c9851d7bd2ec64f8891e\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n",
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5868 sha256=a6205c0fa7a7f8ab12a2c9e5144ae02978edcd2e28d26a1eeb64cab063ef4fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/90/3c/4b5996a95d363fa14525597a19146a940bec467b44b2a14580\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features\n",
            "Successfully installed python_speech_features-0.6\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1: MCD_Evaluator.py\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "class MCDEvaluator:\n",
        "    def __init__(self, sr=Config.SAMPLE_RATE, n_mels=Config.N_MELS, n_mfcc=12, device=Config.DEVICE):\n",
        "        \"\"\"\n",
        "        Initializes the MCD Evaluator.\n",
        "        Args:\n",
        "            sr (int): Sample rate.\n",
        "            n_mels (int): Number of Mel bands for original Mel specs.\n",
        "            n_mfcc (int): Number of MFCC coefficients to extract. Common values are 12, 13, 20.\n",
        "            device (torch.device): Device to perform computations on.\n",
        "        \"\"\"\n",
        "        self.sr = sr\n",
        "        self.n_mels = n_mels\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.device = device\n",
        "\n",
        "    def _mel_to_mfcc(self, mel_spec_tensor):\n",
        "        \"\"\"\n",
        "        Converts a Mel spectrogram tensor to MFCCs.\n",
        "        Args:\n",
        "            mel_spec_tensor (torch.Tensor): A Mel spectrogram tensor (n_mels, num_frames).\n",
        "        Returns:\n",
        "            np.ndarray: MFCC coefficients (n_mfcc, num_frames).\n",
        "        \"\"\"\n",
        "        # Move to CPU and convert to NumPy\n",
        "        mel_spec_np = mel_spec_tensor.cpu().numpy()\n",
        "\n",
        "        # Convert Mel Spectrogram to power spectrogram (if it's log-Mel)\n",
        "        # Assuming your Mel specs are log-Mel for typical neural network outputs\n",
        "        # If your model outputs linear Mel, adjust this part.\n",
        "        power_spec_np = librosa.db_to_power(mel_spec_np)\n",
        "\n",
        "        # Convert power spectrogram to MFCCs\n",
        "        # librosa.feature.mfcc expects a power spectrogram\n",
        "        mfccs = librosa.feature.mfcc(S=power_spec_np, sr=self.sr, n_mfcc=self.n_mfcc)\n",
        "        return mfccs.T # Transpose to (num_frames, n_mfcc)\n",
        "\n",
        "    def calculate_mcd(self, mel_original, mel_reconstructed):\n",
        "        \"\"\"\n",
        "        Calculates Mel Cepstral Distortion between two Mel spectrograms.\n",
        "        Args:\n",
        "            mel_original (torch.Tensor): Original Mel spectrogram (n_mels, num_frames).\n",
        "            mel_reconstructed (torch.Tensor): Reconstructed Mel spectrogram (n_mels, num_frames).\n",
        "        Returns:\n",
        "            float: The MCD value.\n",
        "        \"\"\"\n",
        "        # Convert Mels to MFCCs\n",
        "        mfcc_original = self._mel_to_mfcc(mel_original)\n",
        "        mfcc_reconstructed = self._mel_to_mfcc(mel_reconstructed)\n",
        "\n",
        "        # Align MFCCs using Dynamic Time Warping (DTW)\n",
        "        # fastdtw expects (N, M) arrays where N is sequence length, M is feature dim\n",
        "        distance, path = fastdtw(mfcc_original, mfcc_reconstructed, dist=euclidean)\n",
        "\n",
        "        # Calculate average Euclidean distance (MCD)\n",
        "        # The 'distance' from fastdtw is the sum of distances along the optimal path.\n",
        "        # We need to normalize it by the path length.\n",
        "        mcd_value = distance / len(path)\n",
        "        return mcd_value\n",
        "\n",
        "    def evaluate(self, model, dataloader):\n",
        "        \"\"\"\n",
        "        Evaluates the model's reconstruction quality using MCD.\n",
        "        Args:\n",
        "            model (torch.nn.Module): Your VCModel instance.\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for validation/test set.\n",
        "        Returns:\n",
        "            float: Average MCD across the dataset.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        total_mcd = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                mel_spec = batch['mel_spec'].to(self.device)\n",
        "\n",
        "                # Get reconstructed Mel from the model\n",
        "                # Assuming model's forward pass gives reconstructed_mel\n",
        "                reconstructed_mel, _ = model(mel_spec)\n",
        "\n",
        "                # Process each sample in the batch\n",
        "                for i in range(mel_spec.shape[0]):\n",
        "                    # Remove padding if necessary\n",
        "                    original_len = (mel_spec[i].sum(dim=0) != 0).sum() # Find true length by non-zero frames\n",
        "                    reco_len = (reconstructed_mel[i].sum(dim=0) != 0).sum() # Find true length\n",
        "\n",
        "                    original_sample_mel = mel_spec[i, :, :original_len]\n",
        "                    reco_sample_mel = reconstructed_mel[i, :, :reco_len] # Apply same truncation\n",
        "\n",
        "                    # Ensure both are 2D (n_mels, num_frames)\n",
        "                    if original_sample_mel.ndim == 3: original_sample_mel = original_sample_mel.squeeze(0)\n",
        "                    if reco_sample_mel.ndim == 3: reco_sample_mel = reco_sample_mel.squeeze(0)\n",
        "\n",
        "                    if original_sample_mel.shape[1] == 0 or reco_sample_mel.shape[1] == 0:\n",
        "                        continue # Skip empty samples\n",
        "\n",
        "                    mcd = self.calculate_mcd(original_sample_mel, reco_sample_mel)\n",
        "                    total_mcd += mcd\n",
        "                    num_samples += 1\n",
        "\n",
        "        if num_samples == 0:\n",
        "            return 0.0 # Avoid division by zero\n",
        "        return total_mcd / num_samples\n",
        "\n",
        "print(\"MCDEvaluator class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVmtprrvSAHf",
        "outputId": "971b801e-d57b-4972-8d28-e7462fb0cb34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCDEvaluator class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 2: SpeakerIDConsistencyEvaluator.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpeakerIDConsistencyEvaluator:\n",
        "    def __init__(self, device=Config.DEVICE):\n",
        "        \"\"\"\n",
        "        Initializes the Speaker ID Consistency Evaluator.\n",
        "        Args:\n",
        "            device (torch.device): Device to perform computations on.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate(self, model, dataloader):\n",
        "        \"\"\"\n",
        "        Evaluates the consistency of speaker ID embeddings during reconstruction.\n",
        "        Calculates cosine similarity between z_speaker_id from original and reconstructed Mels.\n",
        "        Args:\n",
        "            model (torch.nn.Module): Your VCModel instance.\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for validation/test set.\n",
        "        Returns:\n",
        "            float: Average cosine similarity across the dataset.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        total_cosine_sim = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                mel_spec = batch['mel_spec'].to(self.device)\n",
        "\n",
        "                # Get reconstructed Mel and original speaker ID embedding (z_speaker_id from input)\n",
        "                reconstructed_mel, encoded_outputs = model(mel_spec)\n",
        "\n",
        "                # Extract z_speaker_id from the original Mel (from the encoder)\n",
        "                z_speaker_id_original = encoded_outputs['z_speaker_id']\n",
        "                # If z_speaker_id_original is (Batch, Z_DIM, N_FRAMES), average over frames\n",
        "                if z_speaker_id_original.ndim == 3:\n",
        "                    z_speaker_id_original = z_speaker_id_original.mean(dim=-1) # (Batch, Z_DIM)\n",
        "\n",
        "                # We need to compute speaker embeddings for each reconstructed sample independently\n",
        "                # to get a 1-to-1 comparison with the original embeddings for that sample.\n",
        "\n",
        "                z_speaker_id_reconstructed_batch = []\n",
        "                for i in range(reconstructed_mel.shape[0]): # Iterate over samples in batch\n",
        "                    # Ensure the sample is 3D (1, N_MELS, T) for processing\n",
        "                    reco_sample_mel_i = reconstructed_mel[i].unsqueeze(0)\n",
        "\n",
        "                    # --- CRITICAL FIX: Pass through shared_extractor and pool first ---\n",
        "                    # 1. Process with shared_extractor\n",
        "                    shared_features_reconstructed_i = model.encoders.shared_extractor(reco_sample_mel_i)\n",
        "\n",
        "                    # 2. Apply adaptive average pooling and unsqueeze as per VoiceConversionEncoders.forward\n",
        "                    pooled_shared_features_reconstructed_i = F.adaptive_avg_pool1d(shared_features_reconstructed_i, 1).squeeze(-1)\n",
        "                    input_to_speaker_id_encoder_i = pooled_shared_features_reconstructed_i.unsqueeze(-1) # Shape (1, C, 1)\n",
        "\n",
        "                    # 3. Now pass the correctly formatted input to speaker_id_encoder\n",
        "                    z_reco_sample_embedding, _ = model.encoders.speaker_id_encoder(input_to_speaker_id_encoder_i)\n",
        "\n",
        "                    # If the encoder outputs (1, Z_DIM, N_FRAMES), average over frames (though unlikely for a single 1D input)\n",
        "                    if z_reco_sample_embedding.ndim == 3:\n",
        "                        z_reco_sample_embedding = z_reco_sample_embedding.mean(dim=-1) # (1, Z_DIM)\n",
        "\n",
        "                    z_speaker_id_reconstructed_batch.append(z_reco_sample_embedding.squeeze(0)) # Remove batch dim (1)\n",
        "\n",
        "                # Stack the individual embeddings back into a batch tensor\n",
        "                z_speaker_id_reconstructed = torch.stack(z_speaker_id_reconstructed_batch) # (Batch, Z_DIM)\n",
        "\n",
        "                # Calculate cosine similarity for each sample in the batch\n",
        "                cosine_sims = F.cosine_similarity(z_speaker_id_original, z_speaker_id_reconstructed, dim=1)\n",
        "\n",
        "                total_cosine_sim += cosine_sims.sum().item()\n",
        "                num_samples += mel_spec.shape[0]\n",
        "\n",
        "        if num_samples == 0:\n",
        "            return 0.0\n",
        "        return total_cosine_sim / num_samples\n",
        "\n",
        "print(\"SpeakerIDConsistencyEvaluator class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5deJeuzfaJ5",
        "outputId": "2bc18940-9db0-44aa-fe78-db0703d27949"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeakerIDConsistencyEvaluator class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 3: ContentDisentanglementEvaluator.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ContentDisentanglementEvaluator:\n",
        "    def __init__(self, num_speakers, device=Config.DEVICE):\n",
        "        \"\"\"\n",
        "        Initializes the Content Disentanglement Evaluator.\n",
        "        Args:\n",
        "            num_speakers (int): Total number of unique speakers in the dataset.\n",
        "            device (torch.device): Device to perform computations on.\n",
        "        \"\"\"\n",
        "        self.num_speakers = num_speakers\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate(self, model, dataloader):\n",
        "        \"\"\"\n",
        "        Evaluates how well speaker information is removed from z_content.\n",
        "        Calculates the accuracy of the ContentSpeakerDiscriminator on z_content.\n",
        "        Lower accuracy here indicates better disentanglement (closer to random chance).\n",
        "        Args:\n",
        "            model (torch.nn.Module): Your VCModel instance.\n",
        "            dataloader (torch.utils.data.DataLoader): DataLoader for validation/test set.\n",
        "        Returns:\n",
        "            float: Average accuracy of the ContentSpeakerDiscriminator.\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                mel_spec = batch['mel_spec'].to(self.device)\n",
        "                speaker_id_labels = batch['speaker_id'].to(self.device)\n",
        "\n",
        "                # Get z_content and discriminator logits directly from model's forward pass\n",
        "                # Note: The GRL is active during forward, so these logits reflect the disentanglement.\n",
        "                _, encoded_outputs = model(mel_spec)\n",
        "                discriminator_logits = encoded_outputs['content_speaker_discriminator_logits']\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted_speaker_id = torch.max(discriminator_logits, 1)\n",
        "                correct_predictions += (predicted_speaker_id == speaker_id_labels).sum().item()\n",
        "                total_predictions += speaker_id_labels.size(0)\n",
        "\n",
        "        if total_predictions == 0:\n",
        "            return 0.0\n",
        "        return correct_predictions / total_predictions\n",
        "\n",
        "print(\"ContentDisentanglementEvaluator class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ijUizgirpY",
        "outputId": "9db9f779-a844-42ba-a169-4d21941045b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ContentDisentanglementEvaluator class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufvfnX3JxsAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=========================**\n",
        "\n",
        "**==========Vocoder========**\n",
        "\n",
        "**=========================**"
      ],
      "metadata": {
        "id": "Swl3SAionKz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "# from utils import init_weights, get_padding\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "################################################\n",
        "\n",
        "\n",
        "LRELU_SLOPE = 0.1\n",
        "\n",
        "\n",
        "class ResBlock1(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock1, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs1:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.convs2:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class ResBlock2(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1])))\n",
        "        ])\n",
        "        self.convs.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c in self.convs:\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
        "                                k, u, padding=(k-u)//2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = h.upsample_initial_channel//(2**(i+1))\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(h, ch, k, d))\n",
        "\n",
        "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_pre(x)\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        remove_weight_norm(self.conv_post)"
      ],
      "metadata": {
        "id": "j50dx5kTnUig"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "# from env import AttrDict\n",
        "# from meldataset import MAX_WAV_VALUE\n",
        "# from models import Generator\n",
        "############################################\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "############################################\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "############################################\n",
        "\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n"
      ],
      "metadata": {
        "id": "4FePbW8eobvE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocoder_directory = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder\"\n",
        "HifiGAN_checkpoint_file=os.path.join(vocoder_directory,\"generator_v3\")\n",
        "config_file = os.path.join(vocoder_directory, 'config.json')\n",
        "print('vocoder_directory= ',vocoder_directory )\n",
        "print('HifiGAN_checkpoint_file = ',HifiGAN_checkpoint_file )\n",
        "print('config_file = ',config_file )\n",
        "\n",
        "###\n",
        "with open(config_file) as f:\n",
        "    data = f.read()\n",
        "###\n",
        "json_config = json.loads(data)\n",
        "h = AttrDict(json_config)\n",
        "\n",
        "print('json_config= ',json_config)\n",
        "print('h= ',h)\n",
        "\n",
        "##\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(h.seed)\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print('device= ',device)\n",
        "\n",
        "## Inference\n",
        "generator = Generator(h).to(device)\n",
        "state_dict_g = load_checkpoint(HifiGAN_checkpoint_file, device)\n",
        "generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "# filelist = os.listdir(a.input_mels_dir)\n",
        "# os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "generator.eval()\n",
        "generator.remove_weight_norm()\n",
        "\n",
        "input_mel_batch = torch.randn(32, 80, 256).to(device)\n",
        "sampling_rate=22050\n",
        "\n",
        "base_output_dir = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed\" # Ensure this directory exists\n",
        "os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "# Generate dummy filenames, assuming 32 items in the batch\n",
        "output_filenames_base = [f\"batch_item_{i:02d}\" for i in range(input_mel_batch.shape[0])]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    y_g_hat = generator(input_mel_batch)\n",
        "    print('y_g_hat shape= ',y_g_hat.shape)\n",
        "    y_g_hat_squeezed = y_g_hat.squeeze()\n",
        "    print('y_g_hat_squeezed shape= ',y_g_hat_squeezed.shape)\n",
        "\n",
        "    audio_batch_denormalized = y_g_hat_squeezed * MAX_WAV_VALUE\n",
        "    audio_batch_int16 = audio_batch_denormalized.cpu().numpy().astype('int16')\n",
        "    for i, audio in enumerate(audio_batch_int16):\n",
        "        # Construct the output filename\n",
        "        output_file = os.path.join(base_output_dir, output_filenames_base[i] + '_generated_e2e.wav')\n",
        "        print('output_file= ')\n",
        "\n",
        "        # Write the audio to a WAV file\n",
        "        write(output_file, sampling_rate, audio)\n",
        "        print(f\"Generated: {output_file}\")\n",
        "\n",
        "print(\"\\nBatch processing complete.\")"
      ],
      "metadata": {
        "id": "-P5g_vUfo1Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3cb6593-3c2a-49ba-dd90-d3590b04afa8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder_directory=  /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder\n",
            "HifiGAN_checkpoint_file =  /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/generator_v3\n",
            "config_file =  /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/config.json\n",
            "json_config=  {'resblock': '2', 'num_gpus': 0, 'batch_size': 16, 'learning_rate': 0.0002, 'adam_b1': 0.8, 'adam_b2': 0.99, 'lr_decay': 0.999, 'seed': 1234, 'upsample_rates': [8, 8, 4], 'upsample_kernel_sizes': [16, 16, 8], 'upsample_initial_channel': 256, 'resblock_kernel_sizes': [3, 5, 7], 'resblock_dilation_sizes': [[1, 2], [2, 6], [3, 12]], 'resblock_initial_channel': 128, 'segment_size': 8192, 'num_mels': 80, 'num_freq': 1025, 'n_fft': 1024, 'hop_size': 256, 'win_size': 1024, 'sampling_rate': 22050, 'fmin': 0, 'fmax': 8000, 'fmax_loss': None, 'num_workers': 4, 'dist_config': {'dist_backend': 'nccl', 'dist_url': 'tcp://localhost:54322', 'world_size': 1}}\n",
            "h=  {'resblock': '2', 'num_gpus': 0, 'batch_size': 16, 'learning_rate': 0.0002, 'adam_b1': 0.8, 'adam_b2': 0.99, 'lr_decay': 0.999, 'seed': 1234, 'upsample_rates': [8, 8, 4], 'upsample_kernel_sizes': [16, 16, 8], 'upsample_initial_channel': 256, 'resblock_kernel_sizes': [3, 5, 7], 'resblock_dilation_sizes': [[1, 2], [2, 6], [3, 12]], 'resblock_initial_channel': 128, 'segment_size': 8192, 'num_mels': 80, 'num_freq': 1025, 'n_fft': 1024, 'hop_size': 256, 'win_size': 1024, 'sampling_rate': 22050, 'fmin': 0, 'fmax': 8000, 'fmax_loss': None, 'num_workers': 4, 'dist_config': {'dist_backend': 'nccl', 'dist_url': 'tcp://localhost:54322', 'world_size': 1}}\n",
            "device=  cuda\n",
            "Loading '/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/generator_v3'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete.\n",
            "Removing weight norm...\n",
            "y_g_hat shape=  torch.Size([32, 1, 65536])\n",
            "y_g_hat_squeezed shape=  torch.Size([32, 65536])\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_00_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_01_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_02_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_03_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_04_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_05_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_06_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_07_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_08_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_09_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_10_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_11_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_12_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_13_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_14_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_15_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_16_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_17_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_18_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_19_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_20_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_21_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_22_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_23_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_24_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_25_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_26_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_27_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_28_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_29_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_30_generated_e2e.wav\n",
            "output_file= \n",
            "Generated: /content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed/batch_item_31_generated_e2e.wav\n",
            "\n",
            "Batch processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=====================================**\n",
        "\n",
        "**===== Applying vocoder to Train LOOP======**\n",
        "\n",
        "**=====================================**"
      ],
      "metadata": {
        "id": "qa6CgSUvIFKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MRH: turning it into class\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "# Assuming Generator and AttrDict are available (e.g., imported from another file or defined globally)\n",
        "\n",
        "############################################\n",
        "# Define AttrDict and MAX_WAV_VALUE if they are not globally available\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "############################################\n",
        "\n",
        "class Vocoder:\n",
        "    def __init__(self, vocoder_directory):\n",
        "        \"\"\"\n",
        "        Initializes the Vocoder class by loading its configuration, setting up the device,\n",
        "        and loading the HifiGAN generator model and its checkpoint.\n",
        "\n",
        "        Args:\n",
        "            vocoder_directory (str): Path to the vocoder directory containing config.json and generator checkpoint.\n",
        "        \"\"\"\n",
        "        self.vocoder_directory = vocoder_directory\n",
        "        self.HifiGAN_checkpoint_file = os.path.join(vocoder_directory, \"generator_v3\")\n",
        "        self.config_file = os.path.join(vocoder_directory, 'config.json')\n",
        "\n",
        "        self.h = None\n",
        "        self.device = None\n",
        "        self.generator = None\n",
        "\n",
        "        # 1. Load config and create 'h'\n",
        "        self._load_config()\n",
        "\n",
        "        # 2. Initialize 'device'\n",
        "        self._initialize_device()\n",
        "\n",
        "        # 3. Load 'Generator' and its checkpoint\n",
        "        self._load_generator()\n",
        "\n",
        "        print(\"Vocoder fully initialized and ready.\")\n",
        "\n",
        "    def _load_config(self):\n",
        "        \"\"\"Loads the configuration from config.json into an AttrDict.\"\"\"\n",
        "        print('Loading vocoder configuration...')\n",
        "        with open(self.config_file) as f:\n",
        "            data = f.read()\n",
        "        json_config = json.loads(data)\n",
        "        self.h = AttrDict(json_config)\n",
        "        print('Configuration loaded successfully.')\n",
        "        # print('json_config= ', json_config) # Uncomment for debug\n",
        "        # print('h= ', self.h) # Uncomment for debug\n",
        "\n",
        "    def _initialize_device(self):\n",
        "        \"\"\"Initializes the PyTorch device (CUDA or CPU).\"\"\"\n",
        "        print('Initializing device...')\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(self.h.seed)\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        print(f'Device initialized to: {self.device}')\n",
        "\n",
        "    def _load_checkpoint(self, filepath, device):\n",
        "        \"\"\"Helper function to load a PyTorch checkpoint.\"\"\"\n",
        "        assert os.path.isfile(filepath)\n",
        "        print(f\"Loading checkpoint from '{filepath}'\")\n",
        "        checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "        print(\"Checkpoint loaded.\")\n",
        "        return checkpoint_dict\n",
        "\n",
        "    def _load_generator(self):\n",
        "        \"\"\"Initializes the Generator model and loads its state dictionary.\"\"\"\n",
        "        print('Loading HifiGAN generator model...')\n",
        "\n",
        "        self.generator = Generator(self.h).to(self.device)\n",
        "        state_dict_g = self._load_checkpoint(self.HifiGAN_checkpoint_file, self.device)\n",
        "        self.generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "        self.generator.eval()\n",
        "        self.generator.remove_weight_norm()\n",
        "        print(\"HifiGAN generator loaded and configured.\")\n",
        "\n",
        "    def generate_audio(self, input_mel_batch, sampling_rate, output_dir=None, output_filenames_base=None):\n",
        "        \"\"\"\n",
        "        Generates audio from a batch of mel spectrograms using the loaded HifiGAN generator.\n",
        "\n",
        "        Args:\n",
        "            input_mel_batch (torch.Tensor): A batch of mel spectrograms.\n",
        "                                             Shape: (batch_size, num_mels, time_steps)\n",
        "            sampling_rate (int): The sampling rate for the output audio.\n",
        "            output_dir (str, optional): Directory to save the generated audio files.\n",
        "                                        If None, audio is not saved to disk.\n",
        "            output_filenames_base (list, optional): List of base filenames for the output audio files.\n",
        "                                                    If None, default filenames will be used.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Denormalized and int16-converted audio batch.\n",
        "                           Shape: (batch_size, audio_length)\n",
        "        \"\"\"\n",
        "        if self.generator is None:\n",
        "            raise RuntimeError(\"Generator not initialized. Check Vocoder setup.\")\n",
        "\n",
        "        # Ensure input_mel_batch is on the correct device\n",
        "        input_mel_batch = input_mel_batch.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_g_hat = self.generator(input_mel_batch)\n",
        "            # print('y_g_hat shape= ', y_g_hat.shape)\n",
        "            y_g_hat_squeezed = y_g_hat.squeeze()\n",
        "            # print('y_g_hat_squeezed shape= ', y_g_hat_squeezed.shape)\n",
        "\n",
        "            audio_batch_denormalized = y_g_hat_squeezed * MAX_WAV_VALUE\n",
        "            audio_batch_int16 = audio_batch_denormalized.cpu().numpy().astype('float32')\n",
        "            # print('audio_batch_int16 made ...!')\n",
        "\n",
        "            # if output_dir:\n",
        "            #     os.makedirs(output_dir, exist_ok=True)\n",
        "            #     if output_filenames_base is None:\n",
        "            #         output_filenames_base = [f\"batch_item_{i:02d}\" for i in range(input_mel_batch.shape[0])]\n",
        "\n",
        "            #     # for i, audio in enumerate(audio_batch_int16):\n",
        "            #     #     output_file = os.path.join(output_dir, output_filenames_base[i] + '_generated_e2e.wav')\n",
        "            #     #     print('output_file= ', output_file)\n",
        "            #     #     write(output_file, sampling_rate, audio)\n",
        "            #     #     print(f\"Generated: {output_file}\")\n",
        "\n",
        "        return audio_batch_int16\n",
        "\n",
        "# Example usage in a Google Colab environment:\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Initialize the Vocoder. All setup is now internal.\n",
        "    vocoder_directory = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder\"\n",
        "    vocoder = Vocoder(vocoder_directory)\n",
        "\n",
        "    # 2. Prepare dummy input for demonstration\n",
        "    # You would get this `input_mel_batch` from your data loader or model output\n",
        "    input_mel_batch = torch.randn(32, 80, 256) # Create on CPU first; generate_audio will move it to device\n",
        "    sampling_rate = 22050\n",
        "    base_output_dir = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed\"\n",
        "\n",
        "    # 3. Generate audio\n",
        "    generated_audio_batch = vocoder.generate_audio(\n",
        "        input_mel_batch,\n",
        "        sampling_rate,\n",
        "        output_dir=base_output_dir\n",
        "    )\n",
        "\n",
        "    print(\"\\nInside Vocoder Batch processing complete.\")\n",
        "    print(\"Inside Vocoder Shape of generated audio batch:\", generated_audio_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7M2XiyaLkNi",
        "outputId": "1d32d939-0749-43fb-cbe7-9394e554f5be"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vocoder configuration...\n",
            "Configuration loaded successfully.\n",
            "Initializing device...\n",
            "Device initialized to: cuda\n",
            "Loading HifiGAN generator model...\n",
            "Loading checkpoint from '/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/generator_v3'\n",
            "Checkpoint loaded.\n",
            "Removing weight norm...\n",
            "HifiGAN generator loaded and configured.\n",
            "Vocoder fully initialized and ready.\n",
            "\n",
            "Inside Vocoder Batch processing complete.\n",
            "Inside Vocoder Shape of generated audio batch: (32, 65536)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JReBlD7zOkS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**===================================**\n",
        "\n",
        "**======== Run Code ==================**\n",
        "\n",
        "**===================================**"
      ],
      "metadata": {
        "id": "5vPgzRiROkui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Dataset paths\n",
        "    VC_DIR = \"/content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental\" # Base VCTK dataset directory\n",
        "    VC_Output = \"/content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2\" # Base VCTK dataset directory\n",
        "    WAV_DIR = os.path.join(VC_DIR, \"wav48\") # Directory containing wav files\n",
        "    METADATA_PATH = os.path.join(VC_DIR, \"speaker-info.txt\") # Speaker metadata file path\n",
        "\n",
        "    vocoder_directory = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder\"\n",
        "    vocoder_wave_output_dir = \"/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/Reconstructed\"\n",
        "\n",
        "    # Audio processing parameters\n",
        "    SAMPLE_RATE = 16000 # Target sample rate for all audio (Hz)\n",
        "    N_MELS = 80         # Number of Mel bands for spectrogram\n",
        "    N_FFT = 400         # FFT window size (samples)\n",
        "    HOP_LENGTH = 160    # Hop length (samples) between windows\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_WORKERS = 2    # Number of CPU workers for data loading\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    NUM_EPOCHS = 50\n",
        "    TRAIN_SPLIT_RATIO = 0.8 # 80% for training, 20% for validation\n",
        "    RANDOM_SEED = 42    # For reproducibility\n",
        "\n",
        "    # Device configuration\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Model parameters (Specific to this VC architecture)\n",
        "    Z_CONTENT_DIM = 256 # Dimension for speaker-invariant content embedding\n",
        "    Z_GENDER_DIM = 16   # Dimension for gender embedding\n",
        "    Z_ACCENT_DIM = 64   # Dimension for accent embedding\n",
        "    Z_SPEAKER_ID_DIM = 256 # Dimension for individual speaker ID embedding (like x-vector)\n",
        "    Z_AGE_DIM = 32      # Dimension for age embedding (you can adjust this)\n",
        "    Z_REGION_DIM = 64   # Dimension for region embedding (you can adjust this)"
      ],
      "metadata": {
        "id": "1Ed-9xt3Hr72"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "\n",
        "results_dir = os.path.join(Config.VC_Output, \"experiment_results\") # Use the same path as in training\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot average_mcds\n",
        "plt.subplot(1, 3, 1)\n",
        "for filename in os.listdir(results_dir):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(results_dir, filename)\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            exp_name = data[\"experiment_config\"][\"name\"]\n",
        "            mcds = data[\"val_average_mcds\"]\n",
        "            epochs = range(1, len(mcds) + 1)\n",
        "            plt.plot(epochs, mcds, label=exp_name, linewidth=2) # Added linewidth=2\n",
        "plt.title(\"Validation Average MCD\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MCD\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot average_speaker_id_consistency\n",
        "plt.subplot(1, 3, 2)\n",
        "for filename in os.listdir(results_dir):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(results_dir, filename)\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            exp_name = data[\"experiment_config\"][\"name\"]\n",
        "            consistencies = data[\"val_average_speaker_id_consistencys\"]\n",
        "            epochs = range(1, len(consistencies) + 1)\n",
        "            plt.plot(epochs, consistencies, label=exp_name, linewidth=2) # Added linewidth=2\n",
        "plt.title(\"Validation Speaker ID Consistency (Cosine Sim)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cosine Similarity\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot disentanglement_accuracy\n",
        "plt.subplot(1, 3, 3)\n",
        "for filename in os.listdir(results_dir):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(results_dir, filename)\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            exp_name = data[\"experiment_config\"][\"name\"]\n",
        "            dis_accs = data[\"val_disentanglement_accuracys\"]\n",
        "            epochs = range(1, len(dis_accs) + 1)\n",
        "            plt.plot(epochs, dis_accs, label=exp_name, linewidth=2) # Added linewidth=2\n",
        "plt.title(\"Validation Disentanglement Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "3dHGVbUwfeA7",
        "outputId": "f78d81d4-b7d9-4ca6-c8d2-66773f424d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyworld\n",
        "!pip install speechbrain\n",
        "!pip install jiwer\n",
        "# !pip install librosa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsJByFMiSrZm",
        "outputId": "ec6339d3-9453-4d08-81cf-69227b86036c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyworld\n",
            "  Downloading pyworld-0.3.5.tar.gz (261 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyworld) (2.0.2)\n",
            "Building wheels for collected packages: pyworld\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.5-cp312-cp312-linux_x86_64.whl size=966259 sha256=150fa7e264d194f6fcb11c83f2467ecff3328f42a4ee01395ed2b0ba4928cba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ac/58/c6a1791ec6d17f3a99b6ccdec92b472f560cb5c564b83dd77e\n",
            "Successfully built pyworld\n",
            "Installing collected packages: pyworld\n",
            "Successfully installed pyworld-0.3.5\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from speechbrain) (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from speechbrain) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from speechbrain) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from speechbrain) (1.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain) (0.2.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.12/dist-packages (from speechbrain) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from speechbrain) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from speechbrain) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->speechbrain) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->speechbrain) (1.1.9)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9->speechbrain) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->speechbrain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->speechbrain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->speechbrain) (2025.8.3)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pesq pystoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWju7ky-MEzJ",
        "outputId": "dc86c088-95ec-48c4-f7fb-93b0c98f957e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pesq\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pystoi\n",
            "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pystoi) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pystoi) (1.16.1)\n",
            "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Building wheels for collected packages: pesq\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp312-cp312-linux_x86_64.whl size=284123 sha256=4107c5dfeaa5716510a86fbe1478f1e63d8f706d74f63ad7009f81e02e7f4dc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/d4/a4/9cf3512534cd47ce4a036d1593ee4013f2bf7509e631a147a3\n",
            "Successfully built pesq\n",
            "Installing collected packages: pesq, pystoi\n",
            "Successfully installed pesq-0.0.4 pystoi-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CV9xyNCMCsUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class VoiceConversionEvaluator:\n",
        "    def __init__(self, sample_rate=16000, n_mels=80, device='cuda'):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        self.reset_metrics()\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        \"\"\"Reset all metrics\"\"\"\n",
        "        self.metrics = {\n",
        "            'mcd': [],\n",
        "            'f0_rmse': [],\n",
        "            'f0_correlation': [],\n",
        "            'energy_rmse': [],\n",
        "            'vuv_error': [],\n",
        "            'speaker_similarity': [],\n",
        "            'speaker_id_consistency': [],\n",
        "            'pesq': [],  # Added PESQ\n",
        "            'stoi': [],  # Added STOI\n",
        "        }\n",
        "\n",
        "    def calculate_mcd(self, original_mel, converted_mel):\n",
        "        \"\"\"\n",
        "        Calculate Mel Cepstral Distortion (MCD) between original and converted mel-spectrograms\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.debug(\"Calculating MCD...\")\n",
        "\n",
        "            # Ensure we're working with numpy arrays\n",
        "            if torch.is_tensor(original_mel):\n",
        "                original_mel = original_mel.cpu().detach().numpy()\n",
        "            if torch.is_tensor(converted_mel):\n",
        "                converted_mel = converted_mel.cpu().detach().numpy()\n",
        "\n",
        "            logger.debug(f\"Original mel shape: {original_mel.shape}\")\n",
        "            logger.debug(f\"Converted mel shape: {converted_mel.shape}\")\n",
        "\n",
        "            # Handle different dimensionalities\n",
        "            if original_mel.ndim == 3:\n",
        "                original_mel = original_mel[0]  # Take first batch element\n",
        "                logger.debug(f\"Original mel after dim reduction: {original_mel.shape}\")\n",
        "            if converted_mel.ndim == 3:\n",
        "                converted_mel = converted_mel[0]  # Take first batch element\n",
        "                logger.debug(f\"Converted mel after dim reduction: {converted_mel.shape}\")\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(original_mel.shape[1], converted_mel.shape[1])\n",
        "            original_mel = original_mel[:, :min_len]\n",
        "            converted_mel = converted_mel[:, :min_len]\n",
        "\n",
        "            logger.debug(f\"After length alignment - Original: {original_mel.shape}, Converted: {converted_mel.shape}\")\n",
        "\n",
        "            # Calculate MCD\n",
        "            diff = original_mel - converted_mel\n",
        "            mcd = np.mean(np.sqrt(np.sum(diff**2, axis=0)))\n",
        "\n",
        "            logger.debug(f\"MCD calculated: {mcd}\")\n",
        "            return mcd\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in MCD calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0\n",
        "\n",
        "    def extract_f0(self, audio):\n",
        "        \"\"\"Extract F0 using PyWorld algorithm\"\"\"\n",
        "        try:\n",
        "            import pyworld as pw\n",
        "\n",
        "            logger.debug(f\"Extracting F0 from audio with shape: {audio.shape}\")\n",
        "            logger.debug(f\"Audio range: [{np.min(audio)}, {np.max(audio)}]\")\n",
        "\n",
        "            # Ensure audio is float64\n",
        "            audio = audio.astype(np.float64)\n",
        "\n",
        "            # Check if audio is not silent\n",
        "            if np.max(np.abs(audio)) < 0.001:\n",
        "                logger.warning(\"Audio is too silent for F0 extraction\")\n",
        "                return np.zeros(100)\n",
        "\n",
        "            # Extract F0\n",
        "            f0, t = pw.dio(audio, self.sample_rate)\n",
        "            f0 = pw.stonemask(audio, f0, t, self.sample_rate)\n",
        "\n",
        "            logger.debug(f\"Extracted F0 shape: {f0.shape}, non-zero values: {np.sum(f0 > 0)}\")\n",
        "            return f0\n",
        "        except ImportError:\n",
        "            logger.error(\"PyWorld not installed. Please install with: pip install pyworld\")\n",
        "            return np.zeros(100)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in F0 extraction: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return np.zeros(100)\n",
        "\n",
        "    def calculate_f0_metrics(self, original_audio, converted_audio):\n",
        "        \"\"\"Calculate F0-related metrics (RMSE, correlation, VUV error)\"\"\"\n",
        "        try:\n",
        "            logger.debug(\"Calculating F0 metrics...\")\n",
        "\n",
        "            # Extract F0\n",
        "            original_f0 = self.extract_f0(original_audio)\n",
        "            converted_f0 = self.extract_f0(converted_audio)\n",
        "\n",
        "            logger.debug(f\"Original F0 shape: {original_f0.shape}, Converted F0 shape: {converted_f0.shape}\")\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(len(original_f0), len(converted_f0))\n",
        "            original_f0 = original_f0[:min_len]\n",
        "            converted_f0 = converted_f0[:min_len]\n",
        "\n",
        "            logger.debug(f\"After length alignment - Original F0: {original_f0.shape}, Converted F0: {converted_f0.shape}\")\n",
        "\n",
        "            # Check if we have enough voiced frames\n",
        "            voiced_mask = (original_f0 > 0) & (converted_f0 > 0)\n",
        "            logger.debug(f\"Voiced frames: {np.sum(voiced_mask)}\")\n",
        "\n",
        "            if np.sum(voiced_mask) < 10:  # Need at least 10 voiced frames\n",
        "                logger.warning(f\"Insufficient voiced frames: {np.sum(voiced_mask)}. Returning default values.\")\n",
        "                return 0.0, 0.0, 1.0  # Return default values\n",
        "\n",
        "            # Calculate RMSE for voiced frames only\n",
        "            f0_rmse = np.sqrt(np.mean((original_f0[voiced_mask] - converted_f0[voiced_mask])**2))\n",
        "\n",
        "            # Calculate correlation\n",
        "            f0_corr, _ = pearsonr(original_f0[voiced_mask], converted_f0[voiced_mask])\n",
        "\n",
        "            # Calculate VUV error\n",
        "            original_vuv = (original_f0 > 0).astype(int)\n",
        "            converted_vuv = (converted_f0 > 0).astype(int)\n",
        "            vuv_error = np.mean(original_vuv != converted_vuv)\n",
        "\n",
        "            logger.debug(f\"F0 metrics - RMSE: {f0_rmse}, Correlation: {f0_corr}, VUV Error: {vuv_error}\")\n",
        "            return f0_rmse, f0_corr, vuv_error\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in F0 metrics calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0, 0.0, 1.0\n",
        "\n",
        "    def calculate_energy_metrics(self, original_audio, converted_audio):\n",
        "        \"\"\"Calculate energy-related metrics\"\"\"\n",
        "        try:\n",
        "            logger.debug(\"Calculating energy metrics...\")\n",
        "\n",
        "            # Check if audio is not silent\n",
        "            if np.max(np.abs(original_audio)) < 0.001 or np.max(np.abs(converted_audio)) < 0.001:\n",
        "                logger.warning(\"Audio is too silent for energy calculation\")\n",
        "                return 0.0\n",
        "\n",
        "            # Calculate RMS energy\n",
        "            original_energy = librosa.feature.rms(y=original_audio, frame_length=1024, hop_length=256)[0]\n",
        "            converted_energy = librosa.feature.rms(y=converted_audio, frame_length=1024, hop_length=256)[0]\n",
        "\n",
        "            logger.debug(f\"Original energy shape: {original_energy.shape}, Converted energy shape: {converted_energy.shape}\")\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(len(original_energy), len(converted_energy))\n",
        "            original_energy = original_energy[:min_len]\n",
        "            converted_energy = converted_energy[:min_len]\n",
        "\n",
        "            # Calculate RMSE\n",
        "            energy_rmse = np.sqrt(np.mean((original_energy - converted_energy)**2))\n",
        "\n",
        "            logger.debug(f\"Energy RMSE: {energy_rmse}\")\n",
        "            return energy_rmse\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in energy metrics calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_speaker_similarity(self, original_audio, converted_audio):\n",
        "        \"\"\"\n",
        "        Calculate speaker similarity using a simple spectral-based approach\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.debug(\"Calculating speaker similarity...\")\n",
        "\n",
        "            # Check if audio is not silent\n",
        "            if np.max(np.abs(original_audio)) < 0.001 or np.max(np.abs(converted_audio)) < 0.001:\n",
        "                logger.warning(\"Audio is too silent for speaker similarity calculation\")\n",
        "                return 0.0\n",
        "\n",
        "            # Extract MFCC features\n",
        "            original_mfcc = librosa.feature.mfcc(y=original_audio, sr=self.sample_rate, n_mfcc=13)\n",
        "            converted_mfcc = librosa.feature.mfcc(y=converted_audio, sr=self.sample_rate, n_mfcc=13)\n",
        "\n",
        "            logger.debug(f\"Original MFCC shape: {original_mfcc.shape}, Converted MFCC shape: {converted_mfcc.shape}\")\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(original_mfcc.shape[1], converted_mfcc.shape[1])\n",
        "            original_mfcc = original_mfcc[:, :min_len]\n",
        "            converted_mfcc = converted_mfcc[:, :min_len]\n",
        "\n",
        "            # Calculate cosine similarity between MFCCs\n",
        "            similarity = 1 - cdist(original_mfcc.T, converted_mfcc.T, metric='cosine').mean()\n",
        "\n",
        "            logger.debug(f\"Speaker similarity: {similarity}\")\n",
        "            return max(0, similarity)  # Ensure non-negative\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in speaker similarity calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_pesq(self, reference_audio, degraded_audio):\n",
        "        \"\"\"\n",
        "        Calculate PESQ (Perceptual Evaluation of Speech Quality) score\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from pesq import pesq\n",
        "\n",
        "            # Ensure audio is in the correct format and sample rate\n",
        "            if len(reference_audio.shape) > 1:\n",
        "                reference_audio = reference_audio.flatten()\n",
        "            if len(degraded_audio.shape) > 1:\n",
        "                degraded_audio = degraded_audio.flatten()\n",
        "\n",
        "            # Resample if necessary (PESQ requires 8k or 16k)\n",
        "            if self.sample_rate not in [8000, 16000]:\n",
        "                logger.warning(f\"PESQ requires 8k or 16k sample rate, got {self.sample_rate}. Resampling to 16k.\")\n",
        "                reference_audio = librosa.resample(reference_audio, orig_sr=self.sample_rate, target_sr=16000)\n",
        "                degraded_audio = librosa.resample(degraded_audio, orig_sr=self.sample_rate, target_sr=16000)\n",
        "                pesq_rate = 16000\n",
        "            else:\n",
        "                pesq_rate = self.sample_rate\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(len(reference_audio), len(degraded_audio))\n",
        "            reference_audio = reference_audio[:min_len]\n",
        "            degraded_audio = degraded_audio[:min_len]\n",
        "\n",
        "            # Calculate PESQ\n",
        "            pesq_score = pesq(pesq_rate, reference_audio, degraded_audio, 'wb')\n",
        "\n",
        "            logger.debug(f\"PESQ score: {pesq_score}\")\n",
        "            return pesq_score\n",
        "        except ImportError:\n",
        "            logger.error(\"PESQ not installed. Please install with: pip install pesq\")\n",
        "            return 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in PESQ calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_stoi(self, reference_audio, degraded_audio):\n",
        "        \"\"\"\n",
        "        Calculate STOI (Short-Time Objective Intelligibility) score\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import pystoi\n",
        "\n",
        "            # Ensure audio is in the correct format\n",
        "            if len(reference_audio.shape) > 1:\n",
        "                reference_audio = reference_audio.flatten()\n",
        "            if len(degraded_audio.shape) > 1:\n",
        "                degraded_audio = degraded_audio.flatten()\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(len(reference_audio), len(degraded_audio))\n",
        "            reference_audio = reference_audio[:min_len]\n",
        "            degraded_audio = degraded_audio[:min_len]\n",
        "\n",
        "            # Calculate STOI\n",
        "            stoi_score = pystoi.stoi(reference_audio, degraded_audio, self.sample_rate, extended=False)\n",
        "\n",
        "            logger.debug(f\"STOI score: {stoi_score}\")\n",
        "            return stoi_score\n",
        "        except ImportError:\n",
        "            logger.error(\"PySTOI not installed. Please install with: pip install pystoi\")\n",
        "            return 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in STOI calculation: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return 0.0\n",
        "\n",
        "    def evaluate_batch(self, original_audio, converted_audio, original_mel=None, converted_mel=None):\n",
        "        \"\"\"\n",
        "        Evaluate a batch of audio samples\n",
        "\n",
        "        Args:\n",
        "            original_audio: Original source audio (batch_size, samples)\n",
        "            converted_audio: Converted audio (batch_size, samples)\n",
        "            original_mel: Original mel spectrograms (batch_size, n_mels, time)\n",
        "            converted_mel: Converted mel spectrograms (batch_size, n_mels, time)\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting batch evaluation...\")\n",
        "        batch_metrics = {}\n",
        "\n",
        "        # Convert audio to numpy if they are tensors\n",
        "        if torch.is_tensor(original_audio):\n",
        "            original_audio = original_audio.cpu().numpy()\n",
        "        if torch.is_tensor(converted_audio):\n",
        "            converted_audio = converted_audio.cpu().numpy()\n",
        "\n",
        "        logger.debug(f\"Original audio shape: {original_audio.shape}\")\n",
        "        logger.debug(f\"Converted audio shape: {converted_audio.shape}\")\n",
        "\n",
        "        # Calculate MCD if mel spectrograms are provided\n",
        "        if original_mel is not None and converted_mel is not None:\n",
        "            mcd = self.calculate_mcd(original_mel, converted_mel)\n",
        "            batch_metrics['mcd'] = mcd\n",
        "            self.metrics['mcd'].append(mcd)\n",
        "\n",
        "        # Calculate audio-based metrics\n",
        "        processed_samples = 0\n",
        "        for i in range(min(len(original_audio), len(converted_audio))):\n",
        "            try:\n",
        "                logger.info(f\"Processing sample {i}...\")\n",
        "                orig_audio = original_audio[i]\n",
        "                conv_audio = converted_audio[i]\n",
        "\n",
        "                # Skip if audio is silent\n",
        "                if np.max(np.abs(orig_audio)) < 0.001 or np.max(np.abs(conv_audio)) < 0.001:\n",
        "                    logger.warning(f\"Silent audio detected in sample {i}. Skipping.\")\n",
        "                    logger.warning(f\"Original audio max: {np.max(np.abs(orig_audio))}, Converted audio max: {np.max(np.abs(conv_audio))}\")\n",
        "                    continue\n",
        "\n",
        "                # F0 metrics\n",
        "                f0_rmse, f0_corr, vuv_error = self.calculate_f0_metrics(orig_audio, conv_audio)\n",
        "                batch_metrics.setdefault('f0_rmse', []).append(f0_rmse)\n",
        "                batch_metrics.setdefault('f0_correlation', []).append(f0_corr)\n",
        "                batch_metrics.setdefault('vuv_error', []).append(vuv_error)\n",
        "\n",
        "                # Energy metrics\n",
        "                energy_rmse = self.calculate_energy_metrics(orig_audio, conv_audio)\n",
        "                batch_metrics.setdefault('energy_rmse', []).append(energy_rmse)\n",
        "\n",
        "                # Speaker similarity\n",
        "                speaker_similarity = self.calculate_speaker_similarity(orig_audio, conv_audio)\n",
        "                batch_metrics.setdefault('speaker_similarity', []).append(speaker_similarity)\n",
        "\n",
        "                # PESQ and STOI\n",
        "                pesq_score = self.calculate_pesq(orig_audio, conv_audio)\n",
        "                stoi_score = self.calculate_stoi(orig_audio, conv_audio)\n",
        "                batch_metrics.setdefault('pesq', []).append(pesq_score)\n",
        "                batch_metrics.setdefault('stoi', []).append(stoi_score)\n",
        "\n",
        "                processed_samples += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error evaluating sample {i}: {e}\")\n",
        "                logger.error(traceback.format_exc())\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Successfully processed {processed_samples} samples\")\n",
        "\n",
        "        # Calculate averages for the batch\n",
        "        avg_metrics = {}\n",
        "        for key, values in batch_metrics.items():\n",
        "            if isinstance(values, list) and len(values) > 0:\n",
        "                avg_metrics[key] = np.mean(values)\n",
        "                # Store for overall summary\n",
        "                if key in self.metrics:\n",
        "                    self.metrics[key].extend(values)\n",
        "            else:\n",
        "                avg_metrics[key] = values if values is not None else 0.0\n",
        "\n",
        "        # Ensure all expected metrics are in the result\n",
        "        expected_metrics = ['mcd', 'f0_rmse', 'f0_correlation', 'vuv_error', 'energy_rmse',\n",
        "                           'speaker_similarity', 'pesq', 'stoi']\n",
        "        for metric in expected_metrics:\n",
        "            if metric not in avg_metrics:\n",
        "                avg_metrics[metric] = 0.0\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get summary of all metrics collected so far\"\"\"\n",
        "        summary = {}\n",
        "        for metric_name, values in self.metrics.items():\n",
        "            if values:  # Only if we have values\n",
        "                summary[metric_name] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values),\n",
        "                    'count': len(values)\n",
        "                }\n",
        "            else:\n",
        "                summary[metric_name] = {\n",
        "                    'mean': 0.0,\n",
        "                    'std': 0.0,\n",
        "                    'min': 0.0,\n",
        "                    'max': 0.0,\n",
        "                    'count': 0\n",
        "                }\n",
        "        return summary\n",
        "\n",
        "    def plot_results(self, save_path=None):\n",
        "        \"\"\"Plot results for all metrics\"\"\"\n",
        "        summary = self.get_summary()\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        # Plot each metric\n",
        "        metrics_to_plot = ['mcd', 'f0_rmse', 'f0_correlation', 'vuv_error', 'energy_rmse',\n",
        "                          'speaker_similarity', 'pesq', 'stoi']\n",
        "\n",
        "        for i, metric_name in enumerate(metrics_to_plot):\n",
        "            if i >= len(axes) or metric_name not in summary:\n",
        "                continue\n",
        "\n",
        "            stats = summary[metric_name]\n",
        "            # Create a bar plot for the mean with error bars for std\n",
        "            axes[i].bar(0, stats['mean'], yerr=stats['std'], capsize=5)\n",
        "            axes[i].set_title(metric_name)\n",
        "            axes[i].set_ylabel('Value')\n",
        "            axes[i].set_xticks([])\n",
        "\n",
        "            # Add value annotation\n",
        "            axes[i].text(0, stats['mean'] + stats['std'], f'{stats[\"mean\"]:.3f}',\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        return fig"
      ],
      "metadata": {
        "id": "BVjb4iaDvbIk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training and Validation Functions (UPDATED FOR FULL VCMODEL) ---\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer,\n",
        "                reconstruction_criterion, # NEW\n",
        "                gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "                age_criterion, region_criterion,\n",
        "                content_disentanglement_criterion, # NEW\n",
        "                device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    reco_loss_sum = 0.0 # NEW\n",
        "    gender_loss_sum = 0.0\n",
        "    accent_loss_sum = 0.0\n",
        "    speaker_id_loss_sum = 0.0\n",
        "    age_loss_sum = 0.0\n",
        "    region_loss_sum = 0.0\n",
        "    disentangle_loss_sum = 0.0 # NEW\n",
        "\n",
        "    # Accuracy tracking (already present)\n",
        "    correct_gender, total_gender = 0, 0\n",
        "    correct_accent, total_accent = 0, 0\n",
        "    correct_speaker_id, total_speaker_id = 0, 0\n",
        "    correct_age, total_age = 0, 0\n",
        "    correct_region, total_region = 0, 0\n",
        "\n",
        "    # Loss weights (these are hyperparameters you'll need to tune!)\n",
        "    lambda_reco = 1.0\n",
        "    lambda_gender = 0.1\n",
        "    lambda_accent = 0.1\n",
        "    lambda_speaker_id = 0.5\n",
        "    lambda_age = 0.1\n",
        "    lambda_region = 0.1\n",
        "    lambda_disentangle = 0.01 # Start small, maybe increase after some epochs\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        mel_spec = batch['mel_spec'].to(device)\n",
        "        speaker_id_labels = batch['speaker_id'].to(device)\n",
        "        gender_id_labels = batch['gender_id'].to(device)\n",
        "        accent_id_labels = batch['accent_id'].to(device)\n",
        "        age_id_labels = batch['age_id'].to(device)\n",
        "        region_id_labels = batch['region_id'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the FULL VCModel\n",
        "        reconstructed_mel, encoded_outputs = model(mel_spec) # Model now returns reconstructed_mel\n",
        "\n",
        "        # Calculate ALL losses\n",
        "        loss_reco = reconstruction_criterion(reconstructed_mel, mel_spec)\n",
        "        loss_gender = gender_criterion(encoded_outputs['gender_logits'], gender_id_labels)\n",
        "        loss_accent = accent_criterion(encoded_outputs['accent_logits'], accent_id_labels)\n",
        "        loss_speaker_id = speaker_id_criterion(encoded_outputs['speaker_id_logits'], speaker_id_labels)\n",
        "        loss_age = age_criterion(encoded_outputs['age_logits'], age_id_labels)\n",
        "        loss_region = region_criterion(encoded_outputs['region_logits'], region_id_labels)\n",
        "        loss_disentangle = content_disentanglement_criterion(\n",
        "            encoded_outputs['content_speaker_discriminator_logits'],\n",
        "            speaker_id_labels # Target for discriminator is the true speaker ID\n",
        "        )\n",
        "\n",
        "        # Combine all losses with weights\n",
        "        total_batch_loss = (\n",
        "            lambda_reco * loss_reco +\n",
        "            lambda_gender * loss_gender +\n",
        "            lambda_accent * loss_accent +\n",
        "            lambda_speaker_id * loss_speaker_id +\n",
        "            lambda_age * loss_age +\n",
        "            lambda_region * loss_region +\n",
        "            lambda_disentangle * loss_disentangle\n",
        "        )\n",
        "\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        total_loss += total_batch_loss.item()\n",
        "        reco_loss_sum += loss_reco.item()\n",
        "        gender_loss_sum += loss_gender.item()\n",
        "        accent_loss_sum += loss_accent.item()\n",
        "        speaker_id_loss_sum += loss_speaker_id.item()\n",
        "        age_loss_sum += loss_age.item()\n",
        "        region_loss_sum += loss_region.item()\n",
        "        disentangle_loss_sum += loss_disentangle.item()\n",
        "\n",
        "        # Update accuracies (unchanged)\n",
        "        _, predicted_gender = torch.max(encoded_outputs['gender_logits'], 1)\n",
        "        correct_gender += (predicted_gender == gender_id_labels).sum().item()\n",
        "        total_gender += gender_id_labels.size(0)\n",
        "\n",
        "        _, predicted_accent = torch.max(encoded_outputs['accent_logits'], 1)\n",
        "        correct_accent += (predicted_accent == accent_id_labels).sum().item()\n",
        "        total_accent += accent_id_labels.size(0)\n",
        "\n",
        "        _, predicted_speaker_id = torch.max(encoded_outputs['speaker_id_logits'], 1)\n",
        "        correct_speaker_id += (predicted_speaker_id == speaker_id_labels).sum().item()\n",
        "        total_speaker_id += speaker_id_labels.size(0)\n",
        "\n",
        "        _, predicted_age = torch.max(encoded_outputs['age_logits'], 1)\n",
        "        correct_age += (predicted_age == age_id_labels).sum().item()\n",
        "        total_age += age_id_labels.size(0)\n",
        "\n",
        "        _, predicted_region = torch.max(encoded_outputs['region_logits'], 1)\n",
        "        correct_region += (predicted_region == region_id_labels).sum().item()\n",
        "        total_region += region_id_labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'T_Loss': f'{total_batch_loss.item():.4f}',\n",
        "            'Reco': f'{loss_reco.item():.4f}', # NEW\n",
        "            'Dis': f'{loss_disentangle.item():.4f}', # NEW\n",
        "            'S_Acc': f'{100 * correct_speaker_id / total_speaker_id:.2f}%'\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_reco_loss = reco_loss_sum / len(dataloader) # NEW\n",
        "    avg_gender_loss = gender_loss_sum / len(dataloader)\n",
        "    avg_accent_loss = accent_loss_sum / len(dataloader)\n",
        "    avg_speaker_id_loss = speaker_id_loss_sum / len(dataloader)\n",
        "    avg_age_loss = age_loss_sum / len(dataloader)\n",
        "    avg_region_loss = region_loss_sum / len(dataloader)\n",
        "    avg_disentangle_loss = disentangle_loss_sum / len(dataloader) # NEW\n",
        "\n",
        "    gender_accuracy = correct_gender / total_gender if total_gender > 0 else 0\n",
        "    accent_accuracy = correct_accent / total_accent if total_accent > 0 else 0\n",
        "    speaker_id_accuracy = correct_speaker_id / total_speaker_id if total_speaker_id > 0 else 0\n",
        "    age_accuracy = correct_age / total_age if total_age > 0 else 0\n",
        "    region_accuracy = correct_region / total_region if total_region > 0 else 0\n",
        "\n",
        "    return (avg_loss, avg_reco_loss, avg_gender_loss, avg_accent_loss, # NEW return value\n",
        "            avg_speaker_id_loss, avg_age_loss, avg_region_loss, avg_disentangle_loss, # NEW return value\n",
        "            gender_accuracy, accent_accuracy, speaker_id_accuracy,\n",
        "            age_accuracy, region_accuracy)\n",
        "\n",
        "# --- The validate_epoch function should be updated in a very similar way ---\n",
        "def validate_epoch(model, dataloader,\n",
        "                   reconstruction_criterion,\n",
        "                   gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "                   age_criterion, region_criterion,\n",
        "                   content_disentanglement_criterion,\n",
        "                   device): # Keep this signature as is\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    reco_loss_sum = 0.0\n",
        "    gender_loss_sum = 0.0\n",
        "    accent_loss_sum = 0.0\n",
        "    speaker_id_loss_sum = 0.0\n",
        "    age_loss_sum = 0.0\n",
        "    region_loss_sum = 0.0\n",
        "    disentangle_loss_sum = 0.0\n",
        "\n",
        "    correct_gender, total_gender = 0, 0\n",
        "    correct_accent, total_accent = 0, 0\n",
        "    correct_speaker_id, total_speaker_id = 0, 0\n",
        "    correct_age, total_age = 0, 0\n",
        "    correct_region, total_region = 0, 0\n",
        "\n",
        "    # Loss weights (should be the same as in train_epoch for consistent evaluation)\n",
        "    lambda_reco = 1.0\n",
        "    lambda_gender = 0.1\n",
        "    lambda_accent = 0.1\n",
        "    lambda_speaker_id = 0.5\n",
        "    lambda_age = 0.1\n",
        "    lambda_region = 0.1\n",
        "    lambda_disentangle = 0.01\n",
        "\n",
        "    # Initialize lists for batch-wise metrics for final average calculation\n",
        "    batch_mcds = []\n",
        "    batch_speaker_id_consistencies = []\n",
        "    batch_disentanglement_accuracies = []\n",
        "\n",
        "    #####\n",
        "    # Initialize evaluator for this epoch\n",
        "    vc_evaluator.reset_metrics()\n",
        "\n",
        "    #####\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            mel_spec = batch['mel_spec'].to(device)\n",
        "            speaker_id_labels = batch['speaker_id'].to(device)\n",
        "            gender_id_labels = batch['gender_id'].to(device)\n",
        "            accent_id_labels = batch['accent_id'].to(device)\n",
        "            age_id_labels = batch['age_id'].to(device)\n",
        "            region_id_labels = batch['region_id'].to(device)\n",
        "\n",
        "            reconstructed_mel, encoded_outputs = model(mel_spec)\n",
        "\n",
        "            loss_reco = reconstruction_criterion(reconstructed_mel, mel_spec)\n",
        "            loss_gender = gender_criterion(encoded_outputs['gender_logits'], gender_id_labels)\n",
        "            loss_accent = accent_criterion(encoded_outputs['accent_logits'], accent_id_labels)\n",
        "            loss_speaker_id = speaker_id_criterion(encoded_outputs['speaker_id_logits'], speaker_id_labels)\n",
        "            loss_age = age_criterion(encoded_outputs['age_logits'], age_id_labels)\n",
        "            loss_region = region_criterion(encoded_outputs['region_logits'], region_id_labels)\n",
        "            loss_disentangle = content_disentanglement_criterion(\n",
        "                encoded_outputs['content_speaker_discriminator_logits'],\n",
        "                speaker_id_labels\n",
        "            )\n",
        "\n",
        "            current_batch_loss = (\n",
        "                lambda_reco * loss_reco +\n",
        "                lambda_gender * loss_gender +\n",
        "                lambda_accent * loss_accent +\n",
        "                lambda_speaker_id * loss_speaker_id +\n",
        "                lambda_age * loss_age +\n",
        "                lambda_region * loss_region +\n",
        "                lambda_disentangle * loss_disentangle\n",
        "            )\n",
        "\n",
        "            total_loss += current_batch_loss.item()\n",
        "            reco_loss_sum += loss_reco.item()\n",
        "            gender_loss_sum += loss_gender.item()\n",
        "            accent_loss_sum += loss_accent.item()\n",
        "            speaker_id_loss_sum += loss_speaker_id.item()\n",
        "            age_loss_sum += loss_age.item()\n",
        "            region_loss_sum += loss_region.item()\n",
        "            disentangle_loss_sum += loss_disentangle.item()\n",
        "\n",
        "            # --- Calculate new metrics for the current batch ---\n",
        "            for i in range(mel_spec.shape[0]):\n",
        "                # MCD for current sample\n",
        "                original_len = (mel_spec[i].sum(dim=0) != 0).sum()\n",
        "                reco_len = (reconstructed_mel[i].sum(dim=0) != 0).sum()\n",
        "                if original_len > 0 and reco_len > 0: # Avoid empty samples\n",
        "                    mcd = mcd_evaluator.calculate_mcd(\n",
        "                        mel_spec[i, :, :original_len],\n",
        "                        reconstructed_mel[i, :, :reco_len]\n",
        "                    )\n",
        "                    batch_mcds.append(mcd)\n",
        "\n",
        "                # Speaker ID Consistency for current sample\n",
        "                # Ensure z_speaker_id from original are 1D vectors for cosine similarity (mean over time if 3D)\n",
        "                z_speaker_id_original = encoded_outputs['z_speaker_id'][i]\n",
        "                if z_speaker_id_original.ndim == 2: # (Z_DIM, N_FRAMES)\n",
        "                    z_speaker_id_original = z_speaker_id_original.mean(dim=-1)\n",
        "\n",
        "                # Re-encode the reconstructed Mel for current sample\n",
        "                reco_sample_mel_i = reconstructed_mel[i].unsqueeze(0) # Add batch dim back (1, N_MELS, T)\n",
        "\n",
        "                # --- CRITICAL FIX: Match the pipeline for speaker_id_encoder ---\n",
        "                shared_features_reconstructed_i = model.encoders.shared_extractor(reco_sample_mel_i)\n",
        "                pooled_shared_features_reconstructed_i = F.adaptive_avg_pool1d(shared_features_reconstructed_i, 1).squeeze(-1)\n",
        "                input_to_speaker_id_encoder_i = pooled_shared_features_reconstructed_i.unsqueeze(-1) # Shape (1, C, 1)\n",
        "\n",
        "                z_speaker_id_reconstructed, _ = model.encoders.speaker_id_encoder(input_to_speaker_id_encoder_i)\n",
        "                if z_speaker_id_reconstructed.ndim == 3: # (1, Z_DIM, N_FRAMES)\n",
        "                    z_speaker_id_reconstructed = z_speaker_id_reconstructed.mean(dim=-1) # (1, Z_DIM)\n",
        "                z_speaker_id_reconstructed = z_speaker_id_reconstructed.squeeze(0) # Remove batch dim\n",
        "\n",
        "                cosine_sim = F.cosine_similarity(z_speaker_id_original.unsqueeze(0), z_speaker_id_reconstructed.unsqueeze(0), dim=1).item()\n",
        "                batch_speaker_id_consistencies.append(cosine_sim)\n",
        "\n",
        "\n",
        "            # Content Disentanglement for current batch (calculated directly from output)\n",
        "            _, predicted_disc_speaker_id = torch.max(encoded_outputs['content_speaker_discriminator_logits'], 1)\n",
        "            correct_disc_predictions = (predicted_disc_speaker_id == speaker_id_labels).sum().item()\n",
        "            batch_disentanglement_accuracies.append(correct_disc_predictions / speaker_id_labels.size(0))\n",
        "\n",
        "\n",
        "\n",
        "            # Update accuracies (your existing logic for accuracies is fine)\n",
        "            _, predicted_gender = torch.max(encoded_outputs['gender_logits'], 1)\n",
        "            correct_gender += (predicted_gender == gender_id_labels).sum().item()\n",
        "            total_gender += gender_id_labels.size(0)\n",
        "\n",
        "            _, predicted_accent = torch.max(encoded_outputs['accent_logits'], 1)\n",
        "            correct_accent += (predicted_accent == accent_id_labels).sum().item()\n",
        "            total_accent += accent_id_labels.size(0)\n",
        "\n",
        "            _, predicted_speaker_id = torch.max(encoded_outputs['speaker_id_logits'], 1)\n",
        "            correct_speaker_id += (predicted_speaker_id == speaker_id_labels).sum().item()\n",
        "            total_speaker_id += speaker_id_labels.size(0)\n",
        "\n",
        "            _, predicted_age = torch.max(encoded_outputs['age_logits'], 1)\n",
        "            correct_age += (predicted_age == age_id_labels).sum().item()\n",
        "            total_age += age_id_labels.size(0)\n",
        "\n",
        "            _, predicted_region = torch.max(encoded_outputs['region_logits'], 1)\n",
        "            correct_region += (predicted_region == region_id_labels).sum().item()\n",
        "            total_region += region_id_labels.size(0)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'V_Loss': f'{current_batch_loss.item():.4f}',\n",
        "                'Reco': f'{loss_reco.item():.4f}',\n",
        "                'Dis': f'{loss_disentangle.item():.4f}',\n",
        "                'MCD': f'{np.mean(batch_mcds):.2f}', # Avg MCD up to current batch\n",
        "                'S_ID_Con': f'{np.mean(batch_speaker_id_consistencies):.2f}', # Avg SID consistency\n",
        "                'Disc_Acc': f'{np.mean(batch_disentanglement_accuracies)*100:.1f}%', # Avg Disc Acc\n",
        "                'S_Acc': f'{100 * correct_speaker_id / total_speaker_id:.2f}%'\n",
        "            })\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_reco_loss = reco_loss_sum / len(dataloader)\n",
        "    avg_gender_loss = gender_loss_sum / len(dataloader)\n",
        "    avg_accent_loss = accent_loss_sum / len(dataloader)\n",
        "    avg_speaker_id_loss = speaker_id_loss_sum / len(dataloader)\n",
        "    avg_age_loss = age_loss_sum / len(dataloader)\n",
        "    avg_region_loss = region_loss_sum / len(dataloader)\n",
        "    avg_disentangle_loss = disentangle_loss_sum / len(dataloader)\n",
        "\n",
        "    gender_accuracy = correct_gender / total_gender if total_gender > 0 else 0\n",
        "    accent_accuracy = correct_accent / total_accent if total_accent > 0 else 0\n",
        "    speaker_id_accuracy = correct_speaker_id / total_speaker_id if total_speaker_id > 0 else 0\n",
        "    age_accuracy = correct_age / total_age if total_age > 0 else 0\n",
        "    region_accuracy = correct_region / total_region if total_region > 0 else 0\n",
        "\n",
        "    # Final averages for new metrics\n",
        "    avg_mcd = np.mean(batch_mcds) if batch_mcds else 0.0\n",
        "    avg_speaker_id_consistency = np.mean(batch_speaker_id_consistencies) if batch_speaker_id_consistencies else 0.0\n",
        "    avg_disentanglement_accuracy = np.mean(batch_disentanglement_accuracies) if batch_disentanglement_accuracies else 0.0\n",
        "\n",
        "\n",
        "    ###\n",
        "    reconstructed_mel, encoded_outputs = model(mel_spec)\n",
        "    source_audio_batch = vocoder.generate_audio(mel_spec, Config.SAMPLE_RATE, output_dir=Config.vocoder_wave_output_dir)\n",
        "    reconstructed_audio_batch = vocoder.generate_audio(reconstructed_mel, Config.SAMPLE_RATE, output_dir=Config.vocoder_wave_output_dir)\n",
        "\n",
        "    print('source_audio_batch= ',source_audio_batch.shape)\n",
        "    print('reconstructed_audio_batch= ',reconstructed_audio_batch.shape)\n",
        "\n",
        "\n",
        "    ###########\n",
        "    reference_audio_batch= source_audio_batch # MRH: it should be changed later\n",
        "                # Evaluate the batch\n",
        "    batch_metrics = vc_evaluator.evaluate_batch(\n",
        "                source_audio_batch,\n",
        "                reconstructed_audio_batch,\n",
        "                original_mel=mel_spec,\n",
        "                converted_mel=reconstructed_mel\n",
        "            )\n",
        "\n",
        "            # Update progress bar with new metrics\n",
        "    pbar.set_postfix({\n",
        "                'V_Loss': f'{current_batch_loss.item():.4f}',\n",
        "                'Reco': f'{loss_reco.item():.4f}',\n",
        "                'Dis': f'{loss_disentangle.item():.4f}',\n",
        "                'MCD': f'{batch_metrics.get(\"mcd\", 0):.2f}',\n",
        "                'F0_RMSE': f'{batch_metrics.get(\"f0_rmse\", 0):.2f}',\n",
        "                'Spk_Sim': f'{batch_metrics.get(\"speaker_similarity\", 0):.3f}',\n",
        "                'S_Acc': f'{100 * correct_speaker_id / total_speaker_id:.2f}%'\n",
        "            })\n",
        "\n",
        "            # ... rest of your existing code ...\n",
        "    metrics_summary = vc_evaluator.get_summary()\n",
        "\n",
        "    ###########\n",
        "\n",
        "    # Return all metrics, including the new ones\n",
        "    # Return all metrics, including the new ones\n",
        "    return {\n",
        "        # Loss metrics\n",
        "        'avg_loss': avg_loss,\n",
        "        'avg_reco_loss': avg_reco_loss,\n",
        "        'avg_gender_loss': avg_gender_loss,\n",
        "        'avg_accent_loss': avg_accent_loss,\n",
        "        'avg_speaker_id_loss': avg_speaker_id_loss,\n",
        "        'avg_age_loss': avg_age_loss,\n",
        "        'avg_region_loss': avg_region_loss,\n",
        "        'avg_disentangle_loss': avg_disentangle_loss,\n",
        "\n",
        "        # Accuracy metrics\n",
        "        'gender_accuracy': gender_accuracy,\n",
        "        'accent_accuracy': accent_accuracy,\n",
        "        'speaker_id_accuracy': speaker_id_accuracy,\n",
        "        'age_accuracy': age_accuracy,\n",
        "        'region_accuracy': region_accuracy,\n",
        "\n",
        "        # Voice conversion metrics (from evaluator)\n",
        "        'mcd': metrics_summary.get('mcd', {}).get('mean', 0),\n",
        "        'speaker_similarity': metrics_summary.get('speaker_similarity', {}).get('mean', 0),\n",
        "        'f0_rmse': metrics_summary.get('f0_rmse', {}).get('mean', 0),\n",
        "        'f0_correlation': metrics_summary.get('f0_correlation', {}).get('mean', 0),\n",
        "        'vuv_error': metrics_summary.get('vuv_error', {}).get('mean', 0),\n",
        "        'energy_rmse': metrics_summary.get('energy_rmse', {}).get('mean', 0),\n",
        "        'pesq': metrics_summary.get('pesq', {}).get('mean', 0),  # Added PESQ\n",
        "        'stoi': metrics_summary.get('stoi', {}).get('mean', 0),  # Added STOI\n",
        "\n",
        "        # Additional metrics from your original return\n",
        "        'avg_mcd':avg_mcd,\n",
        "        'avg_speaker_id_consistency': avg_speaker_id_consistency,\n",
        "        'avg_disentanglement_accuracy': avg_disentanglement_accuracy,\n",
        "\n",
        "        # Content preservation metrics (to be calculated separately)\n",
        "        'cer': 0.0,  # Placeholder - you'll need to implement CER calculation\n",
        "        'wer': 0.0,   # Placeholder - you'll need to implement WER calculation\n",
        "\n",
        "        # Full metrics summary for detailed analysis\n",
        "        'full_metrics_summary': metrics_summary\n",
        "    }\n"
      ],
      "metadata": {
        "id": "_C6lqrQGJzxO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment Configurations ---\n",
        "experiment_configs = [\n",
        "    {\n",
        "        \"name\": \"Baseline_Dims\",\n",
        "        \"Z_CONTENT_DIM\": 256, \"Z_GENDER_DIM\": 16, \"Z_ACCENT_DIM\": 64,\n",
        "        \"Z_SPEAKER_ID_DIM\": 256, \"Z_AGE_DIM\": 32, \"Z_REGION_DIM\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Smaller_All_Dims\",\n",
        "        \"Z_CONTENT_DIM\": 128, \"Z_GENDER_DIM\": 8, \"Z_ACCENT_DIM\": 32,\n",
        "        \"Z_SPEAKER_ID_DIM\": 128, \"Z_AGE_DIM\": 16, \"Z_REGION_DIM\": 32\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Larger_All_Dims\",\n",
        "        \"Z_CONTENT_DIM\": 512, \"Z_GENDER_DIM\": 32, \"Z_ACCENT_DIM\": 128,\n",
        "        \"Z_SPEAKER_ID_DIM\": 512, \"Z_AGE_DIM\": 64, \"Z_REGION_DIM\": 128\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Small_Content_Large_SpeakerID\",\n",
        "        \"Z_CONTENT_DIM\": 64, \"Z_GENDER_DIM\": 16, \"Z_ACCENT_DIM\": 64,\n",
        "        \"Z_SPEAKER_ID_DIM\": 512, \"Z_AGE_DIM\": 32, \"Z_REGION_DIM\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Large_Content_Small_SpeakerID\",\n",
        "        \"Z_CONTENT_DIM\": 512, \"Z_GENDER_DIM\": 16, \"Z_ACCENT_DIM\": 64,\n",
        "        \"Z_SPEAKER_ID_DIM\": 64, \"Z_AGE_DIM\": 32, \"Z_REGION_DIM\": 64\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Minimal_Attributes\",\n",
        "        \"Z_CONTENT_DIM\": 256, \"Z_GENDER_DIM\": 4, \"Z_ACCENT_DIM\": 16,\n",
        "        \"Z_SPEAKER_ID_DIM\": 256, \"Z_AGE_DIM\": 8, \"Z_REGION_DIM\": 16\n",
        "    },\n",
        "]\n",
        "\n",
        "# Ensure a directory for experiment results exists\n",
        "results_output_dir = os.path.join(Config.VC_Output, \"experiment_results\")\n",
        "os.makedirs(results_output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "E6xGaaFdMWmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Experiment Configurations ---\n",
        "experiment_configs = [\n",
        "\n",
        "    {\n",
        "        \"name\": \"Minimal_Attributes\",\n",
        "        \"Z_CONTENT_DIM\": 256, \"Z_GENDER_DIM\": 4, \"Z_ACCENT_DIM\": 16,\n",
        "        \"Z_SPEAKER_ID_DIM\": 256, \"Z_AGE_DIM\": 8, \"Z_REGION_DIM\": 16\n",
        "    },\n",
        "]\n",
        "\n",
        "# Ensure a directory for experiment results exists\n",
        "results_output_dir = os.path.join(Config.VC_Output, \"experiment_results\")\n",
        "os.makedirs(results_output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "r11w8dANXXwx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Config.NUM_EPOCHS=100"
      ],
      "metadata": {
        "id": "kFaChOjHNQ6V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np # Import numpy if you're using it (even if not explicitly, PyTorch can produce numpy types)\n",
        "# import torch # Uncomment if you're using PyTorch\n",
        "\n",
        "def find_unserializable(data, path=\"root\"):\n",
        "    if isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            find_unserializable(v, f\"{path}.{k}\")\n",
        "    elif isinstance(data, list):\n",
        "        for i, item in enumerate(data):\n",
        "            find_unserializable(item, f\"{path}[{i}]\")\n",
        "    else:\n",
        "        # Check for types that are known to cause issues\n",
        "        if isinstance(data, np.float32) or isinstance(data, np.float64):\n",
        "            print(f\"ERROR: Found numpy float at {path} - type: {type(data)} value: {data}\")\n",
        "            # You can also raise an error here if you want to stop execution\n",
        "            # raise TypeError(f\"Object of type {type(data)} at {path} is not JSON serializable\")\n",
        "        # elif isinstance(data, torch.Tensor): # Uncomment if using PyTorch\n",
        "        #     if data.ndim == 0:\n",
        "        #         print(f\"ERROR: Found scalar PyTorch tensor at {path} - type: {type(data)} value: {data.item()}\")\n",
        "        #     else:\n",
        "        #         print(f\"ERROR: Found non-scalar PyTorch tensor at {path} - type: {type(data)} shape: {data.shape}\")\n",
        "        elif not isinstance(data, (str, int, float, bool, type(None))):\n",
        "            # General check for any other non-standard Python type not handled by json\n",
        "            print(f\"WARNING: Found potentially unserializable type at {path} - type: {type(data)} value: {data}\")\n"
      ],
      "metadata": {
        "id": "T-I48R707vRx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "for exp_idx, exp_config in enumerate(experiment_configs):\n",
        "    exp_name = exp_config[\"name\"]\n",
        "    print(f\"\\n{'='*50}\\n--- Starting Experiment {exp_idx + 1}/{len(experiment_configs)}: {exp_name} ---\\n{'='*50}\")\n",
        "\n",
        "    # --- Dynamically update Config for the current experiment ---\n",
        "    Config.Z_CONTENT_DIM = exp_config[\"Z_CONTENT_DIM\"]\n",
        "    Config.Z_GENDER_DIM = exp_config[\"Z_GENDER_DIM\"]\n",
        "    Config.Z_ACCENT_DIM = exp_config[\"Z_ACCENT_DIM\"]\n",
        "    Config.Z_SPEAKER_ID_DIM = exp_config[\"Z_SPEAKER_ID_DIM\"]\n",
        "    Config.Z_AGE_DIM = exp_config[\"Z_AGE_DIM\"]\n",
        "    Config.Z_REGION_DIM = exp_config[\"Z_REGION_DIM\"]\n",
        "\n",
        "    print(f\"Current Config Dims: \"\n",
        "          f\"Z_CONT={Config.Z_CONTENT_DIM}, Z_GND={Config.Z_GENDER_DIM}, Z_ACC={Config.Z_ACCENT_DIM}, \"\n",
        "          f\"Z_SPK={Config.Z_SPEAKER_ID_DIM}, Z_AGE={Config.Z_AGE_DIM}, Z_REG={Config.Z_REGION_DIM}\")\n",
        "\n",
        "    # --- Re-initialize Model and Optimizer for each experiment ---\n",
        "    # This is CRITICAL because model architecture depends on Config dimensions.\n",
        "    # Make sure num_genders, num_accents, etc., are defined globally before this loop\n",
        "    model = VCModel(Config, num_genders, num_accents, num_speakers, num_ages, num_regions).to(Config.DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
        "\n",
        "    # --- Reset best_val_loss and all metric storage lists for the new experiment ---\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    train_losses = []\n",
        "    train_reco_losses = []\n",
        "    train_gender_losses = []\n",
        "    train_accent_losses = []\n",
        "    train_speaker_id_losses = []\n",
        "    train_age_losses = []\n",
        "    train_region_losses = []\n",
        "    train_disentangle_losses = []\n",
        "    train_gender_accs = []\n",
        "    train_accent_accs = []\n",
        "    train_speaker_id_accs = []\n",
        "    train_age_accs = []\n",
        "    train_region_accs = []\n",
        "    #\n",
        "\n",
        "    val_losses = []\n",
        "    val_reco_losses = []\n",
        "    val_gender_losses = []\n",
        "    val_accent_losses = []\n",
        "    val_speaker_id_losses = []\n",
        "    val_age_losses = []\n",
        "    val_region_losses = []\n",
        "    val_disentangle_losses = []\n",
        "    val_gender_accs = []\n",
        "    val_accent_accs = []\n",
        "    val_speaker_id_accs = []\n",
        "    val_age_accs = []\n",
        "    val_region_accs = []\n",
        "    val_average_mcds = []\n",
        "    val_average_speaker_id_consistencys = []\n",
        "    val_disentanglement_accuracys = []\n",
        "    #\n",
        "    val_f0_rmses = []\n",
        "    val_f0_correlations = []\n",
        "    val_vuv_errors = []\n",
        "    val_energy_rmses = []\n",
        "    val_pesq_summarys = []\n",
        "    val_stoi_summarys = []\n",
        "\n",
        "    # Re-initialize evaluators if they depend on Config values (like N_MELS, num_speakers)\n",
        "    # (Your current evaluators are fine, but keep in mind for future changes)\n",
        "    mcd_evaluator = MCDEvaluator(sr=Config.SAMPLE_RATE, n_mels=Config.N_MELS, device=Config.DEVICE)\n",
        "    speaker_id_consistency_evaluator = SpeakerIDConsistencyEvaluator(device=Config.DEVICE)\n",
        "    content_disentanglement_evaluator = ContentDisentanglementEvaluator(num_speakers=num_speakers, device=Config.DEVICE)\n",
        "\n",
        "    ### Vocoder\n",
        "    vocoder = Vocoder(Config.vocoder_directory)\n",
        "\n",
        "    ###\n",
        "    vc_evaluator = VoiceConversionEvaluator(\n",
        "        sample_rate=Config.SAMPLE_RATE,\n",
        "        n_mels=Config.N_MELS,\n",
        "        device=Config.DEVICE\n",
        "    )\n",
        "        # Audio processing parameters\n",
        "    SAMPLE_RATE = 16000 # Target sample rate for all audio (Hz)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Inner Training Loop (your original code, now inside the experiment loop) ---\n",
        "    print(\"\\nStarting full VCModel training for this experiment...\")\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Call train_epoch (make sure it's defined and takes the right args)\n",
        "        (train_loss, train_reco_loss, train_gender_loss, train_accent_loss,\n",
        "         train_speaker_id_loss, train_age_loss, train_region_loss, train_disentangle_loss,\n",
        "         train_gender_acc, train_accent_acc, train_speaker_id_acc,\n",
        "         train_age_acc, train_region_acc) = train_epoch(\n",
        "            model,\n",
        "            train_dataloader, optimizer,\n",
        "            reconstruction_criterion, gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "            age_criterion, region_criterion,\n",
        "            content_disentanglement_criterion,\n",
        "            Config.DEVICE\n",
        "        )\n",
        "\n",
        "        # Call validate_epoch (make sure it's defined and takes the right args)\n",
        "        # Call validate_epoch (make sure it's defined and takes the right args)\n",
        "        val_metrics = validate_epoch(\n",
        "            model,\n",
        "            val_dataloader,\n",
        "            reconstruction_criterion, gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "            age_criterion, region_criterion,\n",
        "            content_disentanglement_criterion,\n",
        "            Config.DEVICE # Pass device here as well, if validate_epoch expects it\n",
        "        )\n",
        "\n",
        "        # Extract all metrics from the dictionary\n",
        "        val_loss = val_metrics['avg_loss']\n",
        "        val_reco_loss = val_metrics['avg_reco_loss']\n",
        "        val_gender_loss = val_metrics['avg_gender_loss']\n",
        "        val_accent_loss = val_metrics['avg_accent_loss']\n",
        "        val_speaker_id_loss = val_metrics['avg_speaker_id_loss']\n",
        "        val_age_loss = val_metrics['avg_age_loss']\n",
        "        val_region_loss = val_metrics['avg_region_loss']\n",
        "        val_disentangle_loss = val_metrics['avg_disentangle_loss']\n",
        "        val_gender_acc = val_metrics['gender_accuracy']\n",
        "        val_accent_acc = val_metrics['accent_accuracy']\n",
        "        val_speaker_id_acc = val_metrics['speaker_id_accuracy']\n",
        "        val_age_acc = val_metrics['age_accuracy']\n",
        "        val_region_acc = val_metrics['region_accuracy']\n",
        "        val_average_mcd2 = val_metrics['mcd']\n",
        "        val_average_mcd1 = val_metrics['avg_mcd']\n",
        "        val_average_speaker_id_consistency = val_metrics['avg_speaker_id_consistency']\n",
        "        val_disentanglement_accuracy = val_metrics['avg_disentanglement_accuracy']\n",
        "        #\n",
        "        val_speaker_similarity_summary_mean = val_metrics['speaker_similarity']\n",
        "        val_f0_rmse_summary_mean = val_metrics['f0_rmse']\n",
        "        val_f0_correlation_summary_mean = val_metrics['f0_correlation']\n",
        "        val_vuv_error_summary_mean = val_metrics['vuv_error']\n",
        "        val_energy_rmse_summary_mean = val_metrics['energy_rmse']\n",
        "        val_pesq_summary_mean = val_metrics['pesq']  # Added PESQ\n",
        "        val_stoi_summary_mean = val_metrics['stoi']  # Added STOI\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        train_reco_losses.append(train_reco_loss)\n",
        "        train_gender_losses.append(train_gender_loss)\n",
        "        train_accent_losses.append(train_accent_loss)\n",
        "        train_speaker_id_losses.append(train_speaker_id_loss)\n",
        "        train_age_losses.append(train_age_loss)\n",
        "        train_region_losses.append(train_region_loss)\n",
        "        train_disentangle_losses.append(train_disentangle_loss)\n",
        "        train_gender_accs.append(train_gender_acc)\n",
        "        train_accent_accs.append(train_accent_acc)\n",
        "        train_speaker_id_accs.append(train_speaker_id_acc)\n",
        "        train_age_accs.append(train_age_acc)\n",
        "        train_region_accs.append(train_region_acc)\n",
        "        #\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_reco_losses.append(val_reco_loss)\n",
        "        val_gender_losses.append(val_gender_loss)\n",
        "        val_accent_losses.append(val_accent_loss)\n",
        "        val_speaker_id_losses.append(val_speaker_id_loss)\n",
        "        val_age_losses.append(val_age_loss)\n",
        "        val_region_losses.append(val_region_loss)\n",
        "        val_disentangle_losses.append(val_disentangle_loss)\n",
        "        val_gender_accs.append(val_gender_acc)\n",
        "        val_accent_accs.append(val_accent_acc)\n",
        "        val_speaker_id_accs.append(val_speaker_id_acc)\n",
        "        val_age_accs.append(val_age_acc)\n",
        "        val_region_accs.append(val_region_acc)\n",
        "        val_average_mcds.append(val_average_mcd1)\n",
        "        val_average_speaker_id_consistencys.append(val_average_speaker_id_consistency)\n",
        "        val_disentanglement_accuracys.append(val_disentanglement_accuracy)\n",
        "        #\n",
        "        val_f0_rmses.append(val_f0_rmse_summary_mean)\n",
        "        val_f0_correlations.append(val_f0_correlation_summary_mean)\n",
        "        val_vuv_errors.append(val_vuv_error_summary_mean)\n",
        "        val_energy_rmses.append(val_energy_rmse_summary_mean)\n",
        "        val_pesq_summarys.append(val_pesq_summary_mean)\n",
        "        val_stoi_summarys.append(val_stoi_summary_mean)\n",
        "\n",
        "\n",
        "        # Print current epoch results\n",
        "        print(f\"\\nEpoch {epoch+1}/{Config.NUM_EPOCHS}:\")\n",
        "        print(f\"  Train -> Total L: {train_loss:.4f} | Reco L: {train_reco_loss:.4f} | Dis L: {train_disentangle_loss:.4f} | \"\n",
        "              f\"G_L: {train_gender_loss:.4f} (A: {train_gender_acc*100:.2f}%) | \"\n",
        "              f\"A_L: {train_accent_loss:.4f} (A: {train_accent_acc*100:.2f}%) | \"\n",
        "              f\"S_L: {train_speaker_id_loss:.4f} (A: {train_speaker_id_acc*100:.2f}%) | \"\n",
        "              f\"Age_L: {train_age_loss:.4f} (A: {train_age_acc*100:.2f}%) | \"\n",
        "              f\"Region_L: {train_region_loss:.4f} (A: {train_region_acc*100:.2f}%)\"\n",
        "             )\n",
        "        print(f\"  Valid -> Total L: {val_loss:.4f} | Reco L: {val_reco_loss:.4f} | Dis L: {val_disentangle_loss:.4f} | \"\n",
        "              f\"G_L: {val_gender_loss:.4f} (A: {val_gender_acc*100:.2f}%) | \"\n",
        "              f\"A_L: {val_accent_loss:.4f} (A: {val_accent_acc*100:.2f}%) | \"\n",
        "              f\"S_L: {val_speaker_id_loss:.4f} (A: {val_speaker_id_acc*100:.2f}%) | \"\n",
        "              f\"Age_L: {val_age_loss:.4f} (A: {val_age_acc*100:.2f}%) | \"\n",
        "              f\"Region_L: {val_region_loss:.4f} (A: {val_region_acc*100:.2f}%) | \"\n",
        "              f\"average_mcd: {val_average_mcd1 :.4f} | \"\n",
        "              f\"average_speaker_id_consistency:{val_average_speaker_id_consistency:.4f} |\"\n",
        "              f\"disentanglement_accuracy:{val_disentanglement_accuracy:.4f} | \"\n",
        "             )\n",
        "        # Add the new summary metrics here:\n",
        "        print(f\"           Summary Metrics -> MCD: {val_average_mcd2:.4f} | Spk Sim: {val_speaker_similarity_summary_mean:.4f} | \")\n",
        "        print(f\"           F0 RMSE: {val_f0_rmse_summary_mean:.4f} | F0 Corr: {val_f0_correlation_summary_mean:.4f} | \")\n",
        "        print(f\"           VUV Error: {val_vuv_error_summary_mean:.4f} | Energy RMSE: {val_energy_rmse_summary_mean:.4f}\")\n",
        "        print(f\"           PESQ: {val_pesq_summary_mean:.4f} | STOI: {val_stoi_summary_mean:.4f}\")\n",
        "\n",
        "\n",
        "        # Save the best model based on validation loss for the CURRENT experiment\n",
        "        model_save_path_exp = os.path.join(Config.VC_Output, f\"best_vc_model_{exp_name}.pth\") # Unique filename\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), model_save_path_exp)\n",
        "            print(f\"  Model saved to {model_save_path_exp} (Validation Loss: {best_val_loss:.4f})\\n\")\n",
        "        else:\n",
        "            print(f\"  Validation loss did not improve from {best_val_loss:.4f}\\n\")\n",
        "\n",
        "    print(f\"\\n--- Training complete for Experiment: {exp_name}! ---\")\n",
        "\n",
        "\n",
        "    # --- Evaluate on Test Sets for the current experiment ---\n",
        "    print(f\"\\n--- Evaluating on Test Sets for Experiment: {exp_name} ---\")\n",
        "\n",
        "    # Load the best model for final evaluation of this experiment\n",
        "    if os.path.exists(model_save_path_exp): # Use the unique filename\n",
        "        model.load_state_dict(torch.load(model_save_path_exp, map_location=Config.DEVICE))\n",
        "        print(f\"Loaded best model from {model_save_path_exp} for final evaluation.\")\n",
        "    else:\n",
        "        print(f\"Warning: Best model not found at {model_save_path_exp}. Evaluating with current model state.\")\n",
        "\n",
        "    # In-Test Evaluation\n",
        "    # Call validate_epoch (make sure it's defined and takes the right args)\n",
        "    val_metrics = validate_epoch(\n",
        "        model,\n",
        "        val_dataloader,\n",
        "        reconstruction_criterion, gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "        age_criterion, region_criterion,\n",
        "        content_disentanglement_criterion,\n",
        "        Config.DEVICE # Pass device here as well, if validate_epoch expects it\n",
        "    )\n",
        "\n",
        "    # Extract all metrics from the dictionary\n",
        "    in_test_loss = val_metrics['avg_loss']\n",
        "    in_test_reco_loss = val_metrics['avg_reco_loss']\n",
        "    in_test_gender_loss = val_metrics['avg_gender_loss']\n",
        "    in_test_accent_loss = val_metrics['avg_accent_loss']\n",
        "    in_test_speaker_id_loss = val_metrics['avg_speaker_id_loss']\n",
        "    in_test_age_loss = val_metrics['avg_age_loss']\n",
        "    in_test_region_loss = val_metrics['avg_region_loss']\n",
        "    in_test_disentangle_loss = val_metrics['avg_disentangle_loss']\n",
        "    in_test_gender_acc = val_metrics['gender_accuracy']\n",
        "    in_test_accent_acc = val_metrics['accent_accuracy']\n",
        "    in_test_speaker_id_acc = val_metrics['speaker_id_accuracy']\n",
        "    in_test_age_acc = val_metrics['age_accuracy']\n",
        "    in_test_region_acc = val_metrics['region_accuracy']\n",
        "    in_test_average_mcd2 = val_metrics['mcd']\n",
        "    in_test_average_mcd1 = val_metrics['avg_mcd']\n",
        "    in_test_average_speaker_id_consistency = val_metrics['avg_speaker_id_consistency']\n",
        "    in_test_disentanglement_accuracy = val_metrics['avg_disentanglement_accuracy']\n",
        "    in_test_speaker_similarity_summary_mean = val_metrics['speaker_similarity']\n",
        "    in_test_f0_rmse_summary_mean = val_metrics['f0_rmse']\n",
        "    in_test_f0_correlation_summary_mean = val_metrics['f0_correlation']\n",
        "    in_test_vuv_error_summary_mean = val_metrics['vuv_error']\n",
        "    in_test_energy_rmse_summary_mean = val_metrics['energy_rmse']\n",
        "    in_test_pesq_summary_mean = val_metrics['pesq']  # Added PESQ\n",
        "    in_test_stoi_summary_mean = val_metrics['stoi']  # Added STOI\n",
        "\n",
        "\n",
        "    print(f\"\\nIn-Test Set ({exp_name}) -> Total L: {in_test_loss:.4f} | Reco L: {in_test_reco_loss:.4f} | Dis L: {in_test_disentangle_loss:.4f} | \"\n",
        "          f\"G_L: {in_test_gender_loss:.4f} (A: {in_test_gender_acc*100:.2f}%) | \"\n",
        "          f\"A_L: {in_test_accent_loss:.4f} (A: {in_test_accent_acc*100:.2f}%) | \"\n",
        "          f\"S_L: {in_test_speaker_id_loss:.4f} (A: {in_test_speaker_id_acc*100:.2f}%) | \"\n",
        "          f\"Age_L: {in_test_age_loss:.4f} (A: {in_test_age_acc*100:.2f}%) | \"\n",
        "          f\"Region_L: {in_test_region_loss:.4f} (A: {in_test_region_acc*100:.2f}%) | \"\n",
        "          f\"average_mcd:{in_test_average_mcd1:.4f} |\"\n",
        "          f\"average_speaker_id_consistency: {in_test_average_speaker_id_consistency:.4f} | \"\n",
        "          f\"disentanglement_accuracy:{in_test_disentanglement_accuracy:.4f} | \"\n",
        "         )\n",
        "    # Add the new summary metrics here:\n",
        "    print(f\"           Summary Metrics -> MCD: {in_test_average_mcd1:.4f} | Spk Sim: {in_test_speaker_similarity_summary_mean:.4f} | \")\n",
        "    print(f\"           F0 RMSE: {in_test_f0_rmse_summary_mean:.4f} | F0 Corr: {in_test_f0_correlation_summary_mean:.4f} | \")\n",
        "    print(f\"           VUV Error: {in_test_vuv_error_summary_mean:.4f} | Energy RMSE: {in_test_energy_rmse_summary_mean:.4f}\")\n",
        "    print(f\"           PESQ: {in_test_pesq_summary_mean:.4f} | STOI: {in_test_stoi_summary_mean:.4f}\")\n",
        "\n",
        "    # Out-Test Evaluation\n",
        "    # Call validate_epoch (make sure it's defined and takes the right args)\n",
        "    val_metrics = validate_epoch(\n",
        "        model,\n",
        "        val_dataloader,\n",
        "        reconstruction_criterion, gender_criterion, accent_criterion, speaker_id_criterion,\n",
        "        age_criterion, region_criterion,\n",
        "        content_disentanglement_criterion,\n",
        "        Config.DEVICE # Pass device here as well, if validate_epoch expects it\n",
        "    )\n",
        "\n",
        "    # Extract all metrics from the dictionary\n",
        "    out_test_loss = val_metrics['avg_loss']\n",
        "    out_test_reco_loss = val_metrics['avg_reco_loss']\n",
        "    out_test_gender_loss = val_metrics['avg_gender_loss']\n",
        "    out_test_accent_loss = val_metrics['avg_accent_loss']\n",
        "    out_test_speaker_id_loss = val_metrics['avg_speaker_id_loss']\n",
        "    out_test_age_loss = val_metrics['avg_age_loss']\n",
        "    out_test_region_loss = val_metrics['avg_region_loss']\n",
        "    out_test_disentangle_loss = val_metrics['avg_disentangle_loss']\n",
        "    out_test_gender_acc = val_metrics['gender_accuracy']\n",
        "    out_test_accent_acc = val_metrics['accent_accuracy']\n",
        "    out_test_speaker_id_acc = val_metrics['speaker_id_accuracy']\n",
        "    out_test_age_acc = val_metrics['age_accuracy']\n",
        "    out_test_region_acc = val_metrics['region_accuracy']\n",
        "    out_test_average_mcd2 = val_metrics['mcd']\n",
        "    out_test_average_mcd1 = val_metrics['avg_mcd']\n",
        "    out_test_average_speaker_id_consistency = val_metrics['avg_speaker_id_consistency']\n",
        "    out_test_disentanglement_accuracy = val_metrics['avg_disentanglement_accuracy']\n",
        "    out_test_speaker_similarity_summary_mean = val_metrics['speaker_similarity']\n",
        "    out_test_f0_rmse_summary_mean = val_metrics['f0_rmse']\n",
        "    out_test_f0_correlation_summary_mean = val_metrics['f0_correlation']\n",
        "    out_test_vuv_error_summary_mean = val_metrics['vuv_error']\n",
        "    out_test_energy_rmse_summary_mean = val_metrics['energy_rmse']\n",
        "    out_test_pesq_summary_mean = val_metrics['pesq']  # Added PESQ\n",
        "    out_test_stoi_summary_mean = val_metrics['stoi']  # Added STOI\n",
        "\n",
        "    print(f\"\\nOut-Test Set ({exp_name}) -> Total L: {out_test_loss:.4f} | Reco L: {out_test_reco_loss:.4f} | Dis L: {out_test_disentangle_loss:.4f} | \"\n",
        "          f\"G_L: {out_test_gender_loss:.4f} (A: {out_test_gender_acc*100:.2f}%) | \"\n",
        "          f\"A_L: {out_test_accent_loss:.4f} (A: {out_test_accent_acc*100:.2f}%) | \"\n",
        "          f\"S_L: {out_test_speaker_id_loss:.4f} (A: {out_test_speaker_id_acc*100:.2f}%) | \"\n",
        "          f\"Age_L: {out_test_age_loss:.4f} (A: {out_test_age_acc*100:.2f}%) | \"\n",
        "          f\"Region_L: {out_test_region_loss:.4f} (A: {out_test_region_acc*100:.2f}%) | \"\n",
        "          f\"average_mcd:{out_test_average_mcd1:.4f} | \"\n",
        "          f\"average_speaker_id_consistency: {out_test_average_speaker_id_consistency:.4f} | \"\n",
        "          f\"disentanglement_accuracy:{out_test_disentanglement_accuracy:.4f} | \"\n",
        "         )\n",
        "\n",
        "    # Add the new summary metrics here:\n",
        "    print(f\"           Summary Metrics -> MCD: {out_test_average_mcd1:.4f} | Spk Sim: {out_test_speaker_similarity_summary_mean:.4f} | \")\n",
        "    print(f\"           F0 RMSE: {out_test_f0_rmse_summary_mean:.4f} | F0 Corr: {out_test_f0_correlation_summary_mean:.4f} | \")\n",
        "    print(f\"           VUV Error: {out_test_vuv_error_summary_mean:.4f} | Energy RMSE: {out_test_energy_rmse_summary_mean:.4f}\")\n",
        "    print(f\"           PESQ: {out_test_pesq_summary_mean:.4f} | STOI: {out_test_stoi_summary_mean:.4f}\")\n",
        "\n",
        "    # --- Save results for the current experiment ---\n",
        "\n",
        "    # experiment_results = {\n",
        "    #     \"experiment_config\": exp_config,\n",
        "    #     \"val_average_mcds\": val_average_mcds,\n",
        "    #     \"val_average_speaker_id_consistencys\": val_average_speaker_id_consistencys,\n",
        "    #     \"val_disentanglement_accuracys\": val_disentanglement_accuracys,\n",
        "    #     \"validation_metrics\": {\n",
        "    #         \"losses\": val_losses,\n",
        "    #         \"reco_losses\": val_reco_losses,\n",
        "    #         \"gender_accs\": val_gender_accs,\n",
        "    #         \"accent_accs\": val_accent_accs,\n",
        "    #         \"speaker_id_accs\": val_speaker_id_accs,\n",
        "    #         \"age_accs\": val_age_accs,\n",
        "    #         \"region_accs\": val_region_accs,\n",
        "    #         #\n",
        "    #         \"val_f0_rmses\": val_f0_rmses,\n",
        "    #         \"val_f0_correlations\": val_f0_correlations,\n",
        "    #         \"val_vuv_errors\": val_vuv_errors,\n",
        "    #         \"val_energy_rmses\": val_energy_rmses,\n",
        "    #         \"val_pesq_summarys\": val_pesq_summarys,\n",
        "    #         \"val_stoi_summarys\": val_stoi_summarys,\n",
        "\n",
        "    #     },\n",
        "    #     \"in_test_metrics\": {\n",
        "    #         \"total_loss\": in_test_loss,\n",
        "    #         \"reco_loss\": in_test_reco_loss,\n",
        "    #         \"gender_acc\": in_test_gender_acc,\n",
        "    #         \"accent_acc\": in_test_accent_acc,\n",
        "    #         \"speaker_id_acc\": in_test_speaker_id_acc,\n",
        "    #         \"age_acc\": in_test_age_acc,\n",
        "    #         \"region_acc\": in_test_region_acc,\n",
        "    #         \"average_mcd\": in_test_average_mcd1,\n",
        "    #         \"speaker_id_consistency\": in_test_average_speaker_id_consistency,\n",
        "    #         \"disentanglement_accuracy\": in_test_disentanglement_accuracy,\n",
        "    #         #\n",
        "    #         \"in_test_f0_rmses\": in_test_f0_rmse_summary_mean,\n",
        "    #         \"in_test_f0_correlations\": in_test_f0_correlation_summary_mean,\n",
        "    #         \"in_test_vuv_errors\": in_test_vuv_error_summary_mean,\n",
        "    #         \"in_test_energy_rmses\": in_test_energy_rmse_summary_mean,\n",
        "    #         \"in_test_pesq_summarys\": in_test_pesq_summary_mean,\n",
        "    #         \"in_test_stoi_summarys\": in_test_stoi_summary_mean,\n",
        "\n",
        "    #     },\n",
        "    #     \"out_test_metrics\": {\n",
        "    #         \"total_loss\": out_test_loss,\n",
        "    #         \"reco_loss\": out_test_reco_loss,\n",
        "    #         \"gender_acc\": out_test_gender_acc,\n",
        "    #         \"accent_acc\": out_test_accent_acc,\n",
        "    #         \"speaker_id_acc\": out_test_speaker_id_acc,\n",
        "    #         \"age_acc\": out_test_age_acc,\n",
        "    #         \"region_acc\": out_test_region_acc,\n",
        "    #         \"average_mcd\": out_test_average_mcd1,\n",
        "    #         \"speaker_id_consistency\": out_test_average_speaker_id_consistency,\n",
        "    #         \"disentanglement_accuracy\": out_test_disentanglement_accuracy,\n",
        "    #         #\n",
        "    #         \"out_test_f0_rmses\": out_test_f0_rmse_summary_mean,\n",
        "    #         \"out_test_f0_correlations\": out_test_f0_correlation_summary_mean,\n",
        "    #         \"out_test_vuv_errors\": out_test_vuv_error_summary_mean,\n",
        "    #         \"out_test_energy_rmses\": out_test_energy_rmse_summary_mean,\n",
        "    #         \"out_test_pesq_summarys\": out_test_pesq_summary_mean,\n",
        "    #         \"out_test_stoi_summarys\": out_test_stoi_summary_mean,\n",
        "    #     }\n",
        "    # }\n",
        "\n",
        "    # results_filename = os.path.join(results_output_dir, f\"results_{exp_name}.json\")\n",
        "    # with open(results_filename, 'w') as f:\n",
        "    #     json.dump(experiment_results, f, indent=4)\n",
        "    # print(f\"\\nResults for Experiment '{exp_name}' saved to {results_filename}\\n\")\n",
        "\n",
        "    # Assuming you have already defined all these variables somewhere above\n",
        "    # exp_config, val_average_mcds, etc.\n",
        "    # in_test_loss, in_test_reco_loss, etc.\n",
        "    # out_test_loss, out_test_reco_loss, etc.\n",
        "\n",
        "\n",
        "    experiment_results = {\n",
        "        \"experiment_config\": exp_config,\n",
        "        # Convert elements of these lists\n",
        "        \"val_average_mcds\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_average_mcds],\n",
        "        \"val_average_speaker_id_consistencys\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_average_speaker_id_consistencys],\n",
        "        \"val_disentanglement_accuracys\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_disentanglement_accuracys],\n",
        "        \"validation_metrics\": {\n",
        "            # Convert elements of these lists using list comprehensions\n",
        "            \"losses\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_losses],\n",
        "            \"reco_losses\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_reco_losses],\n",
        "            \"gender_accs\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_gender_accs],\n",
        "            \"accent_accs\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_accent_accs],\n",
        "            \"speaker_id_accs\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_speaker_id_accs],\n",
        "            \"age_accs\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_age_accs],\n",
        "            \"region_accs\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_region_accs],\n",
        "            #\n",
        "            \"val_f0_rmses\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_f0_rmses],\n",
        "            \"val_f0_correlations\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_f0_correlations],\n",
        "            \"val_vuv_errors\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_vuv_errors],\n",
        "            \"val_energy_rmses\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_energy_rmses],\n",
        "            \"val_pesq_summarys\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_pesq_summarys],\n",
        "            \"val_stoi_summarys\": [float(x.item()) if hasattr(x, 'item') else float(x) for x in val_stoi_summarys],\n",
        "        },\n",
        "        \"in_test_metrics\": {\n",
        "            # Apply the conversion here for each scalar value\n",
        "            \"total_loss\": in_test_loss.item() if hasattr(in_test_loss, 'item') else float(in_test_loss),\n",
        "            \"reco_loss\": in_test_reco_loss.item() if hasattr(in_test_reco_loss, 'item') else float(in_test_reco_loss),\n",
        "            \"gender_acc\": in_test_gender_acc.item() if hasattr(in_test_gender_acc, 'item') else float(in_test_gender_acc),\n",
        "            \"accent_acc\": in_test_accent_acc.item() if hasattr(in_test_accent_acc, 'item') else float(in_test_accent_acc),\n",
        "            \"speaker_id_acc\": in_test_speaker_id_acc.item() if hasattr(in_test_speaker_id_acc, 'item') else float(in_test_speaker_id_acc),\n",
        "            \"age_acc\": in_test_age_acc.item() if hasattr(in_test_age_acc, 'item') else float(in_test_age_acc),\n",
        "            \"region_acc\": in_test_region_acc.item() if hasattr(in_test_region_acc, 'item') else float(in_test_region_acc),\n",
        "            \"average_mcd\": in_test_average_mcd1.item() if hasattr(in_test_average_mcd1, 'item') else float(in_test_average_mcd1),\n",
        "            \"speaker_id_consistency\": in_test_average_speaker_id_consistency.item() if hasattr(in_test_average_speaker_id_consistency, 'item') else float(in_test_average_speaker_id_consistency),\n",
        "            \"disentanglement_accuracy\": in_test_disentanglement_accuracy.item() if hasattr(in_test_disentanglement_accuracy, 'item') else float(in_test_disentanglement_accuracy),\n",
        "            #\n",
        "            \"in_test_f0_rmses\": in_test_f0_rmse_summary_mean.item() if hasattr(in_test_f0_rmse_summary_mean, 'item') else float(in_test_f0_rmse_summary_mean),\n",
        "            \"in_test_f0_correlations\": in_test_f0_correlation_summary_mean.item() if hasattr(in_test_f0_correlation_summary_mean, 'item') else float(in_test_f0_correlation_summary_mean),\n",
        "            \"in_test_vuv_errors\": in_test_vuv_error_summary_mean.item() if hasattr(in_test_vuv_error_summary_mean, 'item') else float(in_test_vuv_error_summary_mean),\n",
        "            \"in_test_energy_rmses\": in_test_energy_rmse_summary_mean.item() if hasattr(in_test_energy_rmse_summary_mean, 'item') else float(in_test_energy_rmse_summary_mean),\n",
        "            \"in_test_pesq_summarys\": in_test_pesq_summary_mean.item() if hasattr(in_test_pesq_summary_mean, 'item') else float(in_test_pesq_summary_mean),\n",
        "            \"in_test_stoi_summarys\": in_test_stoi_summary_mean.item() if hasattr(in_test_stoi_summary_mean, 'item') else float(in_test_stoi_summary_mean),\n",
        "        },\n",
        "        \"out_test_metrics\": {\n",
        "            # Apply the conversion here for each scalar value\n",
        "            \"total_loss\": out_test_loss.item() if hasattr(out_test_loss, 'item') else float(out_test_loss),\n",
        "            \"reco_loss\": out_test_reco_loss.item() if hasattr(out_test_reco_loss, 'item') else float(out_test_reco_loss),\n",
        "            \"gender_acc\": out_test_gender_acc.item() if hasattr(out_test_gender_acc, 'item') else float(out_test_gender_acc),\n",
        "            \"accent_acc\": out_test_accent_acc.item() if hasattr(out_test_accent_acc, 'item') else float(out_test_accent_acc),\n",
        "            \"speaker_id_acc\": out_test_speaker_id_acc.item() if hasattr(out_test_speaker_id_acc, 'item') else float(out_test_speaker_id_acc),\n",
        "            \"age_acc\": out_test_age_acc.item() if hasattr(out_test_age_acc, 'item') else float(out_test_age_acc),\n",
        "            \"region_acc\": out_test_region_acc.item() if hasattr(out_test_region_acc, 'item') else float(out_test_region_acc),\n",
        "            \"average_mcd\": out_test_average_mcd1.item() if hasattr(out_test_average_mcd1, 'item') else float(out_test_average_mcd1),\n",
        "            \"speaker_id_consistency\": out_test_average_speaker_id_consistency.item() if hasattr(out_test_average_speaker_id_consistency, 'item') else float(out_test_average_speaker_id_consistency),\n",
        "            \"disentanglement_accuracy\": out_test_disentanglement_accuracy.item() if hasattr(out_test_disentanglement_accuracy, 'item') else float(out_test_disentanglement_accuracy),\n",
        "            #\n",
        "            \"out_test_f0_rmses\": out_test_f0_rmse_summary_mean.item() if hasattr(out_test_f0_rmse_summary_mean, 'item') else float(out_test_f0_rmse_summary_mean),\n",
        "            \"out_test_f0_correlations\": out_test_f0_correlation_summary_mean.item() if hasattr(out_test_f0_correlation_summary_mean, 'item') else float(out_test_f0_correlation_summary_mean),\n",
        "            \"out_test_vuv_errors\": out_test_vuv_error_summary_mean.item() if hasattr(out_test_vuv_error_summary_mean, 'item') else float(out_test_vuv_error_summary_mean),\n",
        "            \"out_test_energy_rmses\": out_test_energy_rmse_summary_mean.item() if hasattr(out_test_energy_rmse_summary_mean, 'item') else float(out_test_energy_rmse_summary_mean),\n",
        "            \"out_test_pesq_summarys\": out_test_pesq_summary_mean.item() if hasattr(out_test_pesq_summary_mean, 'item') else float(out_test_pesq_summary_mean),\n",
        "            \"out_test_stoi_summarys\": out_test_stoi_summary_mean.item() if hasattr(out_test_stoi_summary_mean, 'item') else float(out_test_stoi_summary_mean),\n",
        "        }\n",
        "    }\n",
        "    # The rest of your saving code remains the same\n",
        "    find_unserializable(experiment_results)\n",
        "    results_filename = os.path.join(results_output_dir, f\"results_{exp_name}.json\")\n",
        "    with open(results_filename, 'w') as f:\n",
        "        json.dump(experiment_results, f, indent=4)\n",
        "    print(f\"\\nResults for Experiment '{exp_name}' saved to {results_filename}\\n\")\n",
        "\n",
        "print(\"\\nAll experiments completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYZlS_MzIEOy",
        "outputId": "c38a2432-264e-485d-9f58-f3537a6a6049"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "--- Starting Experiment 1/1: Minimal_Attributes ---\n",
            "==================================================\n",
            "Current Config Dims: Z_CONT=256, Z_GND=4, Z_ACC=16, Z_SPK=256, Z_AGE=8, Z_REG=16\n",
            "Loading vocoder configuration...\n",
            "Configuration loaded successfully.\n",
            "Initializing device...\n",
            "Device initialized to: cuda\n",
            "Loading HifiGAN generator model...\n",
            "Loading checkpoint from '/content/drive/MyDrive/29_MFCCGAN-VC/Vocoder/generator_v3'\n",
            "Checkpoint loaded.\n",
            "Removing weight norm...\n",
            "HifiGAN generator loaded and configured.\n",
            "Vocoder fully initialized and ready.\n",
            "\n",
            "Starting full VCModel training for this experiment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/41 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "Validation:   9%|▉         | 1/11 [00:45<07:30, 45.09s/it, V_Loss=7.4085, Reco=4.3150, Dis=4.5809, MCD=5.08, S_ID_Con=0.08, Disc_Acc=6.2%, S_Acc=21.88%]/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 1/100:\n",
            "  Train -> Total L: 7.0854 | Reco L: 4.0557 | Dis L: 4.6020 | G_L: 0.3715 (A: 80.48%) | A_L: 2.1206 (A: 30.48%) | S_L: 4.1457 (A: 20.60%) | Age_L: 2.4980 (A: 25.77%) | Region_L: 4.1186 (A: 3.70%)\n",
            "  Valid -> Total L: 7.1462 | Reco L: 4.0141 | Dis L: 4.5773 | G_L: 0.3743 (A: 84.57%) | A_L: 2.1767 (A: 30.56%) | S_L: 4.3279 (A: 16.36%) | Age_L: 2.5269 (A: 28.70%) | Region_L: 4.1459 (A: 2.78%) | average_mcd: 5.0270 | average_speaker_id_consistency:0.1798 |disentanglement_accuracy:0.0511 | \n",
            "           Summary Metrics -> MCD: 53.8485 | Spk Sim: 0.9905 | \n",
            "           F0 RMSE: 47.4214 | F0 Corr: 0.1936 | \n",
            "           VUV Error: 0.2161 | Energy RMSE: 6708.5586\n",
            "           PESQ: 1.8002 | STOI: 0.6064\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 7.1462)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/41 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 2/100:\n",
            "  Train -> Total L: 5.9915 | Reco L: 3.5758 | Dis L: 4.5485 | G_L: 0.1437 (A: 96.91%) | A_L: 1.7973 (A: 46.91%) | S_L: 3.1375 (A: 61.34%) | Age_L: 2.2426 (A: 40.28%) | Region_L: 3.8314 (A: 15.05%)\n",
            "  Valid -> Total L: 5.7369 | Reco L: 3.3617 | Dis L: 4.5237 | G_L: 0.2851 (A: 85.80%) | A_L: 1.8162 (A: 48.15%) | S_L: 3.0188 (A: 40.74%) | Age_L: 2.2931 (A: 37.35%) | Region_L: 3.8112 (A: 16.05%) | average_mcd: 4.2908 | average_speaker_id_consistency:0.3650 |disentanglement_accuracy:0.0597 | \n",
            "           Summary Metrics -> MCD: 47.0078 | Spk Sim: 0.9902 | \n",
            "           F0 RMSE: 58.0176 | F0 Corr: 0.0375 | \n",
            "           VUV Error: 0.2681 | Energy RMSE: 3487.9646\n",
            "           PESQ: 1.0892 | STOI: 0.6495\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 5.7369)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 3/100:\n",
            "  Train -> Total L: 5.0418 | Reco L: 3.1033 | Dis L: 4.4956 | G_L: 0.0957 (A: 98.61%) | A_L: 1.5334 (A: 57.25%) | S_L: 2.3309 (A: 80.25%) | Age_L: 2.0408 (A: 48.46%) | Region_L: 3.6108 (A: 26.23%)\n",
            "  Valid -> Total L: 4.9911 | Reco L: 2.8729 | Dis L: 4.4666 | G_L: 0.0745 (A: 98.46%) | A_L: 1.8359 (A: 40.43%) | S_L: 2.5201 (A: 49.07%) | Age_L: 2.5757 (A: 25.62%) | Region_L: 3.6487 (A: 17.59%) | average_mcd: 3.9061 | average_speaker_id_consistency:0.1630 |disentanglement_accuracy:0.0881 | \n",
            "           Summary Metrics -> MCD: 42.0067 | Spk Sim: 0.9901 | \n",
            "           F0 RMSE: 59.5263 | F0 Corr: 0.1054 | \n",
            "           VUV Error: 0.1763 | Energy RMSE: 3112.9785\n",
            "           PESQ: 1.1148 | STOI: 0.6613\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 4.9911)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 4/100:\n",
            "  Train -> Total L: 4.1344 | Reco L: 2.5789 | Dis L: 4.4386 | G_L: 0.0743 (A: 98.92%) | A_L: 1.3223 (A: 67.44%) | S_L: 1.6885 (A: 88.73%) | Age_L: 1.8638 (A: 53.94%) | Region_L: 3.4081 (A: 31.79%)\n",
            "  Valid -> Total L: 4.2004 | Reco L: 2.5394 | Dis L: 4.4143 | G_L: 0.0461 (A: 99.69%) | A_L: 1.4606 (A: 56.17%) | S_L: 1.8364 (A: 72.84%) | Age_L: 2.0061 (A: 42.90%) | Region_L: 3.4736 (A: 26.85%) | average_mcd: 3.7304 | average_speaker_id_consistency:0.1945 |disentanglement_accuracy:0.0739 | \n",
            "           Summary Metrics -> MCD: 38.3020 | Spk Sim: 0.9906 | \n",
            "           F0 RMSE: 60.6384 | F0 Corr: 0.0572 | \n",
            "           VUV Error: 0.1599 | Energy RMSE: 3151.7617\n",
            "           PESQ: 1.1204 | STOI: 0.6712\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 4.2004)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 5/100:\n",
            "  Train -> Total L: 3.3737 | Reco L: 2.1241 | Dis L: 4.3804 | G_L: 0.0627 (A: 99.00%) | A_L: 1.1387 (A: 73.38%) | S_L: 1.1872 (A: 94.21%) | Age_L: 1.7056 (A: 57.48%) | Region_L: 3.2147 (A: 40.20%)\n",
            "  Valid -> Total L: 3.2814 | Reco L: 1.8413 | Dis L: 4.3629 | G_L: 0.1380 (A: 97.53%) | A_L: 1.2406 (A: 58.33%) | S_L: 1.4871 (A: 84.88%) | Age_L: 1.8321 (A: 45.06%) | Region_L: 3.3186 (A: 27.78%) | average_mcd: 3.4181 | average_speaker_id_consistency:0.1299 |disentanglement_accuracy:0.0852 | \n",
            "           Summary Metrics -> MCD: 30.6200 | Spk Sim: 0.9911 | \n",
            "           F0 RMSE: 62.0680 | F0 Corr: -0.0426 | \n",
            "           VUV Error: 0.1646 | Energy RMSE: 3204.5686\n",
            "           PESQ: 1.1577 | STOI: 0.6865\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 3.2814)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 6/100:\n",
            "  Train -> Total L: 2.6847 | Reco L: 1.6700 | Dis L: 4.3283 | G_L: 0.0497 (A: 99.31%) | A_L: 0.9880 (A: 77.39%) | S_L: 0.8209 (A: 95.76%) | Age_L: 1.5359 (A: 63.89%) | Region_L: 3.0361 (A: 45.68%)\n",
            "  Valid -> Total L: 2.5571 | Reco L: 1.3647 | Dis L: 4.3172 | G_L: 0.0397 (A: 99.38%) | A_L: 1.0972 (A: 70.37%) | S_L: 1.0787 (A: 87.65%) | Age_L: 1.8644 (A: 44.14%) | Region_L: 3.0983 (A: 29.01%) | average_mcd: 3.2372 | average_speaker_id_consistency:0.0824 |disentanglement_accuracy:0.0568 | \n",
            "           Summary Metrics -> MCD: 24.5388 | Spk Sim: 0.9911 | \n",
            "           F0 RMSE: 57.1454 | F0 Corr: -0.0069 | \n",
            "           VUV Error: 0.1564 | Energy RMSE: 3547.5342\n",
            "           PESQ: 1.1904 | STOI: 0.7013\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 2.5571)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 7/100:\n",
            "  Train -> Total L: 2.2037 | Reco L: 1.3630 | Dis L: 4.2923 | G_L: 0.0438 (A: 99.69%) | A_L: 0.8361 (A: 83.26%) | S_L: 0.5704 (A: 97.30%) | Age_L: 1.3962 (A: 69.44%) | Region_L: 2.8496 (A: 53.86%)\n",
            "  Valid -> Total L: 2.4633 | Reco L: 1.2626 | Dis L: 4.3417 | G_L: 0.0352 (A: 99.69%) | A_L: 1.1164 (A: 64.51%) | S_L: 1.0731 (A: 84.26%) | Age_L: 2.0455 (A: 40.43%) | Region_L: 3.0105 (A: 35.49%) | average_mcd: 3.1622 | average_speaker_id_consistency:0.3626 |disentanglement_accuracy:0.0483 | \n",
            "           Summary Metrics -> MCD: 23.5383 | Spk Sim: 0.9909 | \n",
            "           F0 RMSE: 57.0884 | F0 Corr: -0.0096 | \n",
            "           VUV Error: 0.1601 | Energy RMSE: 3904.3347\n",
            "           PESQ: 1.2185 | STOI: 0.6971\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 2.4633)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 8/100:\n",
            "  Train -> Total L: 1.9397 | Reco L: 1.2157 | Dis L: 4.3072 | G_L: 0.0365 (A: 99.54%) | A_L: 0.7419 (A: 85.73%) | S_L: 0.4152 (A: 98.23%) | Age_L: 1.2647 (A: 74.54%) | Region_L: 2.6906 (A: 56.87%)\n",
            "  Valid -> Total L: 2.1825 | Reco L: 1.2039 | Dis L: 4.5144 | G_L: 0.0515 (A: 99.38%) | A_L: 1.4000 (A: 51.23%) | S_L: 0.6811 (A: 91.98%) | Age_L: 1.5743 (A: 62.65%) | Region_L: 2.9033 (A: 35.19%) | average_mcd: 3.1058 | average_speaker_id_consistency:0.2878 |disentanglement_accuracy:0.0284 | \n",
            "           Summary Metrics -> MCD: 23.2963 | Spk Sim: 0.9907 | \n",
            "           F0 RMSE: 56.7097 | F0 Corr: -0.0910 | \n",
            "           VUV Error: 0.1540 | Energy RMSE: 3300.7168\n",
            "           PESQ: 1.2527 | STOI: 0.7160\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 2.1825)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 9/100:\n",
            "  Train -> Total L: 1.7618 | Reco L: 1.1240 | Dis L: 4.2740 | G_L: 0.0292 (A: 99.69%) | A_L: 0.6410 (A: 87.96%) | S_L: 0.3185 (A: 99.07%) | Age_L: 1.1599 (A: 77.93%) | Region_L: 2.5282 (A: 63.19%)\n",
            "  Valid -> Total L: 1.9664 | Reco L: 1.0563 | Dis L: 4.3421 | G_L: 0.0266 (A: 100.00%) | A_L: 0.9314 (A: 65.74%) | S_L: 0.6646 (A: 90.74%) | Age_L: 1.5098 (A: 58.33%) | Region_L: 2.8765 (A: 41.05%) | average_mcd: 2.9740 | average_speaker_id_consistency:0.1818 |disentanglement_accuracy:0.0199 | \n",
            "           Summary Metrics -> MCD: 20.6926 | Spk Sim: 0.9901 | \n",
            "           F0 RMSE: 63.6656 | F0 Corr: 0.0232 | \n",
            "           VUV Error: 0.1349 | Energy RMSE: 3480.8242\n",
            "           PESQ: 1.3375 | STOI: 0.7434\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.9664)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 10/100:\n",
            "  Train -> Total L: 1.6035 | Reco L: 1.0401 | Dis L: 4.2242 | G_L: 0.0232 (A: 99.77%) | A_L: 0.5487 (A: 90.74%) | S_L: 0.2444 (A: 99.31%) | Age_L: 1.0332 (A: 81.40%) | Region_L: 2.3844 (A: 63.81%)\n",
            "  Valid -> Total L: 1.9621 | Reco L: 1.0564 | Dis L: 4.2630 | G_L: 0.0275 (A: 100.00%) | A_L: 0.8160 (A: 73.15%) | S_L: 0.6223 (A: 92.28%) | Age_L: 1.9007 (A: 46.60%) | Region_L: 2.7745 (A: 43.83%) | average_mcd: 2.9785 | average_speaker_id_consistency:0.2421 |disentanglement_accuracy:0.0398 | \n",
            "           Summary Metrics -> MCD: 20.8108 | Spk Sim: 0.9900 | \n",
            "           F0 RMSE: 64.3003 | F0 Corr: 0.0139 | \n",
            "           VUV Error: 0.1459 | Energy RMSE: 3423.0034\n",
            "           PESQ: 1.3197 | STOI: 0.7257\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.9621)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 11/100:\n",
            "  Train -> Total L: 1.4695 | Reco L: 0.9659 | Dis L: 4.1349 | G_L: 0.0172 (A: 99.85%) | A_L: 0.4743 (A: 92.59%) | S_L: 0.1942 (A: 99.38%) | Age_L: 0.9497 (A: 83.64%) | Region_L: 2.2112 (A: 68.44%)\n",
            "  Valid -> Total L: 1.8598 | Reco L: 1.0244 | Dis L: 4.2138 | G_L: 0.0362 (A: 99.69%) | A_L: 0.7467 (A: 81.17%) | S_L: 0.6678 (A: 88.27%) | Age_L: 1.3230 (A: 69.75%) | Region_L: 2.4880 (A: 42.90%) | average_mcd: 2.8979 | average_speaker_id_consistency:0.2829 |disentanglement_accuracy:0.0426 | \n",
            "           Summary Metrics -> MCD: 19.6795 | Spk Sim: 0.9893 | \n",
            "           F0 RMSE: 65.5591 | F0 Corr: -0.0084 | \n",
            "           VUV Error: 0.1304 | Energy RMSE: 3368.8040\n",
            "           PESQ: 1.3947 | STOI: 0.7456\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.8598)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 12/100:\n",
            "  Train -> Total L: 1.3629 | Reco L: 0.9080 | Dis L: 4.1264 | G_L: 0.0172 (A: 99.77%) | A_L: 0.4099 (A: 94.68%) | S_L: 0.1582 (A: 99.46%) | Age_L: 0.8498 (A: 86.57%) | Region_L: 2.0683 (A: 71.53%)\n",
            "  Valid -> Total L: 1.6986 | Reco L: 0.9194 | Dis L: 4.2603 | G_L: 0.0193 (A: 100.00%) | A_L: 0.7232 (A: 79.63%) | S_L: 0.5122 (A: 92.59%) | Age_L: 1.5559 (A: 54.63%) | Region_L: 2.5067 (A: 46.60%) | average_mcd: 2.8467 | average_speaker_id_consistency:0.3467 |disentanglement_accuracy:0.0398 | \n",
            "           Summary Metrics -> MCD: 18.3148 | Spk Sim: 0.9886 | \n",
            "           F0 RMSE: 63.3981 | F0 Corr: 0.0743 | \n",
            "           VUV Error: 0.1302 | Energy RMSE: 3299.1189\n",
            "           PESQ: 1.4070 | STOI: 0.7516\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.6986)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 13/100:\n",
            "  Train -> Total L: 1.2500 | Reco L: 0.8411 | Dis L: 4.0958 | G_L: 0.0179 (A: 99.85%) | A_L: 0.3636 (A: 94.44%) | S_L: 0.1220 (A: 99.77%) | Age_L: 0.7665 (A: 88.19%) | Region_L: 1.9220 (A: 73.30%)\n",
            "  Valid -> Total L: 1.6173 | Reco L: 0.8768 | Dis L: 4.3280 | G_L: 0.0265 (A: 99.07%) | A_L: 0.9784 (A: 66.36%) | S_L: 0.4491 (A: 90.74%) | Age_L: 1.2202 (A: 70.68%) | Region_L: 2.5024 (A: 47.53%) | average_mcd: 2.8117 | average_speaker_id_consistency:0.3578 |disentanglement_accuracy:0.0312 | \n",
            "           Summary Metrics -> MCD: 17.0719 | Spk Sim: 0.9881 | \n",
            "           F0 RMSE: 61.3920 | F0 Corr: 0.0571 | \n",
            "           VUV Error: 0.1277 | Energy RMSE: 3351.9893\n",
            "           PESQ: 1.4431 | STOI: 0.7572\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.6173)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 14/100:\n",
            "  Train -> Total L: 1.1865 | Reco L: 0.8185 | Dis L: 4.0179 | G_L: 0.0142 (A: 99.85%) | A_L: 0.3200 (A: 96.45%) | S_L: 0.0931 (A: 99.92%) | Age_L: 0.6987 (A: 90.12%) | Region_L: 1.7798 (A: 77.85%)\n",
            "  Valid -> Total L: 1.4120 | Reco L: 0.8536 | Dis L: 4.1483 | G_L: 0.0158 (A: 99.69%) | A_L: 0.6266 (A: 81.79%) | S_L: 0.2765 (A: 96.30%) | Age_L: 1.1015 (A: 69.44%) | Region_L: 2.0425 (A: 61.42%) | average_mcd: 2.8163 | average_speaker_id_consistency:0.3219 |disentanglement_accuracy:0.0483 | \n",
            "           Summary Metrics -> MCD: 16.8227 | Spk Sim: 0.9877 | \n",
            "           F0 RMSE: 58.1830 | F0 Corr: 0.2081 | \n",
            "           VUV Error: 0.1326 | Energy RMSE: 3397.8259\n",
            "           PESQ: 1.4523 | STOI: 0.7594\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.4120)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 15/100:\n",
            "  Train -> Total L: 1.1621 | Reco L: 0.8225 | Dis L: 4.0317 | G_L: 0.0125 (A: 99.85%) | A_L: 0.2644 (A: 97.61%) | S_L: 0.0811 (A: 99.85%) | Age_L: 0.6485 (A: 90.51%) | Region_L: 1.6621 (A: 78.55%)\n",
            "  Valid -> Total L: 1.6561 | Reco L: 0.8485 | Dis L: 4.0816 | G_L: 0.0605 (A: 99.07%) | A_L: 1.2650 (A: 55.25%) | S_L: 0.4958 (A: 91.67%) | Age_L: 1.1772 (A: 66.67%) | Region_L: 2.6855 (A: 25.93%) | average_mcd: 2.7904 | average_speaker_id_consistency:0.3861 |disentanglement_accuracy:0.0767 | \n",
            "           Summary Metrics -> MCD: 17.0729 | Spk Sim: 0.9882 | \n",
            "           F0 RMSE: 61.8747 | F0 Corr: 0.0880 | \n",
            "           VUV Error: 0.1467 | Energy RMSE: 2943.6401\n",
            "           PESQ: 1.4482 | STOI: 0.7632\n",
            "  Validation loss did not improve from 1.4120\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 16/100:\n",
            "  Train -> Total L: 1.0891 | Reco L: 0.7737 | Dis L: 3.9432 | G_L: 0.0138 (A: 99.69%) | A_L: 0.2530 (A: 97.69%) | S_L: 0.0736 (A: 100.00%) | Age_L: 0.5909 (A: 91.05%) | Region_L: 1.5342 (A: 81.79%)\n",
            "  Valid -> Total L: 1.4573 | Reco L: 0.8100 | Dis L: 3.9913 | G_L: 0.0139 (A: 100.00%) | A_L: 0.6084 (A: 80.25%) | S_L: 0.4244 (A: 90.74%) | Age_L: 0.9833 (A: 79.63%) | Region_L: 2.3462 (A: 53.09%) | average_mcd: 2.7699 | average_speaker_id_consistency:0.4171 |disentanglement_accuracy:0.0767 | \n",
            "           Summary Metrics -> MCD: 16.1070 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 55.5751 | F0 Corr: 0.1063 | \n",
            "           VUV Error: 0.1463 | Energy RMSE: 3221.2117\n",
            "           PESQ: 1.5072 | STOI: 0.7415\n",
            "  Validation loss did not improve from 1.4120\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 17/100:\n",
            "  Train -> Total L: 1.0428 | Reco L: 0.7516 | Dis L: 3.9507 | G_L: 0.0222 (A: 99.61%) | A_L: 0.2185 (A: 98.15%) | S_L: 0.0637 (A: 99.85%) | Age_L: 0.5212 (A: 93.29%) | Region_L: 1.4360 (A: 84.10%)\n",
            "  Valid -> Total L: 1.4314 | Reco L: 0.8044 | Dis L: 4.0505 | G_L: 0.0633 (A: 98.15%) | A_L: 0.5823 (A: 79.94%) | S_L: 0.3606 (A: 94.14%) | Age_L: 1.1298 (A: 67.28%) | Region_L: 2.2866 (A: 39.81%) | average_mcd: 2.7377 | average_speaker_id_consistency:0.4226 |disentanglement_accuracy:0.0597 | \n",
            "           Summary Metrics -> MCD: 15.6678 | Spk Sim: 0.9874 | \n",
            "           F0 RMSE: 48.8473 | F0 Corr: 0.1728 | \n",
            "           VUV Error: 0.1322 | Energy RMSE: 2920.2241\n",
            "           PESQ: 1.5273 | STOI: 0.7719\n",
            "  Validation loss did not improve from 1.4120\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 18/100:\n",
            "  Train -> Total L: 1.0174 | Reco L: 0.7527 | Dis L: 3.9037 | G_L: 0.0091 (A: 99.92%) | A_L: 0.1950 (A: 98.30%) | S_L: 0.0524 (A: 100.00%) | Age_L: 0.4797 (A: 93.21%) | Region_L: 1.3117 (A: 86.96%)\n",
            "  Valid -> Total L: 1.1947 | Reco L: 0.7689 | Dis L: 3.9251 | G_L: 0.0192 (A: 100.00%) | A_L: 0.3800 (A: 91.05%) | S_L: 0.1939 (A: 96.91%) | Age_L: 0.7481 (A: 81.17%) | Region_L: 1.7478 (A: 67.59%) | average_mcd: 2.7483 | average_speaker_id_consistency:0.4039 |disentanglement_accuracy:0.0795 | \n",
            "           Summary Metrics -> MCD: 15.4238 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 43.0997 | F0 Corr: 0.2212 | \n",
            "           VUV Error: 0.1513 | Energy RMSE: 3126.9504\n",
            "           PESQ: 1.5328 | STOI: 0.7669\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.1947)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 19/100:\n",
            "  Train -> Total L: 0.9777 | Reco L: 0.7319 | Dis L: 3.8679 | G_L: 0.0110 (A: 99.77%) | A_L: 0.1751 (A: 98.53%) | S_L: 0.0435 (A: 100.00%) | Age_L: 0.4438 (A: 94.06%) | Region_L: 1.2235 (A: 88.35%)\n",
            "  Valid -> Total L: 1.2401 | Reco L: 0.7928 | Dis L: 4.1331 | G_L: 0.0269 (A: 99.38%) | A_L: 0.3771 (A: 89.20%) | S_L: 0.2055 (A: 96.60%) | Age_L: 1.1257 (A: 64.20%) | Region_L: 1.5021 (A: 68.21%) | average_mcd: 2.7952 | average_speaker_id_consistency:0.4362 |disentanglement_accuracy:0.0455 | \n",
            "           Summary Metrics -> MCD: 14.8064 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 43.5590 | F0 Corr: 0.0588 | \n",
            "           VUV Error: 0.1474 | Energy RMSE: 3228.7451\n",
            "           PESQ: 1.5337 | STOI: 0.7397\n",
            "  Validation loss did not improve from 1.1947\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 20/100:\n",
            "  Train -> Total L: 0.9730 | Reco L: 0.7426 | Dis L: 3.8414 | G_L: 0.0182 (A: 99.46%) | A_L: 0.1583 (A: 98.69%) | S_L: 0.0400 (A: 100.00%) | Age_L: 0.4125 (A: 94.06%) | Region_L: 1.1300 (A: 89.74%)\n",
            "  Valid -> Total L: 1.5146 | Reco L: 0.8545 | Dis L: 3.8801 | G_L: 0.0350 (A: 98.77%) | A_L: 1.4763 (A: 56.17%) | S_L: 0.3119 (A: 94.14%) | Age_L: 1.1978 (A: 67.90%) | Region_L: 1.9449 (A: 42.90%) | average_mcd: 2.8254 | average_speaker_id_consistency:0.4839 |disentanglement_accuracy:0.0767 | \n",
            "           Summary Metrics -> MCD: 15.3826 | Spk Sim: 0.9863 | \n",
            "           F0 RMSE: 49.6220 | F0 Corr: -0.1017 | \n",
            "           VUV Error: 0.1303 | Energy RMSE: 4286.1387\n",
            "           PESQ: 1.5171 | STOI: 0.7688\n",
            "  Validation loss did not improve from 1.1947\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 21/100:\n",
            "  Train -> Total L: 0.9360 | Reco L: 0.7238 | Dis L: 3.7830 | G_L: 0.0145 (A: 99.61%) | A_L: 0.1447 (A: 98.77%) | S_L: 0.0339 (A: 100.00%) | Age_L: 0.3812 (A: 95.83%) | Region_L: 1.0327 (A: 92.13%)\n",
            "  Valid -> Total L: 1.1827 | Reco L: 0.7253 | Dis L: 4.0152 | G_L: 0.0225 (A: 99.38%) | A_L: 0.4350 (A: 88.89%) | S_L: 0.2058 (A: 96.91%) | Age_L: 0.9224 (A: 76.54%) | Region_L: 1.7631 (A: 60.49%) | average_mcd: 2.7107 | average_speaker_id_consistency:0.4641 |disentanglement_accuracy:0.0568 | \n",
            "           Summary Metrics -> MCD: 14.3497 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 37.0305 | F0 Corr: -0.0149 | \n",
            "           VUV Error: 0.1372 | Energy RMSE: 3644.9319\n",
            "           PESQ: 1.5859 | STOI: 0.7845\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.1827)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 22/100:\n",
            "  Train -> Total L: 0.9100 | Reco L: 0.7091 | Dis L: 3.7605 | G_L: 0.0116 (A: 99.92%) | A_L: 0.1331 (A: 98.61%) | S_L: 0.0324 (A: 100.00%) | Age_L: 0.3474 (A: 95.91%) | Region_L: 0.9782 (A: 93.60%)\n",
            "  Valid -> Total L: 1.2197 | Reco L: 0.7379 | Dis L: 3.9141 | G_L: 0.0112 (A: 99.69%) | A_L: 0.5502 (A: 76.85%) | S_L: 0.1874 (A: 97.22%) | Age_L: 1.5586 (A: 50.31%) | Region_L: 1.3691 (A: 75.31%) | average_mcd: 2.6998 | average_speaker_id_consistency:0.4431 |disentanglement_accuracy:0.0994 | \n",
            "           Summary Metrics -> MCD: 14.4407 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 29.4239 | F0 Corr: -0.0145 | \n",
            "           VUV Error: 0.1422 | Energy RMSE: 3146.4902\n",
            "           PESQ: 1.6023 | STOI: 0.7892\n",
            "  Validation loss did not improve from 1.1827\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/100:\n",
            "  Train -> Total L: 0.8811 | Reco L: 0.6946 | Dis L: 3.6599 | G_L: 0.0104 (A: 99.77%) | A_L: 0.1217 (A: 99.00%) | S_L: 0.0304 (A: 99.92%) | Age_L: 0.3315 (A: 95.68%) | Region_L: 0.8830 (A: 95.14%)\n",
            "  Valid -> Total L: 1.1577 | Reco L: 0.7153 | Dis L: 3.7763 | G_L: 0.0055 (A: 100.00%) | A_L: 0.3166 (A: 92.28%) | S_L: 0.3048 (A: 95.37%) | Age_L: 0.8233 (A: 75.93%) | Region_L: 1.3779 (A: 77.16%) | average_mcd: 2.6773 | average_speaker_id_consistency:0.5408 |disentanglement_accuracy:0.0795 | \n",
            "           Summary Metrics -> MCD: 14.1138 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 28.8244 | F0 Corr: -0.0116 | \n",
            "           VUV Error: 0.3675 | Energy RMSE: 3126.0852\n",
            "           PESQ: 1.5510 | STOI: 0.7788\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.1577)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 24/100:\n",
            "  Train -> Total L: 0.8493 | Reco L: 0.6769 | Dis L: 3.7229 | G_L: 0.0070 (A: 100.00%) | A_L: 0.1044 (A: 99.46%) | S_L: 0.0265 (A: 100.00%) | Age_L: 0.2910 (A: 97.30%) | Region_L: 0.8166 (A: 94.37%)\n",
            "  Valid -> Total L: 1.1756 | Reco L: 0.7387 | Dis L: 4.8741 | G_L: 0.0112 (A: 100.00%) | A_L: 0.2794 (A: 93.21%) | S_L: 0.2407 (A: 95.99%) | Age_L: 0.9336 (A: 75.00%) | Region_L: 1.4533 (A: 67.90%) | average_mcd: 2.6691 | average_speaker_id_consistency:0.5309 |disentanglement_accuracy:0.0085 | \n",
            "           Summary Metrics -> MCD: 14.2235 | Spk Sim: 0.9864 | \n",
            "           F0 RMSE: 37.2473 | F0 Corr: 0.0513 | \n",
            "           VUV Error: 0.1391 | Energy RMSE: 3347.6846\n",
            "           PESQ: 1.6013 | STOI: 0.7794\n",
            "  Validation loss did not improve from 1.1577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 25/100:\n",
            "  Train -> Total L: 0.8322 | Reco L: 0.6720 | Dis L: 3.5593 | G_L: 0.0062 (A: 100.00%) | A_L: 0.0984 (A: 99.15%) | S_L: 0.0236 (A: 100.00%) | Age_L: 0.2701 (A: 97.45%) | Region_L: 0.7534 (A: 96.14%)\n",
            "  Valid -> Total L: 1.2686 | Reco L: 0.7415 | Dis L: 3.5823 | G_L: 0.0383 (A: 98.77%) | A_L: 0.9606 (A: 66.98%) | S_L: 0.2611 (A: 95.06%) | Age_L: 0.9377 (A: 73.77%) | Region_L: 1.6699 (A: 67.28%) | average_mcd: 2.7074 | average_speaker_id_consistency:0.5023 |disentanglement_accuracy:0.1761 | \n",
            "           Summary Metrics -> MCD: 14.1078 | Spk Sim: 0.9863 | \n",
            "           F0 RMSE: 35.7837 | F0 Corr: -0.1550 | \n",
            "           VUV Error: 0.1353 | Energy RMSE: 3332.0645\n",
            "           PESQ: 1.6394 | STOI: 0.7773\n",
            "  Validation loss did not improve from 1.1577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 26/100:\n",
            "  Train -> Total L: 0.8280 | Reco L: 0.6726 | Dis L: 3.6136 | G_L: 0.0092 (A: 99.69%) | A_L: 0.1039 (A: 99.15%) | S_L: 0.0206 (A: 100.00%) | Age_L: 0.2665 (A: 96.45%) | Region_L: 0.7102 (A: 96.45%)\n",
            "  Valid -> Total L: 1.1622 | Reco L: 0.6986 | Dis L: 3.8575 | G_L: 0.0075 (A: 99.69%) | A_L: 0.4755 (A: 85.19%) | S_L: 0.3140 (A: 93.21%) | Age_L: 0.7529 (A: 75.31%) | Region_L: 1.4437 (A: 61.11%) | average_mcd: 2.6949 | average_speaker_id_consistency:0.5992 |disentanglement_accuracy:0.0597 | \n",
            "           Summary Metrics -> MCD: 13.6495 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 32.2389 | F0 Corr: 0.0716 | \n",
            "           VUV Error: 0.1227 | Energy RMSE: 3316.2837\n",
            "           PESQ: 1.6367 | STOI: 0.7836\n",
            "  Validation loss did not improve from 1.1577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 27/100:\n",
            "  Train -> Total L: 0.8063 | Reco L: 0.6638 | Dis L: 3.5672 | G_L: 0.0038 (A: 100.00%) | A_L: 0.0873 (A: 99.23%) | S_L: 0.0183 (A: 100.00%) | Age_L: 0.2493 (A: 97.30%) | Region_L: 0.6367 (A: 97.53%)\n",
            "  Valid -> Total L: 1.0108 | Reco L: 0.6905 | Dis L: 3.5444 | G_L: 0.0078 (A: 100.00%) | A_L: 0.3762 (A: 87.35%) | S_L: 0.1315 (A: 97.84%) | Age_L: 0.6865 (A: 80.25%) | Region_L: 1.1204 (A: 84.26%) | average_mcd: 2.6694 | average_speaker_id_consistency:0.4846 |disentanglement_accuracy:0.1392 | \n",
            "           Summary Metrics -> MCD: 13.5560 | Spk Sim: 0.9860 | \n",
            "           F0 RMSE: 41.3116 | F0 Corr: -0.1455 | \n",
            "           VUV Error: 0.1403 | Energy RMSE: 3744.8235\n",
            "           PESQ: 1.6451 | STOI: 0.7824\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 1.0108)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 28/100:\n",
            "  Train -> Total L: 0.7748 | Reco L: 0.6452 | Dis L: 3.4261 | G_L: 0.0036 (A: 99.92%) | A_L: 0.0796 (A: 98.84%) | S_L: 0.0171 (A: 100.00%) | Age_L: 0.2082 (A: 97.92%) | Region_L: 0.5762 (A: 97.84%)\n",
            "  Valid -> Total L: 1.0743 | Reco L: 0.6807 | Dis L: 3.5587 | G_L: 0.0077 (A: 100.00%) | A_L: 0.6941 (A: 75.31%) | S_L: 0.2101 (A: 95.68%) | Age_L: 0.6559 (A: 82.72%) | Region_L: 1.1721 (A: 74.38%) | average_mcd: 2.6429 | average_speaker_id_consistency:0.5632 |disentanglement_accuracy:0.0909 | \n",
            "           Summary Metrics -> MCD: 13.3506 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 26.3228 | F0 Corr: 0.1153 | \n",
            "           VUV Error: 0.1297 | Energy RMSE: 2898.2808\n",
            "           PESQ: 1.6682 | STOI: 0.7869\n",
            "  Validation loss did not improve from 1.0108\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 29/100:\n",
            "  Train -> Total L: 0.7670 | Reco L: 0.6406 | Dis L: 3.4222 | G_L: 0.0064 (A: 99.85%) | A_L: 0.0681 (A: 99.61%) | S_L: 0.0187 (A: 100.00%) | Age_L: 0.2157 (A: 97.07%) | Region_L: 0.5375 (A: 97.92%)\n",
            "  Valid -> Total L: 1.0122 | Reco L: 0.6671 | Dis L: 4.2540 | G_L: 0.0074 (A: 100.00%) | A_L: 0.4867 (A: 82.72%) | S_L: 0.1391 (A: 97.53%) | Age_L: 0.7245 (A: 84.26%) | Region_L: 1.1107 (A: 76.54%) | average_mcd: 2.6324 | average_speaker_id_consistency:0.5347 |disentanglement_accuracy:0.0540 | \n",
            "           Summary Metrics -> MCD: 13.1345 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 25.5792 | F0 Corr: 0.2752 | \n",
            "           VUV Error: 0.1307 | Energy RMSE: 3218.2656\n",
            "           PESQ: 1.6558 | STOI: 0.7854\n",
            "  Validation loss did not improve from 1.0108\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 30/100:\n",
            "  Train -> Total L: 0.7667 | Reco L: 0.6504 | Dis L: 3.4548 | G_L: 0.0034 (A: 100.00%) | A_L: 0.0571 (A: 99.77%) | S_L: 0.0146 (A: 100.00%) | Age_L: 0.1924 (A: 97.99%) | Region_L: 0.4910 (A: 98.38%)\n",
            "  Valid -> Total L: 0.9740 | Reco L: 0.6888 | Dis L: 3.5711 | G_L: 0.0050 (A: 100.00%) | A_L: 0.2206 (A: 93.52%) | S_L: 0.1356 (A: 98.15%) | Age_L: 0.6984 (A: 80.25%) | Region_L: 0.8926 (A: 88.27%) | average_mcd: 2.7002 | average_speaker_id_consistency:0.5352 |disentanglement_accuracy:0.1278 | \n",
            "           Summary Metrics -> MCD: 13.3373 | Spk Sim: 0.9862 | \n",
            "           F0 RMSE: 31.5170 | F0 Corr: 0.1410 | \n",
            "           VUV Error: 0.1247 | Energy RMSE: 3283.0078\n",
            "           PESQ: 1.6527 | STOI: 0.7824\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.9740)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 31/100:\n",
            "  Train -> Total L: 0.7510 | Reco L: 0.6425 | Dis L: 3.4191 | G_L: 0.0040 (A: 100.00%) | A_L: 0.0514 (A: 99.77%) | S_L: 0.0121 (A: 100.00%) | Age_L: 0.1836 (A: 97.76%) | Region_L: 0.4438 (A: 98.77%)\n",
            "  Valid -> Total L: 1.0079 | Reco L: 0.6607 | Dis L: 3.5298 | G_L: 0.0191 (A: 99.07%) | A_L: 0.3796 (A: 88.89%) | S_L: 0.1908 (A: 96.60%) | Age_L: 0.5400 (A: 87.04%) | Region_L: 1.2268 (A: 71.60%) | average_mcd: 2.6525 | average_speaker_id_consistency:0.5949 |disentanglement_accuracy:0.1250 | \n",
            "           Summary Metrics -> MCD: 13.0919 | Spk Sim: 0.9864 | \n",
            "           F0 RMSE: 27.1243 | F0 Corr: 0.0481 | \n",
            "           VUV Error: 0.1274 | Energy RMSE: 2852.9312\n",
            "           PESQ: 1.6773 | STOI: 0.7858\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 32/100:\n",
            "  Train -> Total L: 0.7579 | Reco L: 0.6458 | Dis L: 3.4105 | G_L: 0.0051 (A: 99.92%) | A_L: 0.0757 (A: 99.07%) | S_L: 0.0138 (A: 100.00%) | Age_L: 0.2004 (A: 97.15%) | Region_L: 0.4291 (A: 98.92%)\n",
            "  Valid -> Total L: 1.1027 | Reco L: 0.7677 | Dis L: 3.3922 | G_L: 0.0083 (A: 99.69%) | A_L: 0.4810 (A: 82.72%) | S_L: 0.1764 (A: 97.22%) | Age_L: 0.7399 (A: 80.86%) | Region_L: 0.8997 (A: 83.02%) | average_mcd: 2.6905 | average_speaker_id_consistency:0.5767 |disentanglement_accuracy:0.1364 | \n",
            "           Summary Metrics -> MCD: 13.9723 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 26.5111 | F0 Corr: 0.0570 | \n",
            "           VUV Error: 0.1277 | Energy RMSE: 3346.6196\n",
            "           PESQ: 1.6469 | STOI: 0.7617\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 33/100:\n",
            "  Train -> Total L: 0.7201 | Reco L: 0.6241 | Dis L: 3.2204 | G_L: 0.0029 (A: 100.00%) | A_L: 0.0475 (A: 99.85%) | S_L: 0.0109 (A: 100.00%) | Age_L: 0.1540 (A: 98.53%) | Region_L: 0.3794 (A: 99.07%)\n",
            "  Valid -> Total L: 1.0296 | Reco L: 0.6480 | Dis L: 3.2644 | G_L: 0.0082 (A: 99.69%) | A_L: 0.3855 (A: 87.04%) | S_L: 0.1918 (A: 96.30%) | Age_L: 0.9448 (A: 70.99%) | Region_L: 1.1925 (A: 76.23%) | average_mcd: 2.6087 | average_speaker_id_consistency:0.6096 |disentanglement_accuracy:0.1847 | \n",
            "           Summary Metrics -> MCD: 12.8544 | Spk Sim: 0.9866 | \n",
            "           F0 RMSE: 26.6320 | F0 Corr: 0.1067 | \n",
            "           VUV Error: 0.1229 | Energy RMSE: 2810.9507\n",
            "           PESQ: 1.6862 | STOI: 0.7870\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 34/100:\n",
            "  Train -> Total L: 0.7374 | Reco L: 0.6425 | Dis L: 3.1957 | G_L: 0.0028 (A: 100.00%) | A_L: 0.0500 (A: 99.85%) | S_L: 0.0111 (A: 100.00%) | Age_L: 0.1618 (A: 97.92%) | Region_L: 0.3594 (A: 98.92%)\n",
            "  Valid -> Total L: 1.1139 | Reco L: 0.7026 | Dis L: 3.3522 | G_L: 0.0167 (A: 99.69%) | A_L: 0.3995 (A: 86.73%) | S_L: 0.1859 (A: 96.60%) | Age_L: 0.9840 (A: 69.75%) | Region_L: 1.4478 (A: 66.98%) | average_mcd: 2.7221 | average_speaker_id_consistency:0.5998 |disentanglement_accuracy:0.1449 | \n",
            "           Summary Metrics -> MCD: 13.0217 | Spk Sim: 0.9861 | \n",
            "           F0 RMSE: 33.7423 | F0 Corr: -0.1011 | \n",
            "           VUV Error: 0.1258 | Energy RMSE: 3607.4170\n",
            "           PESQ: 1.6082 | STOI: 0.7606\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 35/100:\n",
            "  Train -> Total L: 0.7417 | Reco L: 0.6429 | Dis L: 3.1938 | G_L: 0.0270 (A: 99.15%) | A_L: 0.0610 (A: 99.46%) | S_L: 0.0159 (A: 100.00%) | Age_L: 0.1488 (A: 97.84%) | Region_L: 0.3529 (A: 99.31%)\n",
            "  Valid -> Total L: 1.1221 | Reco L: 0.6429 | Dis L: 3.2095 | G_L: 0.0100 (A: 100.00%) | A_L: 1.0077 (A: 65.12%) | S_L: 0.2680 (A: 94.75%) | Age_L: 0.9284 (A: 74.69%) | Region_L: 1.1849 (A: 70.37%) | average_mcd: 2.6069 | average_speaker_id_consistency:0.6075 |disentanglement_accuracy:0.2216 | \n",
            "           Summary Metrics -> MCD: 12.7080 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 26.3292 | F0 Corr: 0.1371 | \n",
            "           VUV Error: 0.1301 | Energy RMSE: 2909.6082\n",
            "           PESQ: 1.6562 | STOI: 0.7909\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 36/100:\n",
            "  Train -> Total L: 0.7164 | Reco L: 0.6192 | Dis L: 3.1533 | G_L: 0.0084 (A: 99.85%) | A_L: 0.0654 (A: 99.31%) | S_L: 0.0194 (A: 99.92%) | Age_L: 0.1363 (A: 98.38%) | Region_L: 0.3501 (A: 98.38%)\n",
            "  Valid -> Total L: 1.0605 | Reco L: 0.6455 | Dis L: 3.2445 | G_L: 0.0304 (A: 98.46%) | A_L: 0.4930 (A: 81.79%) | S_L: 0.2260 (A: 96.60%) | Age_L: 0.7868 (A: 80.56%) | Region_L: 1.3860 (A: 67.90%) | average_mcd: 2.6272 | average_speaker_id_consistency:0.5885 |disentanglement_accuracy:0.2159 | \n",
            "           Summary Metrics -> MCD: 12.7633 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 27.5668 | F0 Corr: 0.1303 | \n",
            "           VUV Error: 0.1377 | Energy RMSE: 2811.4031\n",
            "           PESQ: 1.6841 | STOI: 0.7902\n",
            "  Validation loss did not improve from 0.9740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 37/100:\n",
            "  Train -> Total L: 0.6857 | Reco L: 0.6008 | Dis L: 3.1147 | G_L: 0.0057 (A: 99.92%) | A_L: 0.0502 (A: 99.54%) | S_L: 0.0112 (A: 100.00%) | Age_L: 0.1192 (A: 98.77%) | Region_L: 0.3059 (A: 99.07%)\n",
            "  Valid -> Total L: 0.9024 | Reco L: 0.6297 | Dis L: 3.2367 | G_L: 0.0210 (A: 99.07%) | A_L: 0.2178 (A: 94.14%) | S_L: 0.1804 (A: 96.30%) | Age_L: 0.5422 (A: 85.19%) | Region_L: 0.7209 (A: 90.74%) | average_mcd: 2.6053 | average_speaker_id_consistency:0.5660 |disentanglement_accuracy:0.2131 | \n",
            "           Summary Metrics -> MCD: 12.5086 | Spk Sim: 0.9864 | \n",
            "           F0 RMSE: 32.1007 | F0 Corr: 0.0493 | \n",
            "           VUV Error: 0.1231 | Energy RMSE: 2796.8630\n",
            "           PESQ: 1.7479 | STOI: 0.8026\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.9024)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 38/100:\n",
            "  Train -> Total L: 0.6676 | Reco L: 0.5899 | Dis L: 3.0274 | G_L: 0.0025 (A: 100.00%) | A_L: 0.0464 (A: 99.69%) | S_L: 0.0090 (A: 100.00%) | Age_L: 0.1193 (A: 98.77%) | Region_L: 0.2612 (A: 99.77%)\n",
            "  Valid -> Total L: 0.8320 | Reco L: 0.6228 | Dis L: 3.3726 | G_L: 0.0042 (A: 100.00%) | A_L: 0.1683 (A: 95.68%) | S_L: 0.1150 (A: 97.84%) | Age_L: 0.3788 (A: 91.36%) | Region_L: 0.6287 (A: 93.83%) | average_mcd: 2.6029 | average_speaker_id_consistency:0.5748 |disentanglement_accuracy:0.1307 | \n",
            "           Summary Metrics -> MCD: 12.3963 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 25.0366 | F0 Corr: 0.1010 | \n",
            "           VUV Error: 0.1207 | Energy RMSE: 2763.6936\n",
            "           PESQ: 1.7650 | STOI: 0.8055\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.8320)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 39/100:\n",
            "  Train -> Total L: 0.6724 | Reco L: 0.6003 | Dis L: 3.0570 | G_L: 0.0025 (A: 100.00%) | A_L: 0.0368 (A: 99.92%) | S_L: 0.0077 (A: 100.00%) | Age_L: 0.0979 (A: 99.15%) | Region_L: 0.2391 (A: 99.77%)\n",
            "  Valid -> Total L: 0.8742 | Reco L: 0.6307 | Dis L: 3.2187 | G_L: 0.0044 (A: 100.00%) | A_L: 0.3235 (A: 89.81%) | S_L: 0.1255 (A: 97.84%) | Age_L: 0.5594 (A: 84.26%) | Region_L: 0.5988 (A: 91.05%) | average_mcd: 2.5787 | average_speaker_id_consistency:0.5802 |disentanglement_accuracy:0.1960 | \n",
            "           Summary Metrics -> MCD: 12.3800 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 23.8544 | F0 Corr: 0.2246 | \n",
            "           VUV Error: 0.1169 | Energy RMSE: 2732.2769\n",
            "           PESQ: 1.7703 | STOI: 0.8014\n",
            "  Validation loss did not improve from 0.8320\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 40/100:\n",
            "  Train -> Total L: 0.6690 | Reco L: 0.5998 | Dis L: 3.0272 | G_L: 0.0017 (A: 100.00%) | A_L: 0.0312 (A: 99.92%) | S_L: 0.0070 (A: 100.00%) | Age_L: 0.0957 (A: 98.92%) | Region_L: 0.2255 (A: 99.69%)\n",
            "  Valid -> Total L: 0.8256 | Reco L: 0.6320 | Dis L: 3.0534 | G_L: 0.0044 (A: 100.00%) | A_L: 0.2001 (A: 94.75%) | S_L: 0.1001 (A: 98.46%) | Age_L: 0.4102 (A: 90.43%) | Region_L: 0.5157 (A: 93.83%) | average_mcd: 2.5798 | average_speaker_id_consistency:0.5742 |disentanglement_accuracy:0.2188 | \n",
            "           Summary Metrics -> MCD: 12.4752 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 26.5394 | F0 Corr: 0.1607 | \n",
            "           VUV Error: 0.1176 | Energy RMSE: 2598.0430\n",
            "           PESQ: 1.7393 | STOI: 0.8004\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.8256)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 41/100:\n",
            "  Train -> Total L: 0.6685 | Reco L: 0.5992 | Dis L: 3.0802 | G_L: 0.0097 (A: 99.69%) | A_L: 0.0365 (A: 99.85%) | S_L: 0.0071 (A: 100.00%) | Age_L: 0.0966 (A: 99.15%) | Region_L: 0.2074 (A: 99.77%)\n",
            "  Valid -> Total L: 0.8829 | Reco L: 0.6232 | Dis L: 3.1002 | G_L: 0.0089 (A: 99.69%) | A_L: 0.2967 (A: 91.67%) | S_L: 0.1535 (A: 98.15%) | Age_L: 0.4750 (A: 87.04%) | Region_L: 0.7385 (A: 86.42%) | average_mcd: 2.6169 | average_speaker_id_consistency:0.6378 |disentanglement_accuracy:0.1818 | \n",
            "           Summary Metrics -> MCD: 12.2940 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 31.9310 | F0 Corr: 0.0583 | \n",
            "           VUV Error: 0.1197 | Energy RMSE: 2827.8999\n",
            "           PESQ: 1.7506 | STOI: 0.8061\n",
            "  Validation loss did not improve from 0.8256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 42/100:\n",
            "  Train -> Total L: 0.6545 | Reco L: 0.5897 | Dis L: 2.8537 | G_L: 0.0038 (A: 100.00%) | A_L: 0.0284 (A: 99.92%) | S_L: 0.0068 (A: 100.00%) | Age_L: 0.0975 (A: 99.00%) | Region_L: 0.1986 (A: 99.85%)\n",
            "  Valid -> Total L: 0.9973 | Reco L: 0.6275 | Dis L: 3.3299 | G_L: 0.0120 (A: 99.69%) | A_L: 0.5509 (A: 82.72%) | S_L: 0.2005 (A: 95.68%) | Age_L: 0.6174 (A: 83.02%) | Region_L: 1.1829 (A: 70.99%) | average_mcd: 2.5840 | average_speaker_id_consistency:0.5936 |disentanglement_accuracy:0.0966 | \n",
            "           Summary Metrics -> MCD: 12.4401 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 23.4660 | F0 Corr: 0.0595 | \n",
            "           VUV Error: 0.1143 | Energy RMSE: 2591.4668\n",
            "           PESQ: 1.7445 | STOI: 0.8038\n",
            "  Validation loss did not improve from 0.8256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 43/100:\n",
            "  Train -> Total L: 0.6491 | Reco L: 0.5867 | Dis L: 2.8226 | G_L: 0.0038 (A: 99.92%) | A_L: 0.0241 (A: 100.00%) | S_L: 0.0069 (A: 100.00%) | Age_L: 0.0954 (A: 99.07%) | Region_L: 0.1837 (A: 99.77%)\n",
            "  Valid -> Total L: 0.8594 | Reco L: 0.6539 | Dis L: 2.7707 | G_L: 0.0269 (A: 99.38%) | A_L: 0.1680 (A: 95.37%) | S_L: 0.1068 (A: 98.15%) | Age_L: 0.4526 (A: 86.73%) | Region_L: 0.5964 (A: 91.67%) | average_mcd: 2.5681 | average_speaker_id_consistency:0.5743 |disentanglement_accuracy:0.3182 | \n",
            "           Summary Metrics -> MCD: 12.6416 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 25.2211 | F0 Corr: 0.1400 | \n",
            "           VUV Error: 0.1180 | Energy RMSE: 2587.4893\n",
            "           PESQ: 1.7687 | STOI: 0.8060\n",
            "  Validation loss did not improve from 0.8256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 44/100:\n",
            "  Train -> Total L: 0.6448 | Reco L: 0.5850 | Dis L: 2.7558 | G_L: 0.0027 (A: 100.00%) | A_L: 0.0245 (A: 100.00%) | S_L: 0.0063 (A: 100.00%) | Age_L: 0.0899 (A: 99.38%) | Region_L: 0.1729 (A: 99.92%)\n",
            "  Valid -> Total L: 0.8738 | Reco L: 0.6093 | Dis L: 2.9192 | G_L: 0.0244 (A: 99.07%) | A_L: 0.3990 (A: 87.96%) | S_L: 0.1307 (A: 97.84%) | Age_L: 0.5964 (A: 81.48%) | Region_L: 0.6800 (A: 86.73%) | average_mcd: 2.5795 | average_speaker_id_consistency:0.6102 |disentanglement_accuracy:0.2443 | \n",
            "           Summary Metrics -> MCD: 12.1673 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 31.1914 | F0 Corr: -0.0804 | \n",
            "           VUV Error: 0.1136 | Energy RMSE: 2580.0112\n",
            "           PESQ: 1.7765 | STOI: 0.8081\n",
            "  Validation loss did not improve from 0.8256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 45/100:\n",
            "  Train -> Total L: 0.6325 | Reco L: 0.5777 | Dis L: 2.7244 | G_L: 0.0018 (A: 100.00%) | A_L: 0.0197 (A: 100.00%) | S_L: 0.0057 (A: 100.00%) | Age_L: 0.0742 (A: 99.61%) | Region_L: 0.1520 (A: 100.00%)\n",
            "  Valid -> Total L: 0.8431 | Reco L: 0.6181 | Dis L: 2.7581 | G_L: 0.0032 (A: 100.00%) | A_L: 0.2032 (A: 94.44%) | S_L: 0.1020 (A: 98.46%) | Age_L: 0.7408 (A: 79.63%) | Region_L: 0.5175 (A: 91.67%) | average_mcd: 2.5775 | average_speaker_id_consistency:0.6180 |disentanglement_accuracy:0.2699 | \n",
            "           Summary Metrics -> MCD: 12.2580 | Spk Sim: 0.9868 | \n",
            "           F0 RMSE: 23.4373 | F0 Corr: 0.0788 | \n",
            "           VUV Error: 0.1101 | Energy RMSE: 2450.7056\n",
            "           PESQ: 1.7636 | STOI: 0.8118\n",
            "  Validation loss did not improve from 0.8256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 46/100:\n",
            "  Train -> Total L: 0.6256 | Reco L: 0.5696 | Dis L: 2.7167 | G_L: 0.0016 (A: 100.00%) | A_L: 0.0292 (A: 99.61%) | S_L: 0.0056 (A: 100.00%) | Age_L: 0.0728 (A: 99.61%) | Region_L: 0.1560 (A: 99.61%)\n",
            "  Valid -> Total L: 0.8237 | Reco L: 0.5905 | Dis L: 3.0796 | G_L: 0.0044 (A: 99.69%) | A_L: 0.1590 (A: 96.60%) | S_L: 0.1111 (A: 97.53%) | Age_L: 0.5559 (A: 85.49%) | Region_L: 0.7488 (A: 87.04%) | average_mcd: 2.5595 | average_speaker_id_consistency:0.6048 |disentanglement_accuracy:0.1932 | \n",
            "           Summary Metrics -> MCD: 11.7712 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 28.7336 | F0 Corr: 0.0118 | \n",
            "           VUV Error: 0.1090 | Energy RMSE: 2574.8127\n",
            "           PESQ: 1.7929 | STOI: 0.8169\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.8237)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 47/100:\n",
            "  Train -> Total L: 0.6316 | Reco L: 0.5793 | Dis L: 2.6458 | G_L: 0.0074 (A: 99.77%) | A_L: 0.0200 (A: 100.00%) | S_L: 0.0049 (A: 100.00%) | Age_L: 0.0690 (A: 99.77%) | Region_L: 0.1372 (A: 99.85%)\n",
            "  Valid -> Total L: 0.8620 | Reco L: 0.6249 | Dis L: 3.4454 | G_L: 0.0227 (A: 99.07%) | A_L: 0.2604 (A: 91.36%) | S_L: 0.1287 (A: 97.22%) | Age_L: 0.4267 (A: 87.65%) | Region_L: 0.6733 (A: 87.04%) | average_mcd: 2.4982 | average_speaker_id_consistency:0.6435 |disentanglement_accuracy:0.1023 | \n",
            "           Summary Metrics -> MCD: 11.9324 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 22.4714 | F0 Corr: -0.0063 | \n",
            "           VUV Error: 0.1103 | Energy RMSE: 2321.3899\n",
            "           PESQ: 1.7887 | STOI: 0.8107\n",
            "  Validation loss did not improve from 0.8237\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 48/100:\n",
            "  Train -> Total L: 0.6300 | Reco L: 0.5769 | Dis L: 2.6315 | G_L: 0.0061 (A: 99.85%) | A_L: 0.0214 (A: 100.00%) | S_L: 0.0053 (A: 100.00%) | Age_L: 0.0793 (A: 99.23%) | Region_L: 0.1344 (A: 99.92%)\n",
            "  Valid -> Total L: 0.8039 | Reco L: 0.5953 | Dis L: 2.5808 | G_L: 0.0061 (A: 99.69%) | A_L: 0.1813 (A: 95.68%) | S_L: 0.1139 (A: 97.22%) | Age_L: 0.6308 (A: 83.33%) | Region_L: 0.4394 (A: 94.14%) | average_mcd: 2.5913 | average_speaker_id_consistency:0.6434 |disentanglement_accuracy:0.3722 | \n",
            "           Summary Metrics -> MCD: 11.7815 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 22.2896 | F0 Corr: 0.0115 | \n",
            "           VUV Error: 0.1144 | Energy RMSE: 2572.4668\n",
            "           PESQ: 1.8129 | STOI: 0.8194\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.8039)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 49/100:\n",
            "  Train -> Total L: 0.6226 | Reco L: 0.5742 | Dis L: 2.5746 | G_L: 0.0030 (A: 99.92%) | A_L: 0.0194 (A: 100.00%) | S_L: 0.0048 (A: 100.00%) | Age_L: 0.0586 (A: 99.92%) | Region_L: 0.1216 (A: 100.00%)\n",
            "  Valid -> Total L: 0.8260 | Reco L: 0.6317 | Dis L: 2.7788 | G_L: 0.0063 (A: 99.69%) | A_L: 0.3011 (A: 90.74%) | S_L: 0.1071 (A: 98.15%) | Age_L: 0.2751 (A: 95.06%) | Region_L: 0.5475 (A: 88.27%) | average_mcd: 2.5611 | average_speaker_id_consistency:0.6306 |disentanglement_accuracy:0.2472 | \n",
            "           Summary Metrics -> MCD: 12.3125 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 24.6441 | F0 Corr: 0.1124 | \n",
            "           VUV Error: 0.1158 | Energy RMSE: 2261.8547\n",
            "           PESQ: 1.7692 | STOI: 0.8148\n",
            "  Validation loss did not improve from 0.8039\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 50/100:\n",
            "  Train -> Total L: 0.6069 | Reco L: 0.5621 | Dis L: 2.5528 | G_L: 0.0011 (A: 100.00%) | A_L: 0.0150 (A: 100.00%) | S_L: 0.0044 (A: 100.00%) | Age_L: 0.0490 (A: 99.92%) | Region_L: 0.1056 (A: 99.92%)\n",
            "  Valid -> Total L: 0.7829 | Reco L: 0.6122 | Dis L: 2.6339 | G_L: 0.0026 (A: 100.00%) | A_L: 0.1669 (A: 94.44%) | S_L: 0.0985 (A: 98.15%) | Age_L: 0.3996 (A: 91.67%) | Region_L: 0.3815 (A: 94.75%) | average_mcd: 2.4997 | average_speaker_id_consistency:0.6224 |disentanglement_accuracy:0.3352 | \n",
            "           Summary Metrics -> MCD: 11.7205 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 22.8462 | F0 Corr: 0.1105 | \n",
            "           VUV Error: 0.1045 | Energy RMSE: 2279.2371\n",
            "           PESQ: 1.8040 | STOI: 0.8080\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.7829)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 51/100:\n",
            "  Train -> Total L: 0.6114 | Reco L: 0.5676 | Dis L: 2.5085 | G_L: 0.0054 (A: 99.77%) | A_L: 0.0112 (A: 100.00%) | S_L: 0.0040 (A: 100.00%) | Age_L: 0.0469 (A: 99.92%) | Region_L: 0.1032 (A: 99.85%)\n",
            "  Valid -> Total L: 0.8014 | Reco L: 0.5990 | Dis L: 2.7304 | G_L: 0.1855 (A: 94.75%) | A_L: 0.1471 (A: 95.37%) | S_L: 0.1032 (A: 97.84%) | Age_L: 0.3807 (A: 92.28%) | Region_L: 0.5221 (A: 91.98%) | average_mcd: 2.5511 | average_speaker_id_consistency:0.6145 |disentanglement_accuracy:0.2955 | \n",
            "           Summary Metrics -> MCD: 11.8782 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 20.5544 | F0 Corr: 0.1640 | \n",
            "           VUV Error: 0.1147 | Energy RMSE: 2431.8784\n",
            "           PESQ: 1.7790 | STOI: 0.8200\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 52/100:\n",
            "  Train -> Total L: 0.6086 | Reco L: 0.5568 | Dis L: 2.4924 | G_L: 0.0381 (A: 99.00%) | A_L: 0.0219 (A: 99.85%) | S_L: 0.0063 (A: 100.00%) | Age_L: 0.0619 (A: 99.77%) | Region_L: 0.1147 (A: 100.00%)\n",
            "  Valid -> Total L: 2.1787 | Reco L: 0.6358 | Dis L: 2.5877 | G_L: 0.0107 (A: 99.38%) | A_L: 2.9508 (A: 40.12%) | S_L: 1.7360 (A: 55.56%) | Age_L: 1.0026 (A: 68.52%) | Region_L: 2.5264 (A: 42.59%) | average_mcd: 2.5778 | average_speaker_id_consistency:0.8229 |disentanglement_accuracy:0.2898 | \n",
            "           Summary Metrics -> MCD: 12.2223 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 24.0798 | F0 Corr: 0.2425 | \n",
            "           VUV Error: 0.1054 | Energy RMSE: 2337.5242\n",
            "           PESQ: 1.7643 | STOI: 0.7822\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 53/100:\n",
            "  Train -> Total L: 0.6133 | Reco L: 0.5541 | Dis L: 2.4500 | G_L: 0.0046 (A: 99.85%) | A_L: 0.0519 (A: 99.38%) | S_L: 0.0070 (A: 100.00%) | Age_L: 0.1152 (A: 98.61%) | Region_L: 0.1406 (A: 99.85%)\n",
            "  Valid -> Total L: 1.0579 | Reco L: 0.5775 | Dis L: 2.4748 | G_L: 0.0230 (A: 99.38%) | A_L: 1.2136 (A: 68.21%) | S_L: 0.2839 (A: 93.52%) | Age_L: 1.0607 (A: 69.75%) | Region_L: 0.8403 (A: 80.86%) | average_mcd: 2.5207 | average_speaker_id_consistency:0.7406 |disentanglement_accuracy:0.4205 | \n",
            "           Summary Metrics -> MCD: 11.4558 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 20.8714 | F0 Corr: 0.1338 | \n",
            "           VUV Error: 0.1204 | Energy RMSE: 2193.3894\n",
            "           PESQ: 1.8333 | STOI: 0.8228\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 54/100:\n",
            "  Train -> Total L: 0.6223 | Reco L: 0.5633 | Dis L: 2.4063 | G_L: 0.0061 (A: 99.85%) | A_L: 0.0554 (A: 98.77%) | S_L: 0.0067 (A: 100.00%) | Age_L: 0.1095 (A: 98.30%) | Region_L: 0.1446 (A: 99.69%)\n",
            "  Valid -> Total L: 1.6029 | Reco L: 0.6356 | Dis L: 2.4622 | G_L: 0.0055 (A: 100.00%) | A_L: 2.6544 (A: 37.35%) | S_L: 0.6787 (A: 79.94%) | Age_L: 1.4229 (A: 60.19%) | Region_L: 1.9510 (A: 50.31%) | average_mcd: 2.5099 | average_speaker_id_consistency:0.8067 |disentanglement_accuracy:0.3381 | \n",
            "           Summary Metrics -> MCD: 11.7966 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 27.9825 | F0 Corr: 0.2146 | \n",
            "           VUV Error: 0.1130 | Energy RMSE: 2409.4871\n",
            "           PESQ: 1.8488 | STOI: 0.8094\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 55/100:\n",
            "  Train -> Total L: 0.6141 | Reco L: 0.5513 | Dis L: 2.3900 | G_L: 0.0034 (A: 99.92%) | A_L: 0.0599 (A: 98.69%) | S_L: 0.0199 (A: 99.85%) | Age_L: 0.0859 (A: 99.31%) | Region_L: 0.1406 (A: 99.54%)\n",
            "  Valid -> Total L: 1.0496 | Reco L: 0.5867 | Dis L: 2.3994 | G_L: 0.1534 (A: 93.52%) | A_L: 0.3370 (A: 90.12%) | S_L: 0.3061 (A: 93.52%) | Age_L: 1.1418 (A: 67.28%) | Region_L: 1.2272 (A: 64.81%) | average_mcd: 2.5354 | average_speaker_id_consistency:0.7049 |disentanglement_accuracy:0.3438 | \n",
            "           Summary Metrics -> MCD: 11.4628 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 20.6350 | F0 Corr: 0.0748 | \n",
            "           VUV Error: 0.1207 | Energy RMSE: 2298.8845\n",
            "           PESQ: 1.8170 | STOI: 0.8223\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 56/100:\n",
            "  Train -> Total L: 0.6398 | Reco L: 0.5408 | Dis L: 2.3189 | G_L: 0.0068 (A: 99.61%) | A_L: 0.1031 (A: 97.53%) | S_L: 0.0628 (A: 98.77%) | Age_L: 0.1336 (A: 98.30%) | Region_L: 0.2004 (A: 99.07%)\n",
            "  Valid -> Total L: 2.8815 | Reco L: 0.5818 | Dis L: 2.3054 | G_L: 0.7477 (A: 74.69%) | A_L: 2.0085 (A: 62.04%) | S_L: 1.6184 (A: 64.20%) | Age_L: 4.0547 (A: 20.99%) | Region_L: 7.8634 (A: 13.27%) | average_mcd: 2.5307 | average_speaker_id_consistency:0.8619 |disentanglement_accuracy:0.4006 | \n",
            "           Summary Metrics -> MCD: 11.2984 | Spk Sim: 0.9868 | \n",
            "           F0 RMSE: 22.0147 | F0 Corr: 0.1281 | \n",
            "           VUV Error: 0.1315 | Energy RMSE: 2454.1123\n",
            "           PESQ: 1.7354 | STOI: 0.8178\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 57/100:\n",
            "  Train -> Total L: 0.6538 | Reco L: 0.5486 | Dis L: 2.2826 | G_L: 0.0032 (A: 100.00%) | A_L: 0.1048 (A: 97.53%) | S_L: 0.0552 (A: 99.23%) | Age_L: 0.1705 (A: 96.84%) | Region_L: 0.2693 (A: 97.76%)\n",
            "  Valid -> Total L: 7.3224 | Reco L: 0.5870 | Dis L: 2.5685 | G_L: 0.6823 (A: 78.09%) | A_L: 9.4571 (A: 29.01%) | S_L: 8.6484 (A: 11.11%) | Age_L: 2.8264 (A: 31.17%) | Region_L: 10.8896 (A: 13.27%) | average_mcd: 2.5419 | average_speaker_id_consistency:0.9267 |disentanglement_accuracy:0.2670 | \n",
            "           Summary Metrics -> MCD: 11.6406 | Spk Sim: 0.9877 | \n",
            "           F0 RMSE: 46.5595 | F0 Corr: 0.0963 | \n",
            "           VUV Error: 0.1138 | Energy RMSE: 2430.7175\n",
            "           PESQ: 1.7276 | STOI: 0.8180\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 58/100:\n",
            "  Train -> Total L: 0.6270 | Reco L: 0.5409 | Dis L: 2.2530 | G_L: 0.0032 (A: 100.00%) | A_L: 0.0831 (A: 98.23%) | S_L: 0.0416 (A: 99.38%) | Age_L: 0.1451 (A: 97.53%) | Region_L: 0.1960 (A: 98.61%)\n",
            "  Valid -> Total L: 2.1035 | Reco L: 0.5730 | Dis L: 2.5795 | G_L: 0.2626 (A: 91.05%) | A_L: 0.7098 (A: 75.31%) | S_L: 1.6853 (A: 59.26%) | Age_L: 3.5946 (A: 35.80%) | Region_L: 2.0538 (A: 59.88%) | average_mcd: 2.5600 | average_speaker_id_consistency:0.8499 |disentanglement_accuracy:0.2585 | \n",
            "           Summary Metrics -> MCD: 11.1210 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 41.2776 | F0 Corr: 0.2303 | \n",
            "           VUV Error: 0.1100 | Energy RMSE: 2295.6377\n",
            "           PESQ: 1.8477 | STOI: 0.8283\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 59/100:\n",
            "  Train -> Total L: 0.6110 | Reco L: 0.5500 | Dis L: 2.2178 | G_L: 0.0032 (A: 99.85%) | A_L: 0.0538 (A: 99.15%) | S_L: 0.0149 (A: 100.00%) | Age_L: 0.1164 (A: 97.84%) | Region_L: 0.1398 (A: 99.54%)\n",
            "  Valid -> Total L: 1.3186 | Reco L: 0.5778 | Dis L: 2.4237 | G_L: 0.0210 (A: 99.69%) | A_L: 2.0279 (A: 55.56%) | S_L: 0.3512 (A: 92.90%) | Age_L: 1.6544 (A: 58.33%) | Region_L: 1.7062 (A: 53.70%) | average_mcd: 2.4978 | average_speaker_id_consistency:0.7416 |disentanglement_accuracy:0.3295 | \n",
            "           Summary Metrics -> MCD: 11.1738 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 24.2895 | F0 Corr: 0.2529 | \n",
            "           VUV Error: 0.1196 | Energy RMSE: 2524.7397\n",
            "           PESQ: 1.8598 | STOI: 0.8269\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 60/100:\n",
            "  Train -> Total L: 0.5935 | Reco L: 0.5409 | Dis L: 2.1990 | G_L: 0.0055 (A: 99.92%) | A_L: 0.0488 (A: 99.31%) | S_L: 0.0078 (A: 100.00%) | Age_L: 0.0969 (A: 98.92%) | Region_L: 0.1157 (A: 99.85%)\n",
            "  Valid -> Total L: 1.0568 | Reco L: 0.5530 | Dis L: 3.8866 | G_L: 0.0191 (A: 99.07%) | A_L: 0.9742 (A: 70.37%) | S_L: 0.3564 (A: 90.43%) | Age_L: 0.8274 (A: 75.31%) | Region_L: 1.0467 (A: 76.23%) | average_mcd: 2.4890 | average_speaker_id_consistency:0.7640 |disentanglement_accuracy:0.1023 | \n",
            "           Summary Metrics -> MCD: 10.8825 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 24.4292 | F0 Corr: 0.2565 | \n",
            "           VUV Error: 0.1122 | Energy RMSE: 2045.9893\n",
            "           PESQ: 1.8062 | STOI: 0.8306\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 61/100:\n",
            "  Train -> Total L: 0.5780 | Reco L: 0.5360 | Dis L: 2.1484 | G_L: 0.0029 (A: 99.92%) | A_L: 0.0262 (A: 99.85%) | S_L: 0.0055 (A: 100.00%) | Age_L: 0.0627 (A: 99.54%) | Region_L: 0.0859 (A: 99.85%)\n",
            "  Valid -> Total L: 0.8270 | Reco L: 0.5528 | Dis L: 2.1866 | G_L: 0.0103 (A: 99.69%) | A_L: 0.2223 (A: 93.83%) | S_L: 0.1709 (A: 96.91%) | Age_L: 0.8571 (A: 76.23%) | Region_L: 0.5792 (A: 87.65%) | average_mcd: 2.5012 | average_speaker_id_consistency:0.7540 |disentanglement_accuracy:0.4432 | \n",
            "           Summary Metrics -> MCD: 11.0060 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 20.5004 | F0 Corr: 0.3444 | \n",
            "           VUV Error: 0.1136 | Energy RMSE: 2134.1509\n",
            "           PESQ: 1.8348 | STOI: 0.8312\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 62/100:\n",
            "  Train -> Total L: 0.5752 | Reco L: 0.5361 | Dis L: 2.1237 | G_L: 0.0021 (A: 100.00%) | A_L: 0.0218 (A: 99.77%) | S_L: 0.0071 (A: 99.92%) | Age_L: 0.0460 (A: 99.85%) | Region_L: 0.0735 (A: 100.00%)\n",
            "  Valid -> Total L: 1.3485 | Reco L: 0.5908 | Dis L: 2.7827 | G_L: 0.0138 (A: 99.07%) | A_L: 2.2692 (A: 60.19%) | S_L: 0.4332 (A: 87.65%) | Age_L: 1.3256 (A: 60.19%) | Region_L: 1.5239 (A: 66.05%) | average_mcd: 2.4913 | average_speaker_id_consistency:0.7508 |disentanglement_accuracy:0.2017 | \n",
            "           Summary Metrics -> MCD: 11.1493 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 15.4478 | F0 Corr: 0.1777 | \n",
            "           VUV Error: 0.3190 | Energy RMSE: 2144.7915\n",
            "           PESQ: 1.8370 | STOI: 0.8152\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 63/100:\n",
            "  Train -> Total L: 0.5709 | Reco L: 0.5298 | Dis L: 2.1427 | G_L: 0.0020 (A: 100.00%) | A_L: 0.0313 (A: 99.61%) | S_L: 0.0089 (A: 99.92%) | Age_L: 0.0449 (A: 99.69%) | Region_L: 0.0741 (A: 100.00%)\n",
            "  Valid -> Total L: 0.9243 | Reco L: 0.5524 | Dis L: 2.0677 | G_L: 0.0743 (A: 97.22%) | A_L: 0.7648 (A: 78.40%) | S_L: 0.2212 (A: 94.14%) | Age_L: 0.7256 (A: 80.86%) | Region_L: 0.8410 (A: 82.41%) | average_mcd: 2.5115 | average_speaker_id_consistency:0.7634 |disentanglement_accuracy:0.5114 | \n",
            "           Summary Metrics -> MCD: 10.9121 | Spk Sim: 0.9868 | \n",
            "           F0 RMSE: 26.8491 | F0 Corr: 0.2985 | \n",
            "           VUV Error: 0.1138 | Energy RMSE: 2082.6138\n",
            "           PESQ: 1.8594 | STOI: 0.8319\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 64/100:\n",
            "  Train -> Total L: 0.5648 | Reco L: 0.5298 | Dis L: 2.0761 | G_L: 0.0021 (A: 100.00%) | A_L: 0.0136 (A: 100.00%) | S_L: 0.0060 (A: 99.92%) | Age_L: 0.0339 (A: 99.92%) | Region_L: 0.0629 (A: 99.92%)\n",
            "  Valid -> Total L: 0.8675 | Reco L: 0.5489 | Dis L: 2.4811 | G_L: 0.0213 (A: 99.38%) | A_L: 0.5123 (A: 83.02%) | S_L: 0.2741 (A: 93.52%) | Age_L: 0.4912 (A: 87.65%) | Region_L: 0.5429 (A: 89.20%) | average_mcd: 2.4975 | average_speaker_id_consistency:0.7513 |disentanglement_accuracy:0.3239 | \n",
            "           Summary Metrics -> MCD: 10.8163 | Spk Sim: 0.9868 | \n",
            "           F0 RMSE: 29.5930 | F0 Corr: 0.1077 | \n",
            "           VUV Error: 0.1103 | Energy RMSE: 2147.7549\n",
            "           PESQ: 1.8688 | STOI: 0.8252\n",
            "  Validation loss did not improve from 0.7829\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 65/100:\n",
            "  Train -> Total L: 0.5565 | Reco L: 0.5245 | Dis L: 2.0399 | G_L: 0.0009 (A: 100.00%) | A_L: 0.0103 (A: 100.00%) | S_L: 0.0031 (A: 100.00%) | Age_L: 0.0301 (A: 99.92%) | Region_L: 0.0597 (A: 99.77%)\n",
            "  Valid -> Total L: 0.6912 | Reco L: 0.5481 | Dis L: 2.5582 | G_L: 0.0045 (A: 99.69%) | A_L: 0.1805 (A: 94.44%) | S_L: 0.0790 (A: 98.15%) | Age_L: 0.2318 (A: 94.44%) | Region_L: 0.3637 (A: 93.83%) | average_mcd: 2.5075 | average_speaker_id_consistency:0.7328 |disentanglement_accuracy:0.2642 | \n",
            "           Summary Metrics -> MCD: 10.8008 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 20.6555 | F0 Corr: 0.2340 | \n",
            "           VUV Error: 0.1152 | Energy RMSE: 2184.9258\n",
            "           PESQ: 1.8878 | STOI: 0.8357\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6912)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 66/100:\n",
            "  Train -> Total L: 0.5558 | Reco L: 0.5249 | Dis L: 2.0525 | G_L: 0.0021 (A: 99.92%) | A_L: 0.0088 (A: 100.00%) | S_L: 0.0027 (A: 100.00%) | Age_L: 0.0255 (A: 100.00%) | Region_L: 0.0541 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6822 | Reco L: 0.5548 | Dis L: 2.0216 | G_L: 0.0146 (A: 99.69%) | A_L: 0.1714 (A: 95.37%) | S_L: 0.0845 (A: 98.46%) | Age_L: 0.1994 (A: 95.99%) | Region_L: 0.2639 (A: 95.37%) | average_mcd: 2.5058 | average_speaker_id_consistency:0.7454 |disentanglement_accuracy:0.5369 | \n",
            "           Summary Metrics -> MCD: 10.7636 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 26.5497 | F0 Corr: 0.2514 | \n",
            "           VUV Error: 0.1093 | Energy RMSE: 2168.0947\n",
            "           PESQ: 1.8861 | STOI: 0.8268\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6822)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 67/100:\n",
            "  Train -> Total L: 0.5665 | Reco L: 0.5374 | Dis L: 2.0071 | G_L: 0.0026 (A: 99.92%) | A_L: 0.0091 (A: 100.00%) | S_L: 0.0023 (A: 100.00%) | Age_L: 0.0223 (A: 100.00%) | Region_L: 0.0450 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6779 | Reco L: 0.5479 | Dis L: 2.4249 | G_L: 0.0045 (A: 100.00%) | A_L: 0.1545 (A: 95.06%) | S_L: 0.0823 (A: 98.46%) | Age_L: 0.2190 (A: 95.68%) | Region_L: 0.2682 (A: 94.75%) | average_mcd: 2.5005 | average_speaker_id_consistency:0.7316 |disentanglement_accuracy:0.2955 | \n",
            "           Summary Metrics -> MCD: 10.6944 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 20.4035 | F0 Corr: 0.1738 | \n",
            "           VUV Error: 0.1119 | Energy RMSE: 2098.0723\n",
            "           PESQ: 1.8891 | STOI: 0.8327\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6779)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 68/100:\n",
            "  Train -> Total L: 0.5491 | Reco L: 0.5199 | Dis L: 1.9765 | G_L: 0.0111 (A: 99.61%) | A_L: 0.0082 (A: 100.00%) | S_L: 0.0021 (A: 100.00%) | Age_L: 0.0212 (A: 100.00%) | Region_L: 0.0436 (A: 100.00%)\n",
            "  Valid -> Total L: 0.7579 | Reco L: 0.5611 | Dis L: 2.6927 | G_L: 0.0413 (A: 98.77%) | A_L: 0.2391 (A: 92.90%) | S_L: 0.1392 (A: 96.30%) | Age_L: 0.2939 (A: 91.36%) | Region_L: 0.4284 (A: 92.90%) | average_mcd: 2.5386 | average_speaker_id_consistency:0.7001 |disentanglement_accuracy:0.2045 | \n",
            "           Summary Metrics -> MCD: 10.9462 | Spk Sim: 0.9865 | \n",
            "           F0 RMSE: 15.5975 | F0 Corr: 0.1389 | \n",
            "           VUV Error: 0.3457 | Energy RMSE: 2559.6934\n",
            "           PESQ: 1.8723 | STOI: 0.8310\n",
            "  Validation loss did not improve from 0.6779\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 69/100:\n",
            "  Train -> Total L: 0.5584 | Reco L: 0.5303 | Dis L: 1.9280 | G_L: 0.0038 (A: 99.85%) | A_L: 0.0082 (A: 100.00%) | S_L: 0.0021 (A: 100.00%) | Age_L: 0.0214 (A: 100.00%) | Region_L: 0.0445 (A: 99.92%)\n",
            "  Valid -> Total L: 0.6751 | Reco L: 0.5581 | Dis L: 2.8764 | G_L: 0.0211 (A: 99.07%) | A_L: 0.1174 (A: 97.53%) | S_L: 0.0605 (A: 98.46%) | Age_L: 0.1974 (A: 95.37%) | Region_L: 0.2434 (A: 95.37%) | average_mcd: 2.5107 | average_speaker_id_consistency:0.7145 |disentanglement_accuracy:0.2017 | \n",
            "           Summary Metrics -> MCD: 10.7926 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 23.1692 | F0 Corr: 0.1277 | \n",
            "           VUV Error: 0.1063 | Energy RMSE: 2250.4678\n",
            "           PESQ: 1.8880 | STOI: 0.8361\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6751)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 70/100:\n",
            "  Train -> Total L: 0.5443 | Reco L: 0.5172 | Dis L: 1.9238 | G_L: 0.0021 (A: 99.92%) | A_L: 0.0076 (A: 100.00%) | S_L: 0.0021 (A: 100.00%) | Age_L: 0.0199 (A: 100.00%) | Region_L: 0.0390 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6956 | Reco L: 0.5648 | Dis L: 2.2784 | G_L: 0.0032 (A: 100.00%) | A_L: 0.1424 (A: 96.60%) | S_L: 0.0857 (A: 98.15%) | Age_L: 0.2715 (A: 93.83%) | Region_L: 0.2344 (A: 96.91%) | average_mcd: 2.4445 | average_speaker_id_consistency:0.7381 |disentanglement_accuracy:0.3494 | \n",
            "           Summary Metrics -> MCD: 10.7868 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 19.5338 | F0 Corr: 0.4237 | \n",
            "           VUV Error: 0.1111 | Energy RMSE: 2076.5190\n",
            "           PESQ: 1.8718 | STOI: 0.8244\n",
            "  Validation loss did not improve from 0.6751\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 71/100:\n",
            "  Train -> Total L: 0.5427 | Reco L: 0.5162 | Dis L: 1.9036 | G_L: 0.0024 (A: 99.92%) | A_L: 0.0088 (A: 99.92%) | S_L: 0.0018 (A: 100.00%) | Age_L: 0.0176 (A: 100.00%) | Region_L: 0.0372 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6567 | Reco L: 0.5385 | Dis L: 1.9332 | G_L: 0.0042 (A: 99.69%) | A_L: 0.1402 (A: 96.30%) | S_L: 0.0783 (A: 98.15%) | Age_L: 0.2137 (A: 95.06%) | Region_L: 0.2400 (A: 96.30%) | average_mcd: 2.4907 | average_speaker_id_consistency:0.7362 |disentanglement_accuracy:0.4972 | \n",
            "           Summary Metrics -> MCD: 10.6220 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 24.3174 | F0 Corr: 0.4504 | \n",
            "           VUV Error: 0.1076 | Energy RMSE: 2129.8977\n",
            "           PESQ: 1.9114 | STOI: 0.8351\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6567)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 72/100:\n",
            "  Train -> Total L: 0.5403 | Reco L: 0.5142 | Dis L: 1.8884 | G_L: 0.0024 (A: 100.00%) | A_L: 0.0079 (A: 100.00%) | S_L: 0.0018 (A: 100.00%) | Age_L: 0.0183 (A: 100.00%) | Region_L: 0.0346 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6626 | Reco L: 0.5428 | Dis L: 2.3551 | G_L: 0.0019 (A: 100.00%) | A_L: 0.1535 (A: 96.60%) | S_L: 0.0698 (A: 98.46%) | Age_L: 0.2295 (A: 94.44%) | Region_L: 0.2295 (A: 95.68%) | average_mcd: 2.4895 | average_speaker_id_consistency:0.7085 |disentanglement_accuracy:0.2869 | \n",
            "           Summary Metrics -> MCD: 10.6205 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 20.1187 | F0 Corr: 0.2881 | \n",
            "           VUV Error: 0.1106 | Energy RMSE: 2053.9575\n",
            "           PESQ: 1.8891 | STOI: 0.8357\n",
            "  Validation loss did not improve from 0.6567\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 73/100:\n",
            "  Train -> Total L: 0.5478 | Reco L: 0.5221 | Dis L: 1.8570 | G_L: 0.0011 (A: 100.00%) | A_L: 0.0076 (A: 100.00%) | S_L: 0.0020 (A: 100.00%) | Age_L: 0.0174 (A: 100.00%) | Region_L: 0.0355 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6434 | Reco L: 0.5338 | Dis L: 1.9151 | G_L: 0.0016 (A: 100.00%) | A_L: 0.1020 (A: 97.53%) | S_L: 0.0708 (A: 98.15%) | Age_L: 0.1884 (A: 95.99%) | Region_L: 0.2582 (A: 94.75%) | average_mcd: 2.4729 | average_speaker_id_consistency:0.6888 |disentanglement_accuracy:0.5568 | \n",
            "           Summary Metrics -> MCD: 10.5025 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 21.8386 | F0 Corr: 0.2493 | \n",
            "           VUV Error: 0.1153 | Energy RMSE: 2118.3342\n",
            "           PESQ: 1.9077 | STOI: 0.8366\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6434)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 74/100:\n",
            "  Train -> Total L: 0.5386 | Reco L: 0.5123 | Dis L: 1.8294 | G_L: 0.0006 (A: 100.00%) | A_L: 0.0142 (A: 99.77%) | S_L: 0.0020 (A: 100.00%) | Age_L: 0.0190 (A: 100.00%) | Region_L: 0.0359 (A: 100.00%)\n",
            "  Valid -> Total L: 0.7076 | Reco L: 0.5602 | Dis L: 2.2749 | G_L: 0.0025 (A: 100.00%) | A_L: 0.1652 (A: 95.06%) | S_L: 0.0830 (A: 98.46%) | Age_L: 0.3330 (A: 89.81%) | Region_L: 0.3301 (A: 94.75%) | average_mcd: 2.4583 | average_speaker_id_consistency:0.7493 |disentanglement_accuracy:0.3466 | \n",
            "           Summary Metrics -> MCD: 10.7759 | Spk Sim: 0.9868 | \n",
            "           F0 RMSE: 22.2379 | F0 Corr: 0.2587 | \n",
            "           VUV Error: 0.1143 | Energy RMSE: 2055.8301\n",
            "           PESQ: 1.9182 | STOI: 0.8262\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 9. Returning default values.\n",
            "WARNING:__main__:Insufficient voiced frames: 0. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 75/100:\n",
            "  Train -> Total L: 0.5482 | Reco L: 0.5226 | Dis L: 1.8271 | G_L: 0.0012 (A: 100.00%) | A_L: 0.0074 (A: 100.00%) | S_L: 0.0018 (A: 100.00%) | Age_L: 0.0191 (A: 100.00%) | Region_L: 0.0364 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6599 | Reco L: 0.5500 | Dis L: 2.2991 | G_L: 0.0022 (A: 100.00%) | A_L: 0.1132 (A: 97.84%) | S_L: 0.0610 (A: 98.77%) | Age_L: 0.2121 (A: 94.14%) | Region_L: 0.2367 (A: 96.30%) | average_mcd: 2.4782 | average_speaker_id_consistency:0.6574 |disentanglement_accuracy:0.3182 | \n",
            "           Summary Metrics -> MCD: 10.7378 | Spk Sim: 0.9866 | \n",
            "           F0 RMSE: 7.4689 | F0 Corr: 0.2790 | \n",
            "           VUV Error: 0.5531 | Energy RMSE: 2153.3418\n",
            "           PESQ: 1.8966 | STOI: 0.8347\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 76/100:\n",
            "  Train -> Total L: 0.5448 | Reco L: 0.5197 | Dis L: 1.7888 | G_L: 0.0053 (A: 99.85%) | A_L: 0.0065 (A: 100.00%) | S_L: 0.0019 (A: 100.00%) | Age_L: 0.0190 (A: 100.00%) | Region_L: 0.0311 (A: 100.00%)\n",
            "  Valid -> Total L: 0.7031 | Reco L: 0.5321 | Dis L: 1.8268 | G_L: 0.0801 (A: 97.84%) | A_L: 0.1837 (A: 94.75%) | S_L: 0.1153 (A: 97.22%) | Age_L: 0.3168 (A: 93.21%) | Region_L: 0.3706 (A: 92.28%) | average_mcd: 2.4350 | average_speaker_id_consistency:0.7361 |disentanglement_accuracy:0.5653 | \n",
            "           Summary Metrics -> MCD: 10.5075 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 30.4023 | F0 Corr: 0.2591 | \n",
            "           VUV Error: 0.3506 | Energy RMSE: 2023.8206\n",
            "           PESQ: 1.9023 | STOI: 0.8396\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 77/100:\n",
            "  Train -> Total L: 0.5318 | Reco L: 0.5080 | Dis L: 1.7625 | G_L: 0.0021 (A: 99.92%) | A_L: 0.0050 (A: 100.00%) | S_L: 0.0016 (A: 100.00%) | Age_L: 0.0184 (A: 100.00%) | Region_L: 0.0287 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6813 | Reco L: 0.5583 | Dis L: 2.8950 | G_L: 0.0026 (A: 100.00%) | A_L: 0.1052 (A: 96.91%) | S_L: 0.0818 (A: 98.46%) | Age_L: 0.1816 (A: 95.99%) | Region_L: 0.2425 (A: 94.44%) | average_mcd: 2.4903 | average_speaker_id_consistency:0.7519 |disentanglement_accuracy:0.1761 | \n",
            "           Summary Metrics -> MCD: 10.6516 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 16.4755 | F0 Corr: 0.2549 | \n",
            "           VUV Error: 0.3438 | Energy RMSE: 2075.3948\n",
            "           PESQ: 1.9092 | STOI: 0.8354\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n",
            "WARNING:__main__:Insufficient voiced frames: 1. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 78/100:\n",
            "  Train -> Total L: 0.5366 | Reco L: 0.5129 | Dis L: 1.7438 | G_L: 0.0011 (A: 100.00%) | A_L: 0.0056 (A: 100.00%) | S_L: 0.0018 (A: 100.00%) | Age_L: 0.0170 (A: 100.00%) | Region_L: 0.0299 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6922 | Reco L: 0.5718 | Dis L: 2.8923 | G_L: 0.0071 (A: 99.69%) | A_L: 0.0924 (A: 98.46%) | S_L: 0.0743 (A: 99.07%) | Age_L: 0.2257 (A: 94.75%) | Region_L: 0.2176 (A: 95.99%) | average_mcd: 2.5403 | average_speaker_id_consistency:0.7541 |disentanglement_accuracy:0.1676 | \n",
            "           Summary Metrics -> MCD: 10.6779 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 7.0860 | F0 Corr: 0.3536 | \n",
            "           VUV Error: 0.5457 | Energy RMSE: 2263.2051\n",
            "           PESQ: 1.9000 | STOI: 0.8259\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 1. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 79/100:\n",
            "  Train -> Total L: 0.5390 | Reco L: 0.5159 | Dis L: 1.6767 | G_L: 0.0009 (A: 100.00%) | A_L: 0.0067 (A: 100.00%) | S_L: 0.0014 (A: 100.00%) | Age_L: 0.0194 (A: 99.85%) | Region_L: 0.0288 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6566 | Reco L: 0.5260 | Dis L: 1.7520 | G_L: 0.0017 (A: 100.00%) | A_L: 0.1437 (A: 96.30%) | S_L: 0.0743 (A: 98.46%) | Age_L: 0.3263 (A: 91.67%) | Region_L: 0.2871 (A: 94.75%) | average_mcd: 2.4585 | average_speaker_id_consistency:0.7592 |disentanglement_accuracy:0.6023 | \n",
            "           Summary Metrics -> MCD: 10.2804 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 14.1196 | F0 Corr: 0.2135 | \n",
            "           VUV Error: 0.3436 | Energy RMSE: 1981.1628\n",
            "           PESQ: 1.9246 | STOI: 0.8408\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 80/100:\n",
            "  Train -> Total L: 0.5248 | Reco L: 0.5003 | Dis L: 1.6796 | G_L: 0.0023 (A: 99.92%) | A_L: 0.0064 (A: 100.00%) | S_L: 0.0017 (A: 100.00%) | Age_L: 0.0305 (A: 99.69%) | Region_L: 0.0297 (A: 100.00%)\n",
            "  Valid -> Total L: 0.8851 | Reco L: 0.5297 | Dis L: 1.8263 | G_L: 0.0521 (A: 97.84%) | A_L: 0.4221 (A: 86.73%) | S_L: 0.1701 (A: 95.68%) | Age_L: 1.2802 (A: 73.46%) | Region_L: 0.7663 (A: 81.17%) | average_mcd: 2.4518 | average_speaker_id_consistency:0.7984 |disentanglement_accuracy:0.5199 | \n",
            "           Summary Metrics -> MCD: 10.2621 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 12.2824 | F0 Corr: 0.4990 | \n",
            "           VUV Error: 0.3462 | Energy RMSE: 2020.0725\n",
            "           PESQ: 1.9080 | STOI: 0.8413\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 81/100:\n",
            "  Train -> Total L: 0.5177 | Reco L: 0.4944 | Dis L: 1.6334 | G_L: 0.0036 (A: 99.85%) | A_L: 0.0069 (A: 100.00%) | S_L: 0.0017 (A: 100.00%) | Age_L: 0.0179 (A: 100.00%) | Region_L: 0.0328 (A: 99.92%)\n",
            "  Valid -> Total L: 0.7508 | Reco L: 0.5385 | Dis L: 2.1184 | G_L: 0.0042 (A: 100.00%) | A_L: 0.3314 (A: 88.58%) | S_L: 0.1287 (A: 97.22%) | Age_L: 0.2560 (A: 93.83%) | Region_L: 0.6761 (A: 83.95%) | average_mcd: 2.4686 | average_speaker_id_consistency:0.7412 |disentanglement_accuracy:0.3608 | \n",
            "           Summary Metrics -> MCD: 10.4957 | Spk Sim: 0.9874 | \n",
            "           F0 RMSE: 18.9413 | F0 Corr: 0.4030 | \n",
            "           VUV Error: 0.3442 | Energy RMSE: 1842.1610\n",
            "           PESQ: 1.9209 | STOI: 0.8399\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 4. Returning default values.\n",
            "WARNING:__main__:Insufficient voiced frames: 0. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 82/100:\n",
            "  Train -> Total L: 0.5194 | Reco L: 0.4963 | Dis L: 1.5923 | G_L: 0.0006 (A: 100.00%) | A_L: 0.0057 (A: 100.00%) | S_L: 0.0015 (A: 100.00%) | Age_L: 0.0249 (A: 99.69%) | Region_L: 0.0332 (A: 99.92%)\n",
            "  Valid -> Total L: 0.8512 | Reco L: 0.6184 | Dis L: 2.4920 | G_L: 0.0013 (A: 100.00%) | A_L: 0.1159 (A: 97.84%) | S_L: 0.0890 (A: 98.15%) | Age_L: 1.0449 (A: 70.06%) | Region_L: 0.4713 (A: 87.65%) | average_mcd: 2.5549 | average_speaker_id_consistency:0.7273 |disentanglement_accuracy:0.2585 | \n",
            "           Summary Metrics -> MCD: 10.9937 | Spk Sim: 0.9867 | \n",
            "           F0 RMSE: 6.7002 | F0 Corr: 0.3593 | \n",
            "           VUV Error: 0.5458 | Energy RMSE: 2264.0750\n",
            "           PESQ: 1.9088 | STOI: 0.8195\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 3. Returning default values.\n",
            "WARNING:__main__:Insufficient voiced frames: 9. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 83/100:\n",
            "  Train -> Total L: 0.5206 | Reco L: 0.4989 | Dis L: 1.5770 | G_L: 0.0007 (A: 100.00%) | A_L: 0.0039 (A: 100.00%) | S_L: 0.0015 (A: 100.00%) | Age_L: 0.0236 (A: 99.92%) | Region_L: 0.0237 (A: 100.00%)\n",
            "  Valid -> Total L: 0.7102 | Reco L: 0.5143 | Dis L: 1.9109 | G_L: 0.0032 (A: 100.00%) | A_L: 0.1345 (A: 96.30%) | S_L: 0.0906 (A: 97.84%) | Age_L: 0.8180 (A: 76.23%) | Region_L: 0.3585 (A: 93.52%) | average_mcd: 2.4433 | average_speaker_id_consistency:0.8014 |disentanglement_accuracy:0.4489 | \n",
            "           Summary Metrics -> MCD: 10.2059 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 10.7394 | F0 Corr: 0.3147 | \n",
            "           VUV Error: 0.5475 | Energy RMSE: 1891.2848\n",
            "           PESQ: 1.9242 | STOI: 0.8457\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 84/100:\n",
            "  Train -> Total L: 0.5158 | Reco L: 0.4940 | Dis L: 1.5648 | G_L: 0.0005 (A: 100.00%) | A_L: 0.0117 (A: 99.85%) | S_L: 0.0013 (A: 100.00%) | Age_L: 0.0187 (A: 99.92%) | Region_L: 0.0234 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6552 | Reco L: 0.5184 | Dis L: 2.1703 | G_L: 0.0092 (A: 99.69%) | A_L: 0.2035 (A: 94.14%) | S_L: 0.0726 (A: 98.77%) | Age_L: 0.3032 (A: 92.59%) | Region_L: 0.2728 (A: 95.99%) | average_mcd: 2.4574 | average_speaker_id_consistency:0.7550 |disentanglement_accuracy:0.3153 | \n",
            "           Summary Metrics -> MCD: 10.2144 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 21.7553 | F0 Corr: 0.3969 | \n",
            "           VUV Error: 0.1140 | Energy RMSE: 1904.4724\n",
            "           PESQ: 1.9081 | STOI: 0.8415\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 9. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 85/100:\n",
            "  Train -> Total L: 0.5134 | Reco L: 0.4845 | Dis L: 1.5575 | G_L: 0.0024 (A: 99.92%) | A_L: 0.0524 (A: 98.53%) | S_L: 0.0020 (A: 100.00%) | Age_L: 0.0185 (A: 100.00%) | Region_L: 0.0507 (A: 99.69%)\n",
            "  Valid -> Total L: 1.1763 | Reco L: 0.5076 | Dis L: 1.6465 | G_L: 0.0321 (A: 99.07%) | A_L: 1.0193 (A: 72.84%) | S_L: 0.5511 (A: 85.19%) | Age_L: 0.9699 (A: 72.53%) | Region_L: 1.7458 (A: 59.57%) | average_mcd: 2.4668 | average_speaker_id_consistency:0.8288 |disentanglement_accuracy:0.6080 | \n",
            "           Summary Metrics -> MCD: 10.0003 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 12.1747 | F0 Corr: 0.4570 | \n",
            "           VUV Error: 0.3422 | Energy RMSE: 2012.2369\n",
            "           PESQ: 1.9273 | STOI: 0.8437\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 86/100:\n",
            "  Train -> Total L: 0.5448 | Reco L: 0.5067 | Dis L: 1.5203 | G_L: 0.0119 (A: 99.69%) | A_L: 0.0508 (A: 98.69%) | S_L: 0.0032 (A: 100.00%) | Age_L: 0.0295 (A: 99.69%) | Region_L: 0.1217 (A: 98.77%)\n",
            "  Valid -> Total L: 2.4477 | Reco L: 0.5645 | Dis L: 1.6985 | G_L: 0.0788 (A: 96.60%) | A_L: 2.0756 (A: 58.95%) | S_L: 1.7178 (A: 56.17%) | Age_L: 4.2028 (A: 43.52%) | Region_L: 3.7160 (A: 31.79%) | average_mcd: 2.4403 | average_speaker_id_consistency:0.8960 |disentanglement_accuracy:0.5426 | \n",
            "           Summary Metrics -> MCD: 10.3527 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 16.5528 | F0 Corr: 0.2444 | \n",
            "           VUV Error: 0.1171 | Energy RMSE: 1923.1697\n",
            "           PESQ: 1.9184 | STOI: 0.8399\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 0. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 87/100:\n",
            "  Train -> Total L: 0.5357 | Reco L: 0.4944 | Dis L: 1.5150 | G_L: 0.0101 (A: 99.61%) | A_L: 0.0590 (A: 98.38%) | S_L: 0.0034 (A: 100.00%) | Age_L: 0.0779 (A: 98.69%) | Region_L: 0.0972 (A: 99.23%)\n",
            "  Valid -> Total L: 3.3494 | Reco L: 0.5344 | Dis L: 3.7929 | G_L: 0.2087 (A: 93.83%) | A_L: 2.3721 (A: 50.93%) | S_L: 1.8677 (A: 48.46%) | Age_L: 9.4569 (A: 20.06%) | Region_L: 6.3946 (A: 12.04%) | average_mcd: 2.4314 | average_speaker_id_consistency:0.9025 |disentanglement_accuracy:0.1023 | \n",
            "           Summary Metrics -> MCD: 10.3009 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 13.1347 | F0 Corr: 0.3319 | \n",
            "           VUV Error: 0.3191 | Energy RMSE: 1919.3496\n",
            "           PESQ: 1.9110 | STOI: 0.8378\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 88/100:\n",
            "  Train -> Total L: 0.5296 | Reco L: 0.4813 | Dis L: 1.5180 | G_L: 0.0101 (A: 99.54%) | A_L: 0.0557 (A: 98.38%) | S_L: 0.0043 (A: 100.00%) | Age_L: 0.1362 (A: 96.68%) | Region_L: 0.1069 (A: 99.07%)\n",
            "  Valid -> Total L: 1.8371 | Reco L: 0.5334 | Dis L: 1.9185 | G_L: 0.0226 (A: 99.07%) | A_L: 3.4887 (A: 40.43%) | S_L: 1.0201 (A: 72.22%) | Age_L: 1.0524 (A: 73.15%) | Region_L: 3.1818 (A: 44.75%) | average_mcd: 2.5397 | average_speaker_id_consistency:0.8354 |disentanglement_accuracy:0.3949 | \n",
            "           Summary Metrics -> MCD: 10.2655 | Spk Sim: 0.9866 | \n",
            "           F0 RMSE: 18.1623 | F0 Corr: 0.6596 | \n",
            "           VUV Error: 0.1147 | Energy RMSE: 2513.8835\n",
            "           PESQ: 1.9373 | STOI: 0.8438\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 89/100:\n",
            "  Train -> Total L: 0.5310 | Reco L: 0.4911 | Dis L: 1.4660 | G_L: 0.0019 (A: 100.00%) | A_L: 0.0424 (A: 99.07%) | S_L: 0.0036 (A: 100.00%) | Age_L: 0.1154 (A: 97.38%) | Region_L: 0.0752 (A: 99.69%)\n",
            "  Valid -> Total L: 1.9284 | Reco L: 0.5138 | Dis L: 2.0128 | G_L: 0.0040 (A: 100.00%) | A_L: 1.8960 (A: 55.86%) | S_L: 1.0483 (A: 70.68%) | Age_L: 5.2973 (A: 30.86%) | Region_L: 1.5058 (A: 61.42%) | average_mcd: 2.4466 | average_speaker_id_consistency:0.8842 |disentanglement_accuracy:0.3750 | \n",
            "           Summary Metrics -> MCD: 10.0570 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 14.9473 | F0 Corr: 0.5146 | \n",
            "           VUV Error: 0.1120 | Energy RMSE: 1955.3300\n",
            "           PESQ: 1.7304 | STOI: 0.8445\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 90/100:\n",
            "  Train -> Total L: 0.5158 | Reco L: 0.4887 | Dis L: 1.4385 | G_L: 0.0013 (A: 100.00%) | A_L: 0.0218 (A: 99.69%) | S_L: 0.0022 (A: 100.00%) | Age_L: 0.0552 (A: 98.77%) | Region_L: 0.0375 (A: 100.00%)\n",
            "  Valid -> Total L: 1.4967 | Reco L: 0.5284 | Dis L: 1.7530 | G_L: 0.0202 (A: 99.07%) | A_L: 2.8364 (A: 50.93%) | S_L: 0.6043 (A: 84.26%) | Age_L: 1.8645 (A: 58.02%) | Region_L: 1.7647 (A: 62.04%) | average_mcd: 2.4575 | average_speaker_id_consistency:0.8459 |disentanglement_accuracy:0.5199 | \n",
            "           Summary Metrics -> MCD: 10.1491 | Spk Sim: 0.9874 | \n",
            "           F0 RMSE: 14.5152 | F0 Corr: 0.5277 | \n",
            "           VUV Error: 0.3435 | Energy RMSE: 1869.3994\n",
            "           PESQ: 1.8582 | STOI: 0.8328\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 91/100:\n",
            "  Train -> Total L: 0.5238 | Reco L: 0.4985 | Dis L: 1.4316 | G_L: 0.0011 (A: 100.00%) | A_L: 0.0245 (A: 99.54%) | S_L: 0.0016 (A: 100.00%) | Age_L: 0.0501 (A: 99.23%) | Region_L: 0.0264 (A: 100.00%)\n",
            "  Valid -> Total L: 0.8385 | Reco L: 0.5132 | Dis L: 2.5973 | G_L: 0.0096 (A: 99.69%) | A_L: 0.7838 (A: 75.93%) | S_L: 0.1492 (A: 96.91%) | Age_L: 1.0175 (A: 73.77%) | Region_L: 0.4361 (A: 90.43%) | average_mcd: 2.4556 | average_speaker_id_consistency:0.7946 |disentanglement_accuracy:0.2273 | \n",
            "           Summary Metrics -> MCD: 9.9779 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 13.7934 | F0 Corr: 0.3748 | \n",
            "           VUV Error: 0.1144 | Energy RMSE: 1956.5251\n",
            "           PESQ: 1.9317 | STOI: 0.8432\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 92/100:\n",
            "  Train -> Total L: 0.5253 | Reco L: 0.4982 | Dis L: 1.3899 | G_L: 0.0011 (A: 100.00%) | A_L: 0.0179 (A: 99.69%) | S_L: 0.0020 (A: 100.00%) | Age_L: 0.0690 (A: 98.77%) | Region_L: 0.0346 (A: 100.00%)\n",
            "  Valid -> Total L: 0.9055 | Reco L: 0.5084 | Dis L: 1.5119 | G_L: 0.0026 (A: 100.00%) | A_L: 0.6164 (A: 77.78%) | S_L: 0.2976 (A: 92.59%) | Age_L: 1.0250 (A: 75.93%) | Region_L: 0.6876 (A: 83.02%) | average_mcd: 2.4465 | average_speaker_id_consistency:0.8442 |disentanglement_accuracy:0.6392 | \n",
            "           Summary Metrics -> MCD: 9.8647 | Spk Sim: 0.9869 | \n",
            "           F0 RMSE: 18.1272 | F0 Corr: 0.5180 | \n",
            "           VUV Error: 0.1117 | Energy RMSE: 2067.8513\n",
            "           PESQ: 1.9653 | STOI: 0.8400\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 93/100:\n",
            "  Train -> Total L: 0.5087 | Reco L: 0.4869 | Dis L: 1.3901 | G_L: 0.0023 (A: 99.92%) | A_L: 0.0106 (A: 99.92%) | S_L: 0.0015 (A: 100.00%) | Age_L: 0.0299 (A: 99.77%) | Region_L: 0.0282 (A: 100.00%)\n",
            "  Valid -> Total L: 0.8615 | Reco L: 0.5283 | Dis L: 1.8429 | G_L: 0.0062 (A: 99.69%) | A_L: 0.9042 (A: 77.47%) | S_L: 0.1840 (A: 95.06%) | Age_L: 0.8030 (A: 80.86%) | Region_L: 0.5136 (A: 87.96%) | average_mcd: 2.5053 | average_speaker_id_consistency:0.7638 |disentanglement_accuracy:0.4006 | \n",
            "           Summary Metrics -> MCD: 10.0908 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 18.8601 | F0 Corr: 0.5568 | \n",
            "           VUV Error: 0.1227 | Energy RMSE: 2154.4258\n",
            "           PESQ: 1.9432 | STOI: 0.8390\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 94/100:\n",
            "  Train -> Total L: 0.5139 | Reco L: 0.4946 | Dis L: 1.3436 | G_L: 0.0009 (A: 100.00%) | A_L: 0.0109 (A: 99.92%) | S_L: 0.0013 (A: 100.00%) | Age_L: 0.0186 (A: 100.00%) | Region_L: 0.0217 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6993 | Reco L: 0.4976 | Dis L: 1.7145 | G_L: 0.0020 (A: 100.00%) | A_L: 0.3933 (A: 85.80%) | S_L: 0.0841 (A: 98.15%) | Age_L: 0.5396 (A: 86.11%) | Region_L: 0.4902 (A: 88.58%) | average_mcd: 2.4442 | average_speaker_id_consistency:0.7940 |disentanglement_accuracy:0.5540 | \n",
            "           Summary Metrics -> MCD: 9.8566 | Spk Sim: 0.9872 | \n",
            "           F0 RMSE: 16.7458 | F0 Corr: 0.6681 | \n",
            "           VUV Error: 0.1175 | Energy RMSE: 1973.3302\n",
            "           PESQ: 1.9468 | STOI: 0.8517\n",
            "  Validation loss did not improve from 0.6434\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 1. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 95/100:\n",
            "  Train -> Total L: 0.5043 | Reco L: 0.4864 | Dis L: 1.3152 | G_L: 0.0009 (A: 100.00%) | A_L: 0.0065 (A: 100.00%) | S_L: 0.0012 (A: 100.00%) | Age_L: 0.0158 (A: 99.85%) | Region_L: 0.0182 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6276 | Reco L: 0.5265 | Dis L: 1.4196 | G_L: 0.0012 (A: 100.00%) | A_L: 0.1242 (A: 96.91%) | S_L: 0.0674 (A: 98.46%) | Age_L: 0.2075 (A: 95.68%) | Region_L: 0.1989 (A: 96.30%) | average_mcd: 2.4681 | average_speaker_id_consistency:0.7475 |disentanglement_accuracy:0.6648 | \n",
            "           Summary Metrics -> MCD: 10.2041 | Spk Sim: 0.9874 | \n",
            "           F0 RMSE: 13.3720 | F0 Corr: 0.5415 | \n",
            "           VUV Error: 0.3404 | Energy RMSE: 1841.1727\n",
            "           PESQ: 1.9401 | STOI: 0.8469\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6276)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 0. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 96/100:\n",
            "  Train -> Total L: 0.5095 | Reco L: 0.4924 | Dis L: 1.3211 | G_L: 0.0004 (A: 100.00%) | A_L: 0.0039 (A: 100.00%) | S_L: 0.0011 (A: 100.00%) | Age_L: 0.0130 (A: 100.00%) | Region_L: 0.0162 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6169 | Reco L: 0.5102 | Dis L: 1.4993 | G_L: 0.0011 (A: 100.00%) | A_L: 0.1228 (A: 96.30%) | S_L: 0.0754 (A: 98.46%) | Age_L: 0.2151 (A: 94.14%) | Region_L: 0.2008 (A: 95.68%) | average_mcd: 2.4386 | average_speaker_id_consistency:0.7819 |disentanglement_accuracy:0.6222 | \n",
            "           Summary Metrics -> MCD: 9.9564 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 13.2128 | F0 Corr: 0.5353 | \n",
            "           VUV Error: 0.3385 | Energy RMSE: 2065.6353\n",
            "           PESQ: 1.9189 | STOI: 0.8455\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.6169)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 97/100:\n",
            "  Train -> Total L: 0.4935 | Reco L: 0.4767 | Dis L: 1.2880 | G_L: 0.0005 (A: 100.00%) | A_L: 0.0047 (A: 100.00%) | S_L: 0.0011 (A: 100.00%) | Age_L: 0.0135 (A: 100.00%) | Region_L: 0.0152 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6195 | Reco L: 0.5260 | Dis L: 1.6893 | G_L: 0.0014 (A: 100.00%) | A_L: 0.1371 (A: 96.60%) | S_L: 0.0541 (A: 99.07%) | Age_L: 0.1958 (A: 95.06%) | Region_L: 0.1608 (A: 96.60%) | average_mcd: 2.4689 | average_speaker_id_consistency:0.7051 |disentanglement_accuracy:0.4858 | \n",
            "           Summary Metrics -> MCD: 10.1144 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 16.8518 | F0 Corr: 0.6689 | \n",
            "           VUV Error: 0.1114 | Energy RMSE: 1986.9894\n",
            "           PESQ: 1.7621 | STOI: 0.8397\n",
            "  Validation loss did not improve from 0.6169\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 98/100:\n",
            "  Train -> Total L: 0.4982 | Reco L: 0.4827 | Dis L: 1.2398 | G_L: 0.0006 (A: 100.00%) | A_L: 0.0039 (A: 100.00%) | S_L: 0.0010 (A: 100.00%) | Age_L: 0.0085 (A: 100.00%) | Region_L: 0.0136 (A: 100.00%)\n",
            "  Valid -> Total L: 0.5842 | Reco L: 0.4894 | Dis L: 2.0713 | G_L: 0.0022 (A: 100.00%) | A_L: 0.1103 (A: 97.22%) | S_L: 0.0618 (A: 99.07%) | Age_L: 0.1637 (A: 95.68%) | Region_L: 0.1559 (A: 96.30%) | average_mcd: 2.4420 | average_speaker_id_consistency:0.7996 |disentanglement_accuracy:0.3523 | \n",
            "           Summary Metrics -> MCD: 9.7191 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 11.3486 | F0 Corr: 0.5502 | \n",
            "           VUV Error: 0.3427 | Energy RMSE: 2014.1007\n",
            "           PESQ: 1.9448 | STOI: 0.8513\n",
            "  Model saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth (Validation Loss: 0.5842)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n",
            "\n",
            "Epoch 99/100:\n",
            "  Train -> Total L: 0.4907 | Reco L: 0.4749 | Dis L: 1.2466 | G_L: 0.0004 (A: 100.00%) | A_L: 0.0064 (A: 99.85%) | S_L: 0.0009 (A: 100.00%) | Age_L: 0.0081 (A: 100.00%) | Region_L: 0.0137 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6226 | Reco L: 0.4981 | Dis L: 1.3131 | G_L: 0.0013 (A: 100.00%) | A_L: 0.2826 (A: 91.98%) | S_L: 0.0820 (A: 97.84%) | Age_L: 0.2012 (A: 95.06%) | Region_L: 0.2182 (A: 95.68%) | average_mcd: 2.4044 | average_speaker_id_consistency:0.8027 |disentanglement_accuracy:0.6989 | \n",
            "           Summary Metrics -> MCD: 9.7432 | Spk Sim: 0.9870 | \n",
            "           F0 RMSE: 18.1448 | F0 Corr: 0.4621 | \n",
            "           VUV Error: 0.1077 | Energy RMSE: 1897.3635\n",
            "           PESQ: 1.9654 | STOI: 0.8486\n",
            "  Validation loss did not improve from 0.5842\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 1. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 100/100:\n",
            "  Train -> Total L: 0.4939 | Reco L: 0.4785 | Dis L: 1.1972 | G_L: 0.0004 (A: 100.00%) | A_L: 0.0057 (A: 100.00%) | S_L: 0.0010 (A: 100.00%) | Age_L: 0.0099 (A: 100.00%) | Region_L: 0.0140 (A: 100.00%)\n",
            "  Valid -> Total L: 0.6330 | Reco L: 0.4921 | Dis L: 1.3449 | G_L: 0.0016 (A: 100.00%) | A_L: 0.1667 (A: 96.91%) | S_L: 0.0741 (A: 97.84%) | Age_L: 0.4987 (A: 86.42%) | Region_L: 0.2367 (A: 94.44%) | average_mcd: 2.4237 | average_speaker_id_consistency:0.8030 |disentanglement_accuracy:0.7045 | \n",
            "           Summary Metrics -> MCD: 9.6844 | Spk Sim: 0.9873 | \n",
            "           F0 RMSE: 10.3851 | F0 Corr: 0.6019 | \n",
            "           VUV Error: 0.3429 | Energy RMSE: 1946.8118\n",
            "           PESQ: 1.9501 | STOI: 0.8510\n",
            "  Validation loss did not improve from 0.5842\n",
            "\n",
            "\n",
            "--- Training complete for Experiment: Minimal_Attributes! ---\n",
            "\n",
            "--- Evaluating on Test Sets for Experiment: Minimal_Attributes ---\n",
            "Loaded best model from /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/best_vc_model_Minimal_Attributes.pth for final evaluation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In-Test Set (Minimal_Attributes) -> Total L: 0.5842 | Reco L: 0.4894 | Dis L: 2.0713 | G_L: 0.0022 (A: 100.00%) | A_L: 0.1103 (A: 97.22%) | S_L: 0.0618 (A: 99.07%) | Age_L: 0.1637 (A: 95.68%) | Region_L: 0.1559 (A: 96.30%) | average_mcd:2.4420 |average_speaker_id_consistency: 0.7996 | disentanglement_accuracy:0.3523 | \n",
            "           Summary Metrics -> MCD: 2.4420 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 11.3486 | F0 Corr: 0.5502 | \n",
            "           VUV Error: 0.3427 | Energy RMSE: 2014.1007\n",
            "           PESQ: 1.9448 | STOI: 0.8513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_audio_batch=  (4, 214272)\n",
            "reconstructed_audio_batch=  (4, 214272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Insufficient voiced frames: 2. Returning default values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Out-Test Set (Minimal_Attributes) -> Total L: 0.5842 | Reco L: 0.4894 | Dis L: 2.0713 | G_L: 0.0022 (A: 100.00%) | A_L: 0.1103 (A: 97.22%) | S_L: 0.0618 (A: 99.07%) | Age_L: 0.1637 (A: 95.68%) | Region_L: 0.1559 (A: 96.30%) | average_mcd:2.4420 | average_speaker_id_consistency: 0.7996 | disentanglement_accuracy:0.3523 | \n",
            "           Summary Metrics -> MCD: 2.4420 | Spk Sim: 0.9871 | \n",
            "           F0 RMSE: 11.3486 | F0 Corr: 0.5502 | \n",
            "           VUV Error: 0.3427 | Energy RMSE: 2014.1007\n",
            "           PESQ: 1.9448 | STOI: 0.8513\n",
            "\n",
            "Results for Experiment 'Minimal_Attributes' saved to /content/drive/MyDrive/29_MFCCGAN-VC/PreProcessed2/experiment_results/results_Minimal_Attributes.json\n",
            "\n",
            "\n",
            "All experiments completed!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
